apiVersion: batch/v1
kind: Job
metadata:
  generateName: resilience-sim-native-
  namespace: default
  labels:
    app: resilience-simulation
    platform: native-kubernetes
spec:
  backoffLimit: 0
  ttlSecondsAfterFinished: 86400  # Keep for 24 hours
  template:
    metadata:
      labels:
        app: resilience-simulation
        platform: native-kubernetes
    spec:
      serviceAccountName: chaos-admin
      restartPolicy: Never
      
      nodeSelector:
        kubernetes.io/hostname: master-m003
      
      tolerations:
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule

      containers:
      - name: resilience-controller
        image: kushsahni1/chaos-sim:latest
        imagePullPolicy: Always
        
        securityContext:
          runAsUser: 0
          runAsGroup: 0
        
        env:
        - name: PYTHONUNBUFFERED
          value: "1"
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        
        command: ["/bin/bash", "-c"]
        args:
          - |
            set -e
            
            # ===== CONFIGURATION =====
            export RUN_ID="native-$(date +%Y%m%d-%H%M%S)-${POD_NAME##*-}"
            export LOG_DIR="/app/logs/${RUN_ID}"
            export LOG_FILE="${LOG_DIR}/full_execution.log"
            export METRICS_FILE="${LOG_DIR}/metrics.txt"
            export TIMING_FILE="${LOG_DIR}/timing.csv"
            
            mkdir -p "${LOG_DIR}"
            
            # ===== LOGGING HELPERS =====
            log() {
                local msg="[$(date '+%Y-%m-%d %H:%M:%S')] $1"
                echo "$msg" | tee -a "${LOG_FILE}"
            }
            
            log_section() {
                echo "" | tee -a "${LOG_FILE}"
                echo "================================================================" | tee -a "${LOG_FILE}"
                echo " $1" | tee -a "${LOG_FILE}"
                echo " Time: $(date '+%Y-%m-%d %H:%M:%S')" | tee -a "${LOG_FILE}"
                echo "================================================================" | tee -a "${LOG_FILE}"
            }
            
            record_timing() {
                local step="$1"
                local start="$2"
                local end="$3"
                local duration=$((end - start))
                echo "${step},${start},${end},${duration}" >> "${TIMING_FILE}"
                log "TIMING: ${step} completed in ${duration} seconds"
            }
            
            # ===== INITIALIZE METRICS =====
            WORKFLOW_START=$(date +%s)
            
            # CSV Header for timing
            echo "step,start_epoch,end_epoch,duration_seconds" > "${TIMING_FILE}"
            
            # Metrics file header
            cat > "${METRICS_FILE}" << EOF
            # Resilience Simulation Metrics - Native Kubernetes Job
            # Generated: $(date '+%Y-%m-%d %H:%M:%S')
            
            PLATFORM=Native_Kubernetes
            RUN_ID=${RUN_ID}
            NODE_NAME=${NODE_NAME}
            POD_NAME=${POD_NAME}
            WORKFLOW_START_EPOCH=${WORKFLOW_START}
            WORKFLOW_START_ISO=$(date -d @${WORKFLOW_START} '+%Y-%m-%dT%H:%M:%S%z' 2>/dev/null || date '+%Y-%m-%dT%H:%M:%S%z')
            EOF
            
            log_section "INITIALIZATION"
            log "Run ID: ${RUN_ID}"
            log "Platform: Native Kubernetes Job"
            log "Node: ${NODE_NAME}"
            log "Pod: ${POD_NAME}"
            log "Log Directory: ${LOG_DIR}"
            
            # ===== STEP 1: PARALLEL HEALTH CHECKS =====
            log_section "STEP 1: PARALLEL HEALTH CHECKS (3x)"
            
            HEALTH_CHECK_START=$(date +%s)
            echo "HEALTH_CHECK_START_EPOCH=${HEALTH_CHECK_START}" >> "${METRICS_FILE}"
            
            # Create temp files for parallel execution output
            HC_LOG1="/tmp/health_check_1.log"
            HC_LOG2="/tmp/health_check_2.log"
            HC_LOG3="/tmp/health_check_3.log"
            
            log "Starting 3 parallel health checks..."
            
            # Run 3 health checks in parallel
            python3 /app/rack_resiliency_to_host.py health-check --stabilization-time 10 > "${HC_LOG1}" 2>&1 &
            PID1=$!
            
            python3 /app/rack_resiliency_to_host.py health-check --stabilization-time 10 > "${HC_LOG2}" 2>&1 &
            PID2=$!
            
            python3 /app/rack_resiliency_to_host.py health-check --stabilization-time 10 > "${HC_LOG3}" 2>&1 &
            PID3=$!
            
            # Wait for all to complete
            wait $PID1 && HC1_STATUS="SUCCESS" || HC1_STATUS="FAILED"
            wait $PID2 && HC2_STATUS="SUCCESS" || HC2_STATUS="FAILED"
            wait $PID3 && HC3_STATUS="SUCCESS" || HC3_STATUS="FAILED"
            
            HEALTH_CHECK_END=$(date +%s)
            
            # Append health check logs to main log
            log "--- Health Check 1 Output (${HC1_STATUS}) ---"
            cat "${HC_LOG1}" >> "${LOG_FILE}"
            log "--- Health Check 2 Output (${HC2_STATUS}) ---"
            cat "${HC_LOG2}" >> "${LOG_FILE}"
            log "--- Health Check 3 Output (${HC3_STATUS}) ---"
            cat "${HC_LOG3}" >> "${LOG_FILE}"
            
            record_timing "HEALTH_CHECKS_PARALLEL" "${HEALTH_CHECK_START}" "${HEALTH_CHECK_END}"
            
            echo "HEALTH_CHECK_END_EPOCH=${HEALTH_CHECK_END}" >> "${METRICS_FILE}"
            echo "HEALTH_CHECK_DURATION_SECONDS=$((HEALTH_CHECK_END - HEALTH_CHECK_START))" >> "${METRICS_FILE}"
            echo "HEALTH_CHECK_1_STATUS=${HC1_STATUS}" >> "${METRICS_FILE}"
            echo "HEALTH_CHECK_2_STATUS=${HC2_STATUS}" >> "${METRICS_FILE}"
            echo "HEALTH_CHECK_3_STATUS=${HC3_STATUS}" >> "${METRICS_FILE}"
            
            # ===== STEP 2: NODE FAILURE SIMULATION =====
            log_section "STEP 2: NODE FAILURE SIMULATION"
            
            NODE_SIM_START=$(date +%s)
            echo "NODE_SIM_START_EPOCH=${NODE_SIM_START}" >> "${METRICS_FILE}"
            
            log "Starting node failure simulation (stabilization: 60s)..."
            
            python3 /app/rack_resiliency_to_host.py simulate-node --stabilization-time 60 2>&1 | tee -a "${LOG_FILE}"
            NODE_SIM_STATUS=$?
            
            NODE_SIM_END=$(date +%s)
            record_timing "NODE_SIMULATION" "${NODE_SIM_START}" "${NODE_SIM_END}"
            
            echo "NODE_SIM_END_EPOCH=${NODE_SIM_END}" >> "${METRICS_FILE}"
            echo "NODE_SIM_DURATION_SECONDS=$((NODE_SIM_END - NODE_SIM_START))" >> "${METRICS_FILE}"
            echo "NODE_SIM_STATUS=$([ $NODE_SIM_STATUS -eq 0 ] && echo 'SUCCESS' || echo 'FAILED')" >> "${METRICS_FILE}"
            
            # ===== STEP 3: INTERIM HEALTH CHECK =====
            log_section "STEP 3: INTERIM HEALTH CHECK"
            
            INTERIM_HC_START=$(date +%s)
            echo "INTERIM_HC_START_EPOCH=${INTERIM_HC_START}" >> "${METRICS_FILE}"
            
            python3 /app/rack_resiliency_to_host.py health-check --stabilization-time 10 2>&1 | tee -a "${LOG_FILE}"
            
            INTERIM_HC_END=$(date +%s)
            record_timing "INTERIM_HEALTH_CHECK" "${INTERIM_HC_START}" "${INTERIM_HC_END}"
            
            echo "INTERIM_HC_END_EPOCH=${INTERIM_HC_END}" >> "${METRICS_FILE}"
            echo "INTERIM_HC_DURATION_SECONDS=$((INTERIM_HC_END - INTERIM_HC_START))" >> "${METRICS_FILE}"
            
            # ===== STEP 4: RACK FAILURE SIMULATION =====
            log_section "STEP 4: RACK FAILURE SIMULATION"
            
            RACK_SIM_START=$(date +%s)
            echo "RACK_SIM_START_EPOCH=${RACK_SIM_START}" >> "${METRICS_FILE}"
            
            log "Starting rack failure simulation (stabilization: 120s, downtime: 60s)..."
            
            python3 /app/rack_resiliency_to_host.py simulate-rack --stabilization-time 120 --downtime 60 2>&1 | tee -a "${LOG_FILE}"
            RACK_SIM_STATUS=$?
            
            RACK_SIM_END=$(date +%s)
            record_timing "RACK_SIMULATION" "${RACK_SIM_START}" "${RACK_SIM_END}"
            
            echo "RACK_SIM_END_EPOCH=${RACK_SIM_END}" >> "${METRICS_FILE}"
            echo "RACK_SIM_DURATION_SECONDS=$((RACK_SIM_END - RACK_SIM_START))" >> "${METRICS_FILE}"
            echo "RACK_SIM_STATUS=$([ $RACK_SIM_STATUS -eq 0 ] && echo 'SUCCESS' || echo 'FAILED')" >> "${METRICS_FILE}"
            
            # ===== STEP 5: FINAL HEALTH CHECK =====
            log_section "STEP 5: FINAL HEALTH CHECK"
            
            FINAL_HC_START=$(date +%s)
            echo "FINAL_HC_START_EPOCH=${FINAL_HC_START}" >> "${METRICS_FILE}"
            
            python3 /app/rack_resiliency_to_host.py health-check --stabilization-time 10 2>&1 | tee -a "${LOG_FILE}"
            
            FINAL_HC_END=$(date +%s)
            record_timing "FINAL_HEALTH_CHECK" "${FINAL_HC_START}" "${FINAL_HC_END}"
            
            echo "FINAL_HC_END_EPOCH=${FINAL_HC_END}" >> "${METRICS_FILE}"
            echo "FINAL_HC_DURATION_SECONDS=$((FINAL_HC_END - FINAL_HC_START))" >> "${METRICS_FILE}"
            
            # ===== WORKFLOW COMPLETE =====
            WORKFLOW_END=$(date +%s)
            TOTAL_DURATION=$((WORKFLOW_END - WORKFLOW_START))
            
            log_section "WORKFLOW COMPLETE"
            log "Total Duration: ${TOTAL_DURATION} seconds"
            log "Log files saved to: ${LOG_DIR}"
            
            # Final metrics
            cat >> "${METRICS_FILE}" << EOF
            
            # Final Summary
            WORKFLOW_END_EPOCH=${WORKFLOW_END}
            WORKFLOW_END_ISO=$(date -d @${WORKFLOW_END} '+%Y-%m-%dT%H:%M:%S%z' 2>/dev/null || date '+%Y-%m-%dT%H:%M:%S%z')
            TOTAL_DURATION_SECONDS=${TOTAL_DURATION}
            TOTAL_DURATION_FORMATTED=$(printf '%02d:%02d:%02d' $((TOTAL_DURATION/3600)) $((TOTAL_DURATION%3600/60)) $((TOTAL_DURATION%60)))
            EOF
            
            # Print summary
            log ""
            log "===== TIMING SUMMARY ====="
            cat "${TIMING_FILE}" | tee -a "${LOG_FILE}"
            log ""
            log "===== METRICS SUMMARY ====="
            cat "${METRICS_FILE}" | tee -a "${LOG_FILE}"
            
            # Fix permissions for vagrant user
            chmod -R 755 "${LOG_DIR}"
            chown -R 1000:1000 "${LOG_DIR}" 2>/dev/null || true
            
            log "Native Kubernetes Job completed successfully!"
        
        volumeMounts:
        - name: log-vol
          mountPath: /app/logs
        
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "500m"

      volumes:
      - name: log-vol
        hostPath:
          path: /home/vagrant/k8s-sim-logs/native-runs
          type: DirectoryOrCreate
