[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: Log directory created/verified: /app/logs[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: File logging configured successfully[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:50:17,193 [INFO] Loaded in-cluster Kubernetes config[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:50:17,193 [INFO] Running on host: resilience-scaled-heft-pplkg-node-failure-sim-2665931194[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:50:17,193 [INFO] Detected current node: master-m003, zone: R3[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:50:17,193 [INFO] Checking if we have permissions to modify nodes...[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:50:17,228 [INFO] Testing permissions using node: master-m001[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:50:17,236 [INFO] Permission check successful - we can modify nodes[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:50:17,236 [INFO] Using real Kubernetes API for node control[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:50:17,237 [INFO] Action received: simulate-node[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:50:17,237 [INFO] Stabilization time: 60 seconds[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:50:17,237 [INFO] Simulating node failure: master-m002[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:50:17,237 [INFO] Simulating node failure for master-m002 using Kubernetes API[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:50:17,249 [INFO] Node master-m002 cordoned[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:50:17,309 [INFO] Node master-m002 tainted with NoExecute[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:50:17,309 [INFO] Node master-m002 powered off (delay 5s)[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:50:17,309 [INFO] Node master-m002 down for 10 seconds[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:50:17,309 [INFO] Waiting 60 seconds for the cluster to stabilize before health check...[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:17,369 [INFO] Running health check after node power off[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:17,369 [INFO] Starting full health check[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:17,369 [INFO] [0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: ============ DETAILED NODE STATUS ============[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:17,369 [INFO] Basic Node Information (kubectl get nodes -o wide):[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:29,286 [INFO]   NAME          STATUS                     ROLES           AGE   VERSION    INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:29,287 [INFO]   master-m001   Ready                      control-plane   13d   v1.32.11   192.168.56.101   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:29,287 [INFO]   master-m002   Ready,SchedulingDisabled   control-plane   13d   v1.32.11   192.168.56.102   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:29,287 [INFO]   master-m003   Ready                      control-plane   13d   v1.32.11   192.168.56.103   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:29,287 [INFO]   worker-w001   Ready                      <none>          13d   v1.32.11   192.168.56.104   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:29,287 [INFO]   worker-w002   Ready                      <none>          13d   v1.32.11   192.168.56.105   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:29,287 [INFO]   worker-w003   Ready                      <none>          13d   v1.32.11   192.168.56.106   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:29,287 [INFO]   worker-w004   Ready                      <none>          13d   v1.32.11   192.168.56.107   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:29,287 [INFO]   worker-w005   Ready                      <none>          13d   v1.32.11   192.168.56.108   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:29,288 [INFO]   worker-w006   Ready                      <none>          13d   v1.32.11   192.168.56.109   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:29,288 [INFO] [0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: Enhanced Node Status (with taint and cordon indicators):[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:29,288 [INFO]   NAME                STATUS    ROLES           ZONE   CORDONED   TAINTS[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:29,656 [INFO]   master-m001     Ready  ‚úì worker         R1    No       node-role.kubernetes.io/control-plane [0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:29,877 [INFO]   master-m002     Ready  ‚úì worker         R2    YES     ‚ö†Ô∏è simulated-failure, node.kubernetes.io/unschedulable ‚ö†Ô∏è[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:30,116 [INFO]   master-m003     Ready  ‚úì worker         R3    No       None [0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:30,331 [INFO]   worker-w001     Ready  ‚úì worker         R1    No       None [0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:30,601 [INFO]   worker-w002     Ready  ‚úì worker         R1    No       None [0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:30,826 [INFO]   worker-w003     Ready  ‚úì worker         R2    No       None [0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,036 [INFO]   worker-w004     Ready  ‚úì worker         R2    No       None [0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,225 [INFO]   worker-w005     Ready  ‚úì worker         R3    No       None [0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,387 [INFO]   worker-w006     Ready  ‚úì worker         R3    No       None [0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,388 [INFO] [0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: Legend:[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,388 [INFO]   ‚úì = Node is Ready[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,388 [INFO]   ‚ö†Ô∏è = Warning indicator (NotReady, Cordoned, or has simulated-failure taint)[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,388 [INFO] ============ DETAILED POD INFORMATION ============[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,388 [INFO] Running 'kubectl get pods -o wide' to show detailed pod placement:[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,467 [INFO]   NAMESPACE     NAME                                                       READY   STATUS      RESTARTS        AGE     IP                NODE          NOMINATED NODE   READINESS GATES[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,468 [INFO]   argo          argo-server-5c69cb69db-c8qj8                               1/1     Running     0               74s     192.168.240.163   worker-w003   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,468 [INFO]   argo          resilience-bench-ptngm-initialize-metrics-940927476        0/2     Completed   0               5d9h    192.168.221.125   master-m003   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,468 [INFO]   argo          resilience-bench-ptngm-run-health-check-1168793163         0/2     Completed   0               5d9h    192.168.221.67    master-m003   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,468 [INFO]   argo          resilience-bench-ptngm-run-health-check-1185570782         0/2     Completed   0               5d9h    192.168.221.66    master-m003   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,468 [INFO]   argo          resilience-bench-ptngm-run-health-check-1202348401         0/2     Completed   0               5d9h    192.168.221.122   master-m003   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,468 [INFO]   argo          resilience-bench-wf87f-initialize-metrics-3677018408       0/2     Completed   0               5d11h   192.168.221.100   master-m003   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,468 [INFO]   argo          resilience-bench-wf87f-run-health-check-2914150102         0/2     Completed   0               5d11h   192.168.221.105   master-m003   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,468 [INFO]   argo          resilience-bench-wf87f-run-health-check-3931060015         0/2     Completed   0               5d11h   192.168.221.104   master-m003   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,468 [INFO]   argo          resilience-bench-wf87f-run-health-check-3947837634         0/2     Completed   0               5d11h   192.168.221.98    master-m003   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,469 [INFO]   argo          resilience-bench-wf87f-run-health-check-3964615253         0/2     Completed   0               5d11h   192.168.221.103   master-m003   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,469 [INFO]   argo          resilience-bench-wf87f-run-node-simulation-4173146970      0/2     Completed   0               5d11h   192.168.221.101   master-m003   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,469 [INFO]   argo          resilience-bench-wf87f-run-rack-simulation-1356070019      0/2     Completed   0               5d11h   192.168.221.109   master-m003   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,469 [INFO]   argo          resilience-scaled-heft-pplkg-node-failure-sim-2665931194   2/2     Running     0               5m35s   192.168.221.69    master-m003   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,469 [INFO]   argo          workflow-controller-ccbd949dc-ghwvq                        1/1     Running     4 (4m5s ago)    50m     192.168.191.65    worker-w006   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,469 [INFO]   kube-system   calico-kube-controllers-7498b9bb4c-df4xv                   1/1     Running     1 (3m13s ago)   24m     192.168.195.242   worker-w002   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,469 [INFO]   kube-system   calico-node-4zhd4                                          1/1     Running     0               13d     192.168.56.105    worker-w002   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,469 [INFO]   kube-system   calico-node-75nx6                                          1/1     Running     0               13d     192.168.56.109    worker-w006   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,469 [INFO]   kube-system   calico-node-7lkdq                                          1/1     Running     0               13d     192.168.56.104    worker-w001   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,469 [INFO]   kube-system   calico-node-85f8c                                          1/1     Running     0               13d     192.168.56.102    master-m002   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,470 [INFO]   kube-system   calico-node-j8nb9                                          1/1     Running     0               13d     192.168.56.101    master-m001   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,470 [INFO]   kube-system   calico-node-lbcb2                                          1/1     Running     0               13d     192.168.56.106    worker-w003   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,470 [INFO]   kube-system   calico-node-wlr5v                                          1/1     Running     0               13d     192.168.56.107    worker-w004   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,470 [INFO]   kube-system   calico-node-xnzjw                                          1/1     Running     0               13d     192.168.56.103    master-m003   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,470 [INFO]   kube-system   calico-node-xsltn                                          1/1     Running     0               13d     192.168.56.108    worker-w005   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,470 [INFO]   kube-system   coredns-668d6bf9bc-8dqlk                                   1/1     Running     0               131m    192.168.15.200    worker-w005   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,470 [INFO]   kube-system   coredns-668d6bf9bc-8nn7w                                   1/1     Running     0               24m     192.168.132.179   worker-w001   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,470 [INFO]   kube-system   etcd-master-m001                                           1/1     Running     0               13d     192.168.56.101    master-m001   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,470 [INFO]   kube-system   kube-apiserver-master-m001                                 1/1     Running     1 (5d4h ago)    13d     192.168.56.101    master-m001   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,471 [INFO]   kube-system   kube-controller-manager-master-m001                        1/1     Running     16 (4m7s ago)   13d     192.168.56.101    master-m001   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,471 [INFO]   kube-system   kube-proxy-5lzhj                                           1/1     Running     0               13d     192.168.56.102    master-m002   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,471 [INFO]   kube-system   kube-proxy-7jqkv                                           1/1     Running     0               13d     192.168.56.109    worker-w006   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,471 [INFO]   kube-system   kube-proxy-cv5dt                                           1/1     Running     0               13d     192.168.56.108    worker-w005   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,471 [INFO]   kube-system   kube-proxy-fvpmr                                           1/1     Running     0               13d     192.168.56.101    master-m001   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,471 [INFO]   kube-system   kube-proxy-hgs5z                                           1/1     Running     0               13d     192.168.56.106    worker-w003   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,471 [INFO]   kube-system   kube-proxy-kmgqr                                           1/1     Running     0               13d     192.168.56.105    worker-w002   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,471 [INFO]   kube-system   kube-proxy-rdbz5                                           1/1     Running     0               13d     192.168.56.107    worker-w004   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,471 [INFO]   kube-system   kube-proxy-w8mnb                                           1/1     Running     0               13d     192.168.56.103    master-m003   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,471 [INFO]   kube-system   kube-proxy-x9jxr                                           1/1     Running     0               13d     192.168.56.104    worker-w001   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,472 [INFO]   kube-system   kube-scheduler-master-m001                                 1/1     Running     15 (4m2s ago)   13d     192.168.56.101    master-m001   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,472 [INFO] [0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: Pod distribution by node:[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,646 [INFO]   Node worker-w003: 3 pods[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,646 [INFO]   Node master-m003: 14 pods[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,646 [INFO]   Node 50m: 1 pods[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,646 [INFO]   Node 24m: 1 pods[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,646 [INFO]   Node worker-w002: 2 pods[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,646 [INFO]   Node worker-w006: 2 pods[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,647 [INFO]   Node worker-w001: 3 pods[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,647 [INFO]   Node master-m002: 2 pods[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,647 [INFO]   Node master-m001: 3 pods[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,647 [INFO]   Node worker-w004: 2 pods[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,647 [INFO]   Node worker-w005: 3 pods[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,647 [INFO]   Node 13d: 3 pods[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,647 [INFO] [0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: Filtering for simulation services:[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,798 [INFO] Node master-m001 is Ready[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,798 [INFO] Node master-m002 is Ready[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,798 [INFO] Node master-m003 is Ready[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,798 [INFO] Node worker-w001 is Ready[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,798 [INFO] Node worker-w002 is Ready[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,798 [INFO] Node worker-w003 is Ready[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,798 [INFO] Node worker-w004 is Ready[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,799 [INFO] Node worker-w005 is Ready[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,799 [INFO] Node worker-w006 is Ready[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,802 [WARNING] No pods found for etcd-sim[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,804 [WARNING] No pods found for postgres-sim[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,806 [WARNING] No pods found for redis-sim[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,808 [WARNING] No pods found for nginx-sim[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,810 [WARNING] No pods found for auth-sim[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:31,810 [INFO] Completed full health check[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:41,820 [INFO] Running health check before node power on[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:41,820 [INFO] Starting full health check[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:41,820 [INFO] [0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: ============ DETAILED NODE STATUS ============[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:41,820 [INFO] Basic Node Information (kubectl get nodes -o wide):[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:41,889 [INFO]   NAME          STATUS                     ROLES           AGE   VERSION    INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:41,890 [INFO]   master-m001   Ready                      control-plane   13d   v1.32.11   192.168.56.101   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:41,890 [INFO]   master-m002   Ready,SchedulingDisabled   control-plane   13d   v1.32.11   192.168.56.102   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:41,890 [INFO]   master-m003   Ready                      control-plane   13d   v1.32.11   192.168.56.103   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:41,890 [INFO]   worker-w001   Ready                      <none>          13d   v1.32.11   192.168.56.104   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:41,890 [INFO]   worker-w002   Ready                      <none>          13d   v1.32.11   192.168.56.105   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:41,890 [INFO]   worker-w003   Ready                      <none>          13d   v1.32.11   192.168.56.106   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:41,890 [INFO]   worker-w004   Ready                      <none>          13d   v1.32.11   192.168.56.107   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:41,890 [INFO]   worker-w005   Ready                      <none>          13d   v1.32.11   192.168.56.108   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:41,890 [INFO]   worker-w006   Ready                      <none>          13d   v1.32.11   192.168.56.109   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:41,890 [INFO] [0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: Enhanced Node Status (with taint and cordon indicators):[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:41,890 [INFO]   NAME                STATUS    ROLES           ZONE   CORDONED   TAINTS[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:42,145 [INFO]   master-m001     Ready  ‚úì worker         R1    No       node-role.kubernetes.io/control-plane [0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:42,320 [INFO]   master-m002     Ready  ‚úì worker         R2    YES     ‚ö†Ô∏è simulated-failure, node.kubernetes.io/unschedulable ‚ö†Ô∏è[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:42,533 [INFO]   master-m003     Ready  ‚úì worker         R3    No       None [0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:43,404 [INFO]   worker-w001     Ready  ‚úì worker         R1    No       None [0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:43,628 [INFO]   worker-w002     Ready  ‚úì worker         R1    No       None [0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:43,814 [INFO]   worker-w003     Ready  ‚úì worker         R2    No       None [0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,024 [INFO]   worker-w004     Ready  ‚úì worker         R2    No       None [0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,290 [INFO]   worker-w005     Ready  ‚úì worker         R3    No       None [0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,503 [INFO]   worker-w006     Ready  ‚úì worker         R3    No       None [0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,503 [INFO] [0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: Legend:[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,503 [INFO]   ‚úì = Node is Ready[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,503 [INFO]   ‚ö†Ô∏è = Warning indicator (NotReady, Cordoned, or has simulated-failure taint)[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,503 [INFO] ============ DETAILED POD INFORMATION ============[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,503 [INFO] Running 'kubectl get pods -o wide' to show detailed pod placement:[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,618 [INFO]   NAMESPACE     NAME                                                       READY   STATUS      RESTARTS         AGE     IP                NODE          NOMINATED NODE   READINESS GATES[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,619 [INFO]   argo          argo-server-5c69cb69db-c8qj8                               1/1     Running     0                87s     192.168.240.163   worker-w003   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,619 [INFO]   argo          resilience-bench-ptngm-initialize-metrics-940927476        0/2     Completed   0                5d9h    192.168.221.125   master-m003   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,619 [INFO]   argo          resilience-bench-ptngm-run-health-check-1168793163         0/2     Completed   0                5d9h    192.168.221.67    master-m003   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,619 [INFO]   argo          resilience-bench-ptngm-run-health-check-1185570782         0/2     Completed   0                5d9h    192.168.221.66    master-m003   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,619 [INFO]   argo          resilience-bench-ptngm-run-health-check-1202348401         0/2     Completed   0                5d9h    192.168.221.122   master-m003   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,619 [INFO]   argo          resilience-bench-wf87f-initialize-metrics-3677018408       0/2     Completed   0                5d11h   192.168.221.100   master-m003   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,619 [INFO]   argo          resilience-bench-wf87f-run-health-check-2914150102         0/2     Completed   0                5d11h   192.168.221.105   master-m003   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,619 [INFO]   argo          resilience-bench-wf87f-run-health-check-3931060015         0/2     Completed   0                5d11h   192.168.221.104   master-m003   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,619 [INFO]   argo          resilience-bench-wf87f-run-health-check-3947837634         0/2     Completed   0                5d11h   192.168.221.98    master-m003   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,619 [INFO]   argo          resilience-bench-wf87f-run-health-check-3964615253         0/2     Completed   0                5d11h   192.168.221.103   master-m003   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,619 [INFO]   argo          resilience-bench-wf87f-run-node-simulation-4173146970      0/2     Completed   0                5d11h   192.168.221.101   master-m003   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,619 [INFO]   argo          resilience-bench-wf87f-run-rack-simulation-1356070019      0/2     Completed   0                5d11h   192.168.221.109   master-m003   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,619 [INFO]   argo          resilience-scaled-heft-pplkg-node-failure-sim-2665931194   2/2     Running     0                5m48s   192.168.221.69    master-m003   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,620 [INFO]   argo          workflow-controller-ccbd949dc-ghwvq                        1/1     Running     4 (4m18s ago)    50m     192.168.191.65    worker-w006   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,620 [INFO]   kube-system   calico-kube-controllers-7498b9bb4c-df4xv                   1/1     Running     1 (3m26s ago)    24m     192.168.195.242   worker-w002   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,620 [INFO]   kube-system   calico-node-4zhd4                                          1/1     Running     0                13d     192.168.56.105    worker-w002   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,620 [INFO]   kube-system   calico-node-75nx6                                          1/1     Running     0                13d     192.168.56.109    worker-w006   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,620 [INFO]   kube-system   calico-node-7lkdq                                          1/1     Running     0                13d     192.168.56.104    worker-w001   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,620 [INFO]   kube-system   calico-node-85f8c                                          1/1     Running     0                13d     192.168.56.102    master-m002   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,620 [INFO]   kube-system   calico-node-j8nb9                                          1/1     Running     0                13d     192.168.56.101    master-m001   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,620 [INFO]   kube-system   calico-node-lbcb2                                          1/1     Running     0                13d     192.168.56.106    worker-w003   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,620 [INFO]   kube-system   calico-node-wlr5v                                          1/1     Running     0                13d     192.168.56.107    worker-w004   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,620 [INFO]   kube-system   calico-node-xnzjw                                          1/1     Running     0                13d     192.168.56.103    master-m003   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,620 [INFO]   kube-system   calico-node-xsltn                                          1/1     Running     0                13d     192.168.56.108    worker-w005   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,620 [INFO]   kube-system   coredns-668d6bf9bc-8dqlk                                   1/1     Running     0                131m    192.168.15.200    worker-w005   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,620 [INFO]   kube-system   coredns-668d6bf9bc-8nn7w                                   1/1     Running     0                24m     192.168.132.179   worker-w001   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,620 [INFO]   kube-system   etcd-master-m001                                           1/1     Running     0                13d     192.168.56.101    master-m001   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,620 [INFO]   kube-system   kube-apiserver-master-m001                                 1/1     Running     1 (5d4h ago)     13d     192.168.56.101    master-m001   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,621 [INFO]   kube-system   kube-controller-manager-master-m001                        1/1     Running     16 (4m20s ago)   13d     192.168.56.101    master-m001   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,621 [INFO]   kube-system   kube-proxy-5lzhj                                           1/1     Running     0                13d     192.168.56.102    master-m002   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,621 [INFO]   kube-system   kube-proxy-7jqkv                                           1/1     Running     0                13d     192.168.56.109    worker-w006   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,621 [INFO]   kube-system   kube-proxy-cv5dt                                           1/1     Running     0                13d     192.168.56.108    worker-w005   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,621 [INFO]   kube-system   kube-proxy-fvpmr                                           1/1     Running     0                13d     192.168.56.101    master-m001   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,621 [INFO]   kube-system   kube-proxy-hgs5z                                           1/1     Running     0                13d     192.168.56.106    worker-w003   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,621 [INFO]   kube-system   kube-proxy-kmgqr                                           1/1     Running     0                13d     192.168.56.105    worker-w002   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,621 [INFO]   kube-system   kube-proxy-rdbz5                                           1/1     Running     0                13d     192.168.56.107    worker-w004   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,621 [INFO]   kube-system   kube-proxy-w8mnb                                           1/1     Running     0                13d     192.168.56.103    master-m003   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,621 [INFO]   kube-system   kube-proxy-x9jxr                                           1/1     Running     0                13d     192.168.56.104    worker-w001   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,621 [INFO]   kube-system   kube-scheduler-master-m001                                 1/1     Running     15 (4m15s ago)   13d     192.168.56.101    master-m001   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,621 [INFO] [0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: Pod distribution by node:[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,706 [INFO]   Node worker-w003: 3 pods[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,706 [INFO]   Node master-m003: 14 pods[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,706 [INFO]   Node 50m: 1 pods[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,706 [INFO]   Node 24m: 1 pods[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,706 [INFO]   Node worker-w002: 2 pods[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,706 [INFO]   Node worker-w006: 2 pods[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,706 [INFO]   Node worker-w001: 3 pods[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,707 [INFO]   Node master-m002: 2 pods[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,707 [INFO]   Node master-m001: 3 pods[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,707 [INFO]   Node worker-w004: 2 pods[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,707 [INFO]   Node worker-w005: 3 pods[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,707 [INFO]   Node 13d: 3 pods[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,707 [INFO] [0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: Filtering for simulation services:[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,824 [INFO] Node master-m001 is Ready[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,824 [INFO] Node master-m002 is Ready[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,824 [INFO] Node master-m003 is Ready[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,825 [INFO] Node worker-w001 is Ready[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,825 [INFO] Node worker-w002 is Ready[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,825 [INFO] Node worker-w003 is Ready[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,825 [INFO] Node worker-w004 is Ready[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,825 [INFO] Node worker-w005 is Ready[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,825 [INFO] Node worker-w006 is Ready[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,827 [WARNING] No pods found for etcd-sim[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,830 [WARNING] No pods found for postgres-sim[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,832 [WARNING] No pods found for redis-sim[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,834 [WARNING] No pods found for nginx-sim[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,836 [WARNING] No pods found for auth-sim[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,836 [INFO] Completed full health check[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:44,836 [INFO] Simulating node recovery for master-m002 using Kubernetes API[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:45,033 [INFO] Removed simulated-failure taint from node master-m002[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:46,134 [INFO] Node master-m002 uncordoned and ready[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:46,134 [INFO] Node master-m002 has been powered back on[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:51:46,135 [INFO] Waiting 60 seconds for the cluster to stabilize after recovery...[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:46,193 [INFO] Running final health check[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:46,193 [INFO] Starting full health check[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:46,194 [INFO] [0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: ============ DETAILED NODE STATUS ============[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:46,194 [INFO] Basic Node Information (kubectl get nodes -o wide):[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:46,433 [INFO]   NAME          STATUS   ROLES           AGE   VERSION    INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:46,433 [INFO]   master-m001   Ready    control-plane   13d   v1.32.11   192.168.56.101   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:46,433 [INFO]   master-m002   Ready    control-plane   13d   v1.32.11   192.168.56.102   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:46,433 [INFO]   master-m003   Ready    control-plane   13d   v1.32.11   192.168.56.103   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:46,433 [INFO]   worker-w001   Ready    <none>          13d   v1.32.11   192.168.56.104   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:46,434 [INFO]   worker-w002   Ready    <none>          13d   v1.32.11   192.168.56.105   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:46,434 [INFO]   worker-w003   Ready    <none>          13d   v1.32.11   192.168.56.106   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:46,434 [INFO]   worker-w004   Ready    <none>          13d   v1.32.11   192.168.56.107   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:46,434 [INFO]   worker-w005   Ready    <none>          13d   v1.32.11   192.168.56.108   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:46,434 [INFO]   worker-w006   Ready    <none>          13d   v1.32.11   192.168.56.109   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:46,434 [INFO] [0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: Enhanced Node Status (with taint and cordon indicators):[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:46,434 [INFO]   NAME                STATUS    ROLES           ZONE   CORDONED   TAINTS[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:46,700 [INFO]   master-m001     Ready  ‚úì worker         R1    No       node-role.kubernetes.io/control-plane [0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:46,944 [INFO]   master-m002     Ready  ‚úì worker         R2    No       None [0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:47,126 [INFO]   master-m003     Ready  ‚úì worker         R3    No       None [0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:47,317 [INFO]   worker-w001     Ready  ‚úì worker         R1    No       None [0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:47,509 [INFO]   worker-w002     Ready  ‚úì worker         R1    No       None [0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:47,751 [INFO]   worker-w003     Ready  ‚úì worker         R2    No       None [0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:47,936 [INFO]   worker-w004     Ready  ‚úì worker         R2    No       None [0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,141 [INFO]   worker-w005     Ready  ‚úì worker         R3    No       None [0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,404 [INFO]   worker-w006     Ready  ‚úì worker         R3    No       None [0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,405 [INFO] [0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: Legend:[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,405 [INFO]   ‚úì = Node is Ready[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,405 [INFO]   ‚ö†Ô∏è = Warning indicator (NotReady, Cordoned, or has simulated-failure taint)[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,405 [INFO] ============ DETAILED POD INFORMATION ============[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,406 [INFO] Running 'kubectl get pods -o wide' to show detailed pod placement:[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,544 [INFO]   NAMESPACE     NAME                                                       READY   STATUS      RESTARTS         AGE     IP                NODE          NOMINATED NODE   READINESS GATES[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,545 [INFO]   argo          argo-server-5c69cb69db-c8qj8                               1/1     Running     0                2m31s   192.168.240.163   worker-w003   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,545 [INFO]   argo          resilience-bench-ptngm-initialize-metrics-940927476        0/2     Completed   0                5d9h    192.168.221.125   master-m003   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,545 [INFO]   argo          resilience-bench-ptngm-run-health-check-1168793163         0/2     Completed   0                5d9h    192.168.221.67    master-m003   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,545 [INFO]   argo          resilience-bench-ptngm-run-health-check-1185570782         0/2     Completed   0                5d9h    192.168.221.66    master-m003   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,545 [INFO]   argo          resilience-bench-ptngm-run-health-check-1202348401         0/2     Completed   0                5d9h    192.168.221.122   master-m003   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,545 [INFO]   argo          resilience-bench-wf87f-initialize-metrics-3677018408       0/2     Completed   0                5d11h   192.168.221.100   master-m003   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,546 [INFO]   argo          resilience-bench-wf87f-run-health-check-2914150102         0/2     Completed   0                5d11h   192.168.221.105   master-m003   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,546 [INFO]   argo          resilience-bench-wf87f-run-health-check-3931060015         0/2     Completed   0                5d11h   192.168.221.104   master-m003   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,546 [INFO]   argo          resilience-bench-wf87f-run-health-check-3947837634         0/2     Completed   0                5d11h   192.168.221.98    master-m003   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,546 [INFO]   argo          resilience-bench-wf87f-run-health-check-3964615253         0/2     Completed   0                5d11h   192.168.221.103   master-m003   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,546 [INFO]   argo          resilience-bench-wf87f-run-node-simulation-4173146970      0/2     Completed   0                5d11h   192.168.221.101   master-m003   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,546 [INFO]   argo          resilience-bench-wf87f-run-rack-simulation-1356070019      0/2     Completed   0                5d11h   192.168.221.109   master-m003   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,546 [INFO]   argo          resilience-scaled-heft-pplkg-node-failure-sim-2665931194   2/2     Running     0                6m52s   192.168.221.69    master-m003   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,546 [INFO]   argo          workflow-controller-ccbd949dc-ghwvq                        1/1     Running     4 (5m22s ago)    51m     192.168.191.65    worker-w006   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,547 [INFO]   kube-system   calico-kube-controllers-7498b9bb4c-df4xv                   1/1     Running     1 (4m30s ago)    25m     192.168.195.242   worker-w002   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,547 [INFO]   kube-system   calico-node-4zhd4                                          1/1     Running     0                13d     192.168.56.105    worker-w002   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,547 [INFO]   kube-system   calico-node-75nx6                                          1/1     Running     0                13d     192.168.56.109    worker-w006   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,547 [INFO]   kube-system   calico-node-7lkdq                                          1/1     Running     0                13d     192.168.56.104    worker-w001   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,547 [INFO]   kube-system   calico-node-85f8c                                          1/1     Running     0                13d     192.168.56.102    master-m002   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,547 [INFO]   kube-system   calico-node-j8nb9                                          1/1     Running     0                13d     192.168.56.101    master-m001   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,547 [INFO]   kube-system   calico-node-lbcb2                                          1/1     Running     0                13d     192.168.56.106    worker-w003   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,547 [INFO]   kube-system   calico-node-wlr5v                                          1/1     Running     0                13d     192.168.56.107    worker-w004   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,547 [INFO]   kube-system   calico-node-xnzjw                                          1/1     Running     0                13d     192.168.56.103    master-m003   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,548 [INFO]   kube-system   calico-node-xsltn                                          1/1     Running     0                13d     192.168.56.108    worker-w005   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,548 [INFO]   kube-system   coredns-668d6bf9bc-8dqlk                                   1/1     Running     0                132m    192.168.15.200    worker-w005   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,548 [INFO]   kube-system   coredns-668d6bf9bc-8nn7w                                   1/1     Running     0                25m     192.168.132.179   worker-w001   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,548 [INFO]   kube-system   etcd-master-m001                                           1/1     Running     0                13d     192.168.56.101    master-m001   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,548 [INFO]   kube-system   kube-apiserver-master-m001                                 1/1     Running     1 (5d4h ago)     13d     192.168.56.101    master-m001   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,548 [INFO]   kube-system   kube-controller-manager-master-m001                        1/1     Running     16 (5m24s ago)   13d     192.168.56.101    master-m001   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,548 [INFO]   kube-system   kube-proxy-5lzhj                                           1/1     Running     0                13d     192.168.56.102    master-m002   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,548 [INFO]   kube-system   kube-proxy-7jqkv                                           1/1     Running     0                13d     192.168.56.109    worker-w006   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,549 [INFO]   kube-system   kube-proxy-cv5dt                                           1/1     Running     0                13d     192.168.56.108    worker-w005   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,549 [INFO]   kube-system   kube-proxy-fvpmr                                           1/1     Running     0                13d     192.168.56.101    master-m001   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,549 [INFO]   kube-system   kube-proxy-hgs5z                                           1/1     Running     0                13d     192.168.56.106    worker-w003   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,552 [INFO]   kube-system   kube-proxy-kmgqr                                           1/1     Running     0                13d     192.168.56.105    worker-w002   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,552 [INFO]   kube-system   kube-proxy-rdbz5                                           1/1     Running     0                13d     192.168.56.107    worker-w004   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,552 [INFO]   kube-system   kube-proxy-w8mnb                                           1/1     Running     0                13d     192.168.56.103    master-m003   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,552 [INFO]   kube-system   kube-proxy-x9jxr                                           1/1     Running     0                13d     192.168.56.104    worker-w001   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,552 [INFO]   kube-system   kube-scheduler-master-m001                                 1/1     Running     15 (5m19s ago)   13d     192.168.56.101    master-m001   <none>           <none>[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,552 [INFO] [0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: Pod distribution by node:[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,644 [INFO]   Node worker-w003: 3 pods[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,644 [INFO]   Node master-m003: 14 pods[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,644 [INFO]   Node 51m: 1 pods[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,644 [INFO]   Node 25m: 1 pods[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,644 [INFO]   Node worker-w002: 2 pods[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,645 [INFO]   Node worker-w006: 2 pods[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,645 [INFO]   Node worker-w001: 3 pods[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,645 [INFO]   Node master-m002: 2 pods[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,645 [INFO]   Node master-m001: 3 pods[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,645 [INFO]   Node worker-w004: 2 pods[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,645 [INFO]   Node worker-w005: 3 pods[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,645 [INFO]   Node 13d: 3 pods[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,645 [INFO] [0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: Filtering for simulation services:[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,743 [INFO] Node master-m001 is Ready[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,744 [INFO] Node master-m002 is Ready[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,744 [INFO] Node master-m003 is Ready[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,744 [INFO] Node worker-w001 is Ready[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,744 [INFO] Node worker-w002 is Ready[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,744 [INFO] Node worker-w003 is Ready[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,744 [INFO] Node worker-w004 is Ready[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,744 [INFO] Node worker-w005 is Ready[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,744 [INFO] Node worker-w006 is Ready[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,747 [WARNING] No pods found for etcd-sim[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,749 [WARNING] No pods found for postgres-sim[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,751 [WARNING] No pods found for redis-sim[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,753 [WARNING] No pods found for nginx-sim[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,755 [WARNING] No pods found for auth-sim[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: 2026-01-08 07:52:48,755 [INFO] Completed full health check[0m
[32mresilience-scaled-heft-pplkg-node-failure-sim-2665931194: time="2026-01-08T07:52:48.887Z" level=info msg="sub-process exited" argo=true error="<nil>"[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: Log directory created/verified: /app/logs[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: File logging configured successfully[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:53:11,573 [INFO] Loaded in-cluster Kubernetes config[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:53:11,573 [INFO] Running on host: resilience-scaled-heft-pplkg-node-failure-sim-2649153575[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:53:11,573 [INFO] Detected current node: master-m003, zone: R3[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:53:11,573 [INFO] Checking if we have permissions to modify nodes...[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:53:11,628 [INFO] Testing permissions using node: master-m001[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:53:11,636 [INFO] Permission check successful - we can modify nodes[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:53:11,637 [INFO] Using real Kubernetes API for node control[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:53:11,638 [INFO] Action received: simulate-node[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:53:11,638 [INFO] Stabilization time: 60 seconds[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:53:11,638 [INFO] Simulating node failure: worker-w003[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:53:11,638 [INFO] Simulating node failure for worker-w003 using Kubernetes API[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:53:11,652 [INFO] Node worker-w003 cordoned[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:53:11,664 [INFO] Node worker-w003 tainted with NoExecute[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:53:11,664 [INFO] Node worker-w003 powered off (delay 5s)[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:53:11,664 [INFO] Node worker-w003 down for 10 seconds[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:53:11,665 [INFO] Waiting 60 seconds for the cluster to stabilize before health check...[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:11,720 [INFO] Running health check after node power off[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:11,722 [INFO] Starting full health check[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:11,722 [INFO] [0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: ============ DETAILED NODE STATUS ============[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:11,722 [INFO] Basic Node Information (kubectl get nodes -o wide):[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:11,842 [INFO]   NAME          STATUS                     ROLES           AGE   VERSION    INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:11,842 [INFO]   master-m001   Ready                      control-plane   13d   v1.32.11   192.168.56.101   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:11,842 [INFO]   master-m002   Ready                      control-plane   13d   v1.32.11   192.168.56.102   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:11,842 [INFO]   master-m003   Ready                      control-plane   13d   v1.32.11   192.168.56.103   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:11,842 [INFO]   worker-w001   Ready                      <none>          13d   v1.32.11   192.168.56.104   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:11,843 [INFO]   worker-w002   Ready                      <none>          13d   v1.32.11   192.168.56.105   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:11,843 [INFO]   worker-w003   Ready,SchedulingDisabled   <none>          13d   v1.32.11   192.168.56.106   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:11,843 [INFO]   worker-w004   Ready                      <none>          13d   v1.32.11   192.168.56.107   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:11,843 [INFO]   worker-w005   Ready                      <none>          13d   v1.32.11   192.168.56.108   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:11,843 [INFO]   worker-w006   Ready                      <none>          13d   v1.32.11   192.168.56.109   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:11,843 [INFO] [0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: Enhanced Node Status (with taint and cordon indicators):[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:11,843 [INFO]   NAME                STATUS    ROLES           ZONE   CORDONED   TAINTS[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:12,314 [INFO]   master-m001     Ready  ‚úì worker         R1    No       node-role.kubernetes.io/control-plane [0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:12,585 [INFO]   master-m002     Ready  ‚úì worker         R2    No       None [0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:12,794 [INFO]   master-m003     Ready  ‚úì worker         R3    No       None [0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:12,978 [INFO]   worker-w001     Ready  ‚úì worker         R1    No       None [0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:13,163 [INFO]   worker-w002     Ready  ‚úì worker         R1    No       None [0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:13,354 [INFO]   worker-w003     Ready  ‚úì worker         R2    YES     ‚ö†Ô∏è simulated-failure, node.kubernetes.io/unschedulable ‚ö†Ô∏è[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:14,530 [INFO]   worker-w004     Ready  ‚úì worker         R2    No       None [0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:14,704 [INFO]   worker-w005     Ready  ‚úì worker         R3    No       None [0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,004 [INFO]   worker-w006     Ready  ‚úì worker         R3    No       None [0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,005 [INFO] [0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: Legend:[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,005 [INFO]   ‚úì = Node is Ready[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,005 [INFO]   ‚ö†Ô∏è = Warning indicator (NotReady, Cordoned, or has simulated-failure taint)[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,006 [INFO] ============ DETAILED POD INFORMATION ============[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,013 [INFO] Running 'kubectl get pods -o wide' to show detailed pod placement:[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,187 [INFO]   NAMESPACE     NAME                                                       READY   STATUS      RESTARTS         AGE     IP                NODE          NOMINATED NODE   READINESS GATES[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,187 [INFO]   argo          argo-server-5c69cb69db-qk7vj                               1/1     Running     0                64s     192.168.153.248   worker-w004   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,187 [INFO]   argo          resilience-bench-ptngm-initialize-metrics-940927476        0/2     Completed   0                5d9h    192.168.221.125   master-m003   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,187 [INFO]   argo          resilience-bench-ptngm-run-health-check-1168793163         0/2     Completed   0                5d9h    192.168.221.67    master-m003   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,188 [INFO]   argo          resilience-bench-ptngm-run-health-check-1185570782         0/2     Completed   0                5d9h    192.168.221.66    master-m003   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,188 [INFO]   argo          resilience-bench-ptngm-run-health-check-1202348401         0/2     Completed   0                5d9h    192.168.221.122   master-m003   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,188 [INFO]   argo          resilience-bench-wf87f-initialize-metrics-3677018408       0/2     Completed   0                5d11h   192.168.221.100   master-m003   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,188 [INFO]   argo          resilience-bench-wf87f-run-health-check-2914150102         0/2     Completed   0                5d11h   192.168.221.105   master-m003   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,188 [INFO]   argo          resilience-bench-wf87f-run-health-check-3931060015         0/2     Completed   0                5d11h   192.168.221.104   master-m003   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,188 [INFO]   argo          resilience-bench-wf87f-run-health-check-3947837634         0/2     Completed   0                5d11h   192.168.221.98    master-m003   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,188 [INFO]   argo          resilience-bench-wf87f-run-health-check-3964615253         0/2     Completed   0                5d11h   192.168.221.103   master-m003   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,188 [INFO]   argo          resilience-bench-wf87f-run-node-simulation-4173146970      0/2     Completed   0                5d11h   192.168.221.101   master-m003   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,188 [INFO]   argo          resilience-bench-wf87f-run-rack-simulation-1356070019      0/2     Completed   0                5d11h   192.168.221.109   master-m003   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,189 [INFO]   argo          resilience-scaled-heft-pplkg-node-failure-sim-2649153575   2/2     Running     0                76s     192.168.221.66    master-m003   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,189 [INFO]   argo          resilience-scaled-heft-pplkg-node-failure-sim-2665931194   0/2     Completed   0                8m19s   192.168.221.69    master-m003   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,189 [INFO]   argo          workflow-controller-ccbd949dc-ghwvq                        1/1     Running     4 (6m49s ago)    52m     192.168.191.65    worker-w006   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,189 [INFO]   kube-system   calico-kube-controllers-7498b9bb4c-df4xv                   1/1     Running     1 (5m57s ago)    26m     192.168.195.242   worker-w002   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,189 [INFO]   kube-system   calico-node-4zhd4                                          1/1     Running     0                13d     192.168.56.105    worker-w002   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,189 [INFO]   kube-system   calico-node-75nx6                                          1/1     Running     0                13d     192.168.56.109    worker-w006   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,189 [INFO]   kube-system   calico-node-7lkdq                                          1/1     Running     0                13d     192.168.56.104    worker-w001   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,189 [INFO]   kube-system   calico-node-85f8c                                          1/1     Running     0                13d     192.168.56.102    master-m002   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,189 [INFO]   kube-system   calico-node-j8nb9                                          1/1     Running     0                13d     192.168.56.101    master-m001   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,189 [INFO]   kube-system   calico-node-lbcb2                                          1/1     Running     0                13d     192.168.56.106    worker-w003   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,190 [INFO]   kube-system   calico-node-wlr5v                                          1/1     Running     0                13d     192.168.56.107    worker-w004   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,190 [INFO]   kube-system   calico-node-xnzjw                                          1/1     Running     0                13d     192.168.56.103    master-m003   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,190 [INFO]   kube-system   calico-node-xsltn                                          1/1     Running     0                13d     192.168.56.108    worker-w005   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,190 [INFO]   kube-system   coredns-668d6bf9bc-8dqlk                                   1/1     Running     0                133m    192.168.15.200    worker-w005   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,190 [INFO]   kube-system   coredns-668d6bf9bc-8nn7w                                   1/1     Running     0                26m     192.168.132.179   worker-w001   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,190 [INFO]   kube-system   etcd-master-m001                                           1/1     Running     0                13d     192.168.56.101    master-m001   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,191 [INFO]   kube-system   kube-apiserver-master-m001                                 1/1     Running     1 (5d4h ago)     13d     192.168.56.101    master-m001   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,191 [INFO]   kube-system   kube-controller-manager-master-m001                        1/1     Running     16 (6m51s ago)   13d     192.168.56.101    master-m001   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,191 [INFO]   kube-system   kube-proxy-5lzhj                                           1/1     Running     0                13d     192.168.56.102    master-m002   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,191 [INFO]   kube-system   kube-proxy-7jqkv                                           1/1     Running     0                13d     192.168.56.109    worker-w006   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,191 [INFO]   kube-system   kube-proxy-cv5dt                                           1/1     Running     0                13d     192.168.56.108    worker-w005   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,191 [INFO]   kube-system   kube-proxy-fvpmr                                           1/1     Running     0                13d     192.168.56.101    master-m001   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,191 [INFO]   kube-system   kube-proxy-hgs5z                                           1/1     Running     0                13d     192.168.56.106    worker-w003   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,191 [INFO]   kube-system   kube-proxy-kmgqr                                           1/1     Running     0                13d     192.168.56.105    worker-w002   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,192 [INFO]   kube-system   kube-proxy-rdbz5                                           1/1     Running     0                13d     192.168.56.107    worker-w004   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,192 [INFO]   kube-system   kube-proxy-w8mnb                                           1/1     Running     0                13d     192.168.56.103    master-m003   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,192 [INFO]   kube-system   kube-proxy-x9jxr                                           1/1     Running     0                13d     192.168.56.104    worker-w001   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,192 [INFO]   kube-system   kube-scheduler-master-m001                                 1/1     Running     15 (6m46s ago)   13d     192.168.56.101    master-m001   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,192 [INFO] [0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: Pod distribution by node:[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,341 [INFO]   Node worker-w004: 3 pods[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,342 [INFO]   Node master-m003: 15 pods[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,342 [INFO]   Node 52m: 1 pods[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,342 [INFO]   Node 26m: 1 pods[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,342 [INFO]   Node worker-w002: 2 pods[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,342 [INFO]   Node worker-w006: 2 pods[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,342 [INFO]   Node worker-w001: 3 pods[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,342 [INFO]   Node master-m002: 2 pods[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,342 [INFO]   Node master-m001: 3 pods[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,342 [INFO]   Node worker-w003: 2 pods[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,343 [INFO]   Node worker-w005: 3 pods[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,343 [INFO]   Node 13d: 3 pods[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,343 [INFO] [0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: Filtering for simulation services:[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,498 [INFO] Node master-m001 is Ready[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,499 [INFO] Node master-m002 is Ready[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,499 [INFO] Node master-m003 is Ready[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,500 [INFO] Node worker-w001 is Ready[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,500 [INFO] Node worker-w002 is Ready[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,500 [INFO] Node worker-w003 is Ready[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,500 [INFO] Node worker-w004 is Ready[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,500 [INFO] Node worker-w005 is Ready[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,500 [INFO] Node worker-w006 is Ready[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,504 [WARNING] No pods found for etcd-sim[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,506 [WARNING] No pods found for postgres-sim[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,508 [WARNING] No pods found for redis-sim[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,510 [WARNING] No pods found for nginx-sim[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,513 [WARNING] No pods found for auth-sim[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:15,513 [INFO] Completed full health check[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:25,522 [INFO] Running health check before node power on[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:25,522 [INFO] Starting full health check[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:25,523 [INFO] [0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: ============ DETAILED NODE STATUS ============[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:25,523 [INFO] Basic Node Information (kubectl get nodes -o wide):[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:25,657 [INFO]   NAME          STATUS                     ROLES           AGE   VERSION    INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:25,658 [INFO]   master-m001   Ready                      control-plane   13d   v1.32.11   192.168.56.101   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:25,658 [INFO]   master-m002   Ready                      control-plane   13d   v1.32.11   192.168.56.102   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:25,658 [INFO]   master-m003   Ready                      control-plane   13d   v1.32.11   192.168.56.103   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:25,658 [INFO]   worker-w001   Ready                      <none>          13d   v1.32.11   192.168.56.104   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:25,658 [INFO]   worker-w002   Ready                      <none>          13d   v1.32.11   192.168.56.105   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:25,658 [INFO]   worker-w003   Ready,SchedulingDisabled   <none>          13d   v1.32.11   192.168.56.106   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:25,658 [INFO]   worker-w004   Ready                      <none>          13d   v1.32.11   192.168.56.107   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:25,658 [INFO]   worker-w005   Ready                      <none>          13d   v1.32.11   192.168.56.108   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:25,659 [INFO]   worker-w006   Ready                      <none>          13d   v1.32.11   192.168.56.109   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:25,659 [INFO] [0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: Enhanced Node Status (with taint and cordon indicators):[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:25,659 [INFO]   NAME                STATUS    ROLES           ZONE   CORDONED   TAINTS[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:26,067 [INFO]   master-m001     Ready  ‚úì worker         R1    No       node-role.kubernetes.io/control-plane [0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:26,424 [INFO]   master-m002     Ready  ‚úì worker         R2    No       None [0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:26,804 [INFO]   master-m003     Ready  ‚úì worker         R3    No       None [0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:27,163 [INFO]   worker-w001     Ready  ‚úì worker         R1    No       None [0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:27,429 [INFO]   worker-w002     Ready  ‚úì worker         R1    No       None [0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:27,629 [INFO]   worker-w003     Ready  ‚úì worker         R2    YES     ‚ö†Ô∏è simulated-failure, node.kubernetes.io/unschedulable ‚ö†Ô∏è[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:27,956 [INFO]   worker-w004     Ready  ‚úì worker         R2    No       None [0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,255 [INFO]   worker-w005     Ready  ‚úì worker         R3    No       None [0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,525 [INFO]   worker-w006     Ready  ‚úì worker         R3    No       None [0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,525 [INFO] [0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: Legend:[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,525 [INFO]   ‚úì = Node is Ready[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,525 [INFO]   ‚ö†Ô∏è = Warning indicator (NotReady, Cordoned, or has simulated-failure taint)[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,526 [INFO] ============ DETAILED POD INFORMATION ============[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,526 [INFO] Running 'kubectl get pods -o wide' to show detailed pod placement:[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,670 [INFO]   NAMESPACE     NAME                                                       READY   STATUS      RESTARTS         AGE     IP                NODE          NOMINATED NODE   READINESS GATES[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,670 [INFO]   argo          argo-server-5c69cb69db-qk7vj                               1/1     Running     0                77s     192.168.153.248   worker-w004   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,670 [INFO]   argo          resilience-bench-ptngm-initialize-metrics-940927476        0/2     Completed   0                5d9h    192.168.221.125   master-m003   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,670 [INFO]   argo          resilience-bench-ptngm-run-health-check-1168793163         0/2     Completed   0                5d9h    192.168.221.67    master-m003   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,670 [INFO]   argo          resilience-bench-ptngm-run-health-check-1185570782         0/2     Completed   0                5d9h    192.168.221.66    master-m003   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,671 [INFO]   argo          resilience-bench-ptngm-run-health-check-1202348401         0/2     Completed   0                5d9h    192.168.221.122   master-m003   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,671 [INFO]   argo          resilience-bench-wf87f-initialize-metrics-3677018408       0/2     Completed   0                5d11h   192.168.221.100   master-m003   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,671 [INFO]   argo          resilience-bench-wf87f-run-health-check-2914150102         0/2     Completed   0                5d11h   192.168.221.105   master-m003   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,671 [INFO]   argo          resilience-bench-wf87f-run-health-check-3931060015         0/2     Completed   0                5d11h   192.168.221.104   master-m003   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,671 [INFO]   argo          resilience-bench-wf87f-run-health-check-3947837634         0/2     Completed   0                5d11h   192.168.221.98    master-m003   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,671 [INFO]   argo          resilience-bench-wf87f-run-health-check-3964615253         0/2     Completed   0                5d11h   192.168.221.103   master-m003   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,671 [INFO]   argo          resilience-bench-wf87f-run-node-simulation-4173146970      0/2     Completed   0                5d11h   192.168.221.101   master-m003   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,671 [INFO]   argo          resilience-bench-wf87f-run-rack-simulation-1356070019      0/2     Completed   0                5d11h   192.168.221.109   master-m003   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,672 [INFO]   argo          resilience-scaled-heft-pplkg-node-failure-sim-2649153575   2/2     Running     0                89s     192.168.221.66    master-m003   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,672 [INFO]   argo          resilience-scaled-heft-pplkg-node-failure-sim-2665931194   0/2     Completed   0                8m32s   192.168.221.69    master-m003   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,672 [INFO]   argo          workflow-controller-ccbd949dc-ghwvq                        1/1     Running     4 (7m2s ago)     53m     192.168.191.65    worker-w006   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,672 [INFO]   kube-system   calico-kube-controllers-7498b9bb4c-df4xv                   1/1     Running     1 (6m10s ago)    27m     192.168.195.242   worker-w002   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,672 [INFO]   kube-system   calico-node-4zhd4                                          1/1     Running     0                13d     192.168.56.105    worker-w002   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,672 [INFO]   kube-system   calico-node-75nx6                                          1/1     Running     0                13d     192.168.56.109    worker-w006   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,672 [INFO]   kube-system   calico-node-7lkdq                                          1/1     Running     0                13d     192.168.56.104    worker-w001   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,673 [INFO]   kube-system   calico-node-85f8c                                          1/1     Running     0                13d     192.168.56.102    master-m002   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,673 [INFO]   kube-system   calico-node-j8nb9                                          1/1     Running     0                13d     192.168.56.101    master-m001   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,673 [INFO]   kube-system   calico-node-lbcb2                                          1/1     Running     0                13d     192.168.56.106    worker-w003   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,673 [INFO]   kube-system   calico-node-wlr5v                                          1/1     Running     0                13d     192.168.56.107    worker-w004   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,673 [INFO]   kube-system   calico-node-xnzjw                                          1/1     Running     0                13d     192.168.56.103    master-m003   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,673 [INFO]   kube-system   calico-node-xsltn                                          1/1     Running     0                13d     192.168.56.108    worker-w005   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,673 [INFO]   kube-system   coredns-668d6bf9bc-8dqlk                                   1/1     Running     0                133m    192.168.15.200    worker-w005   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,673 [INFO]   kube-system   coredns-668d6bf9bc-8nn7w                                   1/1     Running     0                27m     192.168.132.179   worker-w001   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,673 [INFO]   kube-system   etcd-master-m001                                           1/1     Running     0                13d     192.168.56.101    master-m001   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,673 [INFO]   kube-system   kube-apiserver-master-m001                                 1/1     Running     1 (5d4h ago)     13d     192.168.56.101    master-m001   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,674 [INFO]   kube-system   kube-controller-manager-master-m001                        1/1     Running     16 (7m4s ago)    13d     192.168.56.101    master-m001   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,674 [INFO]   kube-system   kube-proxy-5lzhj                                           1/1     Running     0                13d     192.168.56.102    master-m002   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,674 [INFO]   kube-system   kube-proxy-7jqkv                                           1/1     Running     0                13d     192.168.56.109    worker-w006   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,674 [INFO]   kube-system   kube-proxy-cv5dt                                           1/1     Running     0                13d     192.168.56.108    worker-w005   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,674 [INFO]   kube-system   kube-proxy-fvpmr                                           1/1     Running     0                13d     192.168.56.101    master-m001   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,674 [INFO]   kube-system   kube-proxy-hgs5z                                           1/1     Running     0                13d     192.168.56.106    worker-w003   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,674 [INFO]   kube-system   kube-proxy-kmgqr                                           1/1     Running     0                13d     192.168.56.105    worker-w002   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,674 [INFO]   kube-system   kube-proxy-rdbz5                                           1/1     Running     0                13d     192.168.56.107    worker-w004   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,674 [INFO]   kube-system   kube-proxy-w8mnb                                           1/1     Running     0                13d     192.168.56.103    master-m003   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,674 [INFO]   kube-system   kube-proxy-x9jxr                                           1/1     Running     0                13d     192.168.56.104    worker-w001   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,674 [INFO]   kube-system   kube-scheduler-master-m001                                 1/1     Running     15 (6m59s ago)   13d     192.168.56.101    master-m001   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,674 [INFO] [0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: Pod distribution by node:[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,789 [INFO]   Node worker-w004: 3 pods[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,793 [INFO]   Node master-m003: 15 pods[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,793 [INFO]   Node 53m: 1 pods[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,793 [INFO]   Node 27m: 1 pods[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,793 [INFO]   Node worker-w002: 2 pods[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,793 [INFO]   Node worker-w006: 2 pods[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,793 [INFO]   Node worker-w001: 3 pods[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,793 [INFO]   Node master-m002: 2 pods[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,793 [INFO]   Node master-m001: 3 pods[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,793 [INFO]   Node worker-w003: 2 pods[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,794 [INFO]   Node worker-w005: 3 pods[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,794 [INFO]   Node 13d: 3 pods[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,794 [INFO] [0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: Filtering for simulation services:[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,915 [INFO] Node master-m001 is Ready[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,915 [INFO] Node master-m002 is Ready[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,915 [INFO] Node master-m003 is Ready[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,915 [INFO] Node worker-w001 is Ready[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,915 [INFO] Node worker-w002 is Ready[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,915 [INFO] Node worker-w003 is Ready[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,916 [INFO] Node worker-w004 is Ready[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,916 [INFO] Node worker-w005 is Ready[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,916 [INFO] Node worker-w006 is Ready[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,918 [WARNING] No pods found for etcd-sim[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,921 [WARNING] No pods found for postgres-sim[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,923 [WARNING] No pods found for redis-sim[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,925 [WARNING] No pods found for nginx-sim[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,927 [WARNING] No pods found for auth-sim[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,927 [INFO] Completed full health check[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:28,927 [INFO] Simulating node recovery for worker-w003 using Kubernetes API[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:29,047 [INFO] Removed simulated-failure taint from node worker-w003[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:29,149 [INFO] Node worker-w003 uncordoned and ready[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:29,149 [INFO] Node worker-w003 has been powered back on[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:54:29,149 [INFO] Waiting 60 seconds for the cluster to stabilize after recovery...[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:29,205 [INFO] Running final health check[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:29,206 [INFO] Starting full health check[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:29,206 [INFO] [0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: ============ DETAILED NODE STATUS ============[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:29,206 [INFO] Basic Node Information (kubectl get nodes -o wide):[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:29,299 [INFO]   NAME          STATUS   ROLES           AGE   VERSION    INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:29,300 [INFO]   master-m001   Ready    control-plane   13d   v1.32.11   192.168.56.101   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:29,300 [INFO]   master-m002   Ready    control-plane   13d   v1.32.11   192.168.56.102   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:29,300 [INFO]   master-m003   Ready    control-plane   13d   v1.32.11   192.168.56.103   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:29,300 [INFO]   worker-w001   Ready    <none>          13d   v1.32.11   192.168.56.104   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:29,300 [INFO]   worker-w002   Ready    <none>          13d   v1.32.11   192.168.56.105   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:29,300 [INFO]   worker-w003   Ready    <none>          13d   v1.32.11   192.168.56.106   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:29,300 [INFO]   worker-w004   Ready    <none>          13d   v1.32.11   192.168.56.107   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:29,300 [INFO]   worker-w005   Ready    <none>          13d   v1.32.11   192.168.56.108   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:29,300 [INFO]   worker-w006   Ready    <none>          13d   v1.32.11   192.168.56.109   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:29,300 [INFO] [0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: Enhanced Node Status (with taint and cordon indicators):[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:29,300 [INFO]   NAME                STATUS    ROLES           ZONE   CORDONED   TAINTS[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:29,690 [INFO]   master-m001     Ready  ‚úì worker         R1    No       node-role.kubernetes.io/control-plane [0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:29,965 [INFO]   master-m002     Ready  ‚úì worker         R2    No       None [0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:30,231 [INFO]   master-m003     Ready  ‚úì worker         R3    No       None [0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:30,508 [INFO]   worker-w001     Ready  ‚úì worker         R1    No       None [0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:30,763 [INFO]   worker-w002     Ready  ‚úì worker         R1    No       None [0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:31,067 [INFO]   worker-w003     Ready  ‚úì worker         R2    No       None [0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:31,338 [INFO]   worker-w004     Ready  ‚úì worker         R2    No       None [0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:31,624 [INFO]   worker-w005     Ready  ‚úì worker         R3    No       None [0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:31,955 [INFO]   worker-w006     Ready  ‚úì worker         R3    No       None [0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:31,956 [INFO] [0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: Legend:[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:31,956 [INFO]   ‚úì = Node is Ready[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:31,956 [INFO]   ‚ö†Ô∏è = Warning indicator (NotReady, Cordoned, or has simulated-failure taint)[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:31,956 [INFO] ============ DETAILED POD INFORMATION ============[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:31,956 [INFO] Running 'kubectl get pods -o wide' to show detailed pod placement:[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,093 [INFO]   NAMESPACE     NAME                                                       READY   STATUS      RESTARTS        AGE     IP                NODE          NOMINATED NODE   READINESS GATES[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,093 [INFO]   argo          argo-server-5c69cb69db-qk7vj                               1/1     Running     0               2m21s   192.168.153.248   worker-w004   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,093 [INFO]   argo          resilience-bench-ptngm-initialize-metrics-940927476        0/2     Completed   0               5d9h    192.168.221.125   master-m003   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,093 [INFO]   argo          resilience-bench-ptngm-run-health-check-1168793163         0/2     Completed   0               5d9h    192.168.221.67    master-m003   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,094 [INFO]   argo          resilience-bench-ptngm-run-health-check-1185570782         0/2     Completed   0               5d9h    192.168.221.66    master-m003   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,094 [INFO]   argo          resilience-bench-ptngm-run-health-check-1202348401         0/2     Completed   0               5d9h    192.168.221.122   master-m003   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,096 [INFO]   argo          resilience-bench-wf87f-initialize-metrics-3677018408       0/2     Completed   0               5d11h   192.168.221.100   master-m003   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,096 [INFO]   argo          resilience-bench-wf87f-run-health-check-2914150102         0/2     Completed   0               5d11h   192.168.221.105   master-m003   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,096 [INFO]   argo          resilience-bench-wf87f-run-health-check-3931060015         0/2     Completed   0               5d11h   192.168.221.104   master-m003   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,096 [INFO]   argo          resilience-bench-wf87f-run-health-check-3947837634         0/2     Completed   0               5d11h   192.168.221.98    master-m003   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,096 [INFO]   argo          resilience-bench-wf87f-run-health-check-3964615253         0/2     Completed   0               5d11h   192.168.221.103   master-m003   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,096 [INFO]   argo          resilience-bench-wf87f-run-node-simulation-4173146970      0/2     Completed   0               5d11h   192.168.221.101   master-m003   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,096 [INFO]   argo          resilience-bench-wf87f-run-rack-simulation-1356070019      0/2     Completed   0               5d11h   192.168.221.109   master-m003   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,097 [INFO]   argo          resilience-scaled-heft-pplkg-node-failure-sim-2649153575   2/2     Running     0               2m33s   192.168.221.66    master-m003   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,097 [INFO]   argo          resilience-scaled-heft-pplkg-node-failure-sim-2665931194   0/2     Completed   0               9m36s   192.168.221.69    master-m003   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,097 [INFO]   argo          workflow-controller-ccbd949dc-ghwvq                        1/1     Running     4 (8m6s ago)    54m     192.168.191.65    worker-w006   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,097 [INFO]   kube-system   calico-kube-controllers-7498b9bb4c-df4xv                   1/1     Running     1 (7m14s ago)   28m     192.168.195.242   worker-w002   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,097 [INFO]   kube-system   calico-node-4zhd4                                          1/1     Running     0               13d     192.168.56.105    worker-w002   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,097 [INFO]   kube-system   calico-node-75nx6                                          1/1     Running     0               13d     192.168.56.109    worker-w006   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,098 [INFO]   kube-system   calico-node-7lkdq                                          1/1     Running     0               13d     192.168.56.104    worker-w001   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,098 [INFO]   kube-system   calico-node-85f8c                                          1/1     Running     0               13d     192.168.56.102    master-m002   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,098 [INFO]   kube-system   calico-node-j8nb9                                          1/1     Running     0               13d     192.168.56.101    master-m001   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,098 [INFO]   kube-system   calico-node-lbcb2                                          1/1     Running     0               13d     192.168.56.106    worker-w003   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,098 [INFO]   kube-system   calico-node-wlr5v                                          1/1     Running     0               13d     192.168.56.107    worker-w004   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,098 [INFO]   kube-system   calico-node-xnzjw                                          1/1     Running     0               13d     192.168.56.103    master-m003   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,098 [INFO]   kube-system   calico-node-xsltn                                          1/1     Running     0               13d     192.168.56.108    worker-w005   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,098 [INFO]   kube-system   coredns-668d6bf9bc-8dqlk                                   1/1     Running     0               135m    192.168.15.200    worker-w005   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,098 [INFO]   kube-system   coredns-668d6bf9bc-8nn7w                                   1/1     Running     0               28m     192.168.132.179   worker-w001   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,099 [INFO]   kube-system   etcd-master-m001                                           1/1     Running     0               13d     192.168.56.101    master-m001   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,099 [INFO]   kube-system   kube-apiserver-master-m001                                 1/1     Running     1 (5d4h ago)    13d     192.168.56.101    master-m001   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,099 [INFO]   kube-system   kube-controller-manager-master-m001                        1/1     Running     16 (8m8s ago)   13d     192.168.56.101    master-m001   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,099 [INFO]   kube-system   kube-proxy-5lzhj                                           1/1     Running     0               13d     192.168.56.102    master-m002   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,099 [INFO]   kube-system   kube-proxy-7jqkv                                           1/1     Running     0               13d     192.168.56.109    worker-w006   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,099 [INFO]   kube-system   kube-proxy-cv5dt                                           1/1     Running     0               13d     192.168.56.108    worker-w005   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,099 [INFO]   kube-system   kube-proxy-fvpmr                                           1/1     Running     0               13d     192.168.56.101    master-m001   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,099 [INFO]   kube-system   kube-proxy-hgs5z                                           1/1     Running     0               13d     192.168.56.106    worker-w003   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,099 [INFO]   kube-system   kube-proxy-kmgqr                                           1/1     Running     0               13d     192.168.56.105    worker-w002   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,100 [INFO]   kube-system   kube-proxy-rdbz5                                           1/1     Running     0               13d     192.168.56.107    worker-w004   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,100 [INFO]   kube-system   kube-proxy-w8mnb                                           1/1     Running     0               13d     192.168.56.103    master-m003   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,100 [INFO]   kube-system   kube-proxy-x9jxr                                           1/1     Running     0               13d     192.168.56.104    worker-w001   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,100 [INFO]   kube-system   kube-scheduler-master-m001                                 1/1     Running     15 (8m3s ago)   13d     192.168.56.101    master-m001   <none>           <none>[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,101 [INFO] [0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: Pod distribution by node:[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,271 [INFO]   Node worker-w004: 3 pods[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,271 [INFO]   Node master-m003: 15 pods[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,272 [INFO]   Node 54m: 1 pods[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,272 [INFO]   Node 28m: 1 pods[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,272 [INFO]   Node worker-w002: 2 pods[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,272 [INFO]   Node worker-w006: 2 pods[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,272 [INFO]   Node worker-w001: 3 pods[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,272 [INFO]   Node master-m002: 2 pods[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,272 [INFO]   Node master-m001: 3 pods[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,272 [INFO]   Node worker-w003: 2 pods[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,272 [INFO]   Node worker-w005: 3 pods[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,272 [INFO]   Node 13d: 3 pods[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,273 [INFO] [0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: Filtering for simulation services:[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,460 [INFO] Node master-m001 is Ready[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,460 [INFO] Node master-m002 is Ready[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,460 [INFO] Node master-m003 is Ready[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,460 [INFO] Node worker-w001 is Ready[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,460 [INFO] Node worker-w002 is Ready[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,460 [INFO] Node worker-w003 is Ready[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,460 [INFO] Node worker-w004 is Ready[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,461 [INFO] Node worker-w005 is Ready[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,461 [INFO] Node worker-w006 is Ready[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,463 [WARNING] No pods found for etcd-sim[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,498 [WARNING] No pods found for postgres-sim[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,512 [WARNING] No pods found for redis-sim[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,514 [WARNING] No pods found for nginx-sim[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,517 [WARNING] No pods found for auth-sim[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: 2026-01-08 07:55:32,517 [INFO] Completed full health check[0m
[33mresilience-scaled-heft-pplkg-node-failure-sim-2649153575: time="2026-01-08T07:55:34.330Z" level=info msg="sub-process exited" argo=true error="<nil>"[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: Log directory created/verified: /app/logs[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: File logging configured successfully[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:55:55,879 [INFO] Loaded in-cluster Kubernetes config[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:55:56,122 [INFO] Running on host: resilience-scaled-heft-pplkg-health-check-2427202394[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:55:56,123 [INFO] Detected current node: master-m003, zone: R3[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:55:56,123 [INFO] Checking if we have permissions to modify nodes...[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:55:56,446 [INFO] Testing permissions using node: master-m001[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:55:56,591 [INFO] Permission check successful - we can modify nodes[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:55:56,591 [INFO] Using real Kubernetes API for node control[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:55:56,594 [INFO] Action received: health-check[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:55:56,594 [INFO] Stabilization time: 10 seconds[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:55:56,594 [INFO] Starting full health check[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:55:56,594 [INFO] [0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: ============ DETAILED NODE STATUS ============[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:55:56,594 [INFO] Basic Node Information (kubectl get nodes -o wide):[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:55:59,505 [INFO]   NAME          STATUS   ROLES           AGE   VERSION    INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:55:59,567 [INFO]   master-m001   Ready    control-plane   13d   v1.32.11   192.168.56.101   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:55:59,567 [INFO]   master-m002   Ready    control-plane   13d   v1.32.11   192.168.56.102   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:55:59,567 [INFO]   master-m003   Ready    control-plane   13d   v1.32.11   192.168.56.103   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:55:59,567 [INFO]   worker-w001   Ready    <none>          13d   v1.32.11   192.168.56.104   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:55:59,567 [INFO]   worker-w002   Ready    <none>          13d   v1.32.11   192.168.56.105   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:55:59,568 [INFO]   worker-w003   Ready    <none>          13d   v1.32.11   192.168.56.106   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:55:59,568 [INFO]   worker-w004   Ready    <none>          13d   v1.32.11   192.168.56.107   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:55:59,568 [INFO]   worker-w005   Ready    <none>          13d   v1.32.11   192.168.56.108   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:55:59,568 [INFO]   worker-w006   Ready    <none>          13d   v1.32.11   192.168.56.109   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:55:59,568 [INFO] [0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: Enhanced Node Status (with taint and cordon indicators):[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:55:59,568 [INFO]   NAME                STATUS    ROLES           ZONE   CORDONED   TAINTS[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:00,018 [INFO]   master-m001     Ready  ‚úì worker         R1    No       node-role.kubernetes.io/control-plane [0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:00,305 [INFO]   master-m002     Ready  ‚úì worker         R2    No       None [0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:00,620 [INFO]   master-m003     Ready  ‚úì worker         R3    No       None [0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:00,917 [INFO]   worker-w001     Ready  ‚úì worker         R1    No       None [0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:01,219 [INFO]   worker-w002     Ready  ‚úì worker         R1    No       None [0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:01,568 [INFO]   worker-w003     Ready  ‚úì worker         R2    No       None [0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:01,944 [INFO]   worker-w004     Ready  ‚úì worker         R2    No       None [0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,279 [INFO]   worker-w005     Ready  ‚úì worker         R3    No       None [0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,578 [INFO]   worker-w006     Ready  ‚úì worker         R3    No       None [0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,579 [INFO] [0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: Legend:[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,579 [INFO]   ‚úì = Node is Ready[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,579 [INFO]   ‚ö†Ô∏è = Warning indicator (NotReady, Cordoned, or has simulated-failure taint)[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,579 [INFO] ============ DETAILED POD INFORMATION ============[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,579 [INFO] Running 'kubectl get pods -o wide' to show detailed pod placement:[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,754 [INFO]   NAMESPACE     NAME                                                       READY   STATUS      RESTARTS         AGE     IP                NODE          NOMINATED NODE   READINESS GATES[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,755 [INFO]   argo          argo-server-5c69cb69db-qk7vj                               1/1     Running     0                2m51s   192.168.153.248   worker-w004   <none>           <none>[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,755 [INFO]   argo          resilience-bench-ptngm-initialize-metrics-940927476        0/2     Completed   0                5d9h    192.168.221.125   master-m003   <none>           <none>[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,755 [INFO]   argo          resilience-bench-ptngm-run-health-check-1168793163         0/2     Completed   0                5d9h    192.168.221.67    master-m003   <none>           <none>[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,755 [INFO]   argo          resilience-bench-ptngm-run-health-check-1185570782         0/2     Completed   0                5d9h    192.168.221.66    master-m003   <none>           <none>[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,755 [INFO]   argo          resilience-bench-ptngm-run-health-check-1202348401         0/2     Completed   0                5d9h    192.168.221.122   master-m003   <none>           <none>[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,755 [INFO]   argo          resilience-bench-wf87f-initialize-metrics-3677018408       0/2     Completed   0                5d11h   192.168.221.100   master-m003   <none>           <none>[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,755 [INFO]   argo          resilience-bench-wf87f-run-health-check-2914150102         0/2     Completed   0                5d11h   192.168.221.105   master-m003   <none>           <none>[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,755 [INFO]   argo          resilience-bench-wf87f-run-health-check-3931060015         0/2     Completed   0                5d11h   192.168.221.104   master-m003   <none>           <none>[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,755 [INFO]   argo          resilience-bench-wf87f-run-health-check-3947837634         0/2     Completed   0                5d11h   192.168.221.98    master-m003   <none>           <none>[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,755 [INFO]   argo          resilience-bench-wf87f-run-health-check-3964615253         0/2     Completed   0                5d11h   192.168.221.103   master-m003   <none>           <none>[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,755 [INFO]   argo          resilience-bench-wf87f-run-node-simulation-4173146970      0/2     Completed   0                5d11h   192.168.221.101   master-m003   <none>           <none>[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,755 [INFO]   argo          resilience-bench-wf87f-run-rack-simulation-1356070019      0/2     Completed   0                5d11h   192.168.221.109   master-m003   <none>           <none>[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,755 [INFO]   argo          resilience-scaled-heft-pplkg-health-check-2427202394       2/2     Running     0                17s     192.168.221.122   master-m003   <none>           <none>[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,755 [INFO]   argo          resilience-scaled-heft-pplkg-node-failure-sim-2649153575   0/2     Completed   0                3m3s    192.168.221.66    master-m003   <none>           <none>[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,755 [INFO]   argo          resilience-scaled-heft-pplkg-node-failure-sim-2665931194   0/2     Completed   0                10m     192.168.221.69    master-m003   <none>           <none>[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,755 [INFO]   argo          workflow-controller-ccbd949dc-ghwvq                        1/1     Running     4 (8m36s ago)    54m     192.168.191.65    worker-w006   <none>           <none>[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,755 [INFO]   kube-system   calico-kube-controllers-7498b9bb4c-df4xv                   1/1     Running     1 (7m44s ago)    28m     192.168.195.242   worker-w002   <none>           <none>[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,756 [INFO]   kube-system   calico-node-4zhd4                                          1/1     Running     0                13d     192.168.56.105    worker-w002   <none>           <none>[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,756 [INFO]   kube-system   calico-node-75nx6                                          1/1     Running     0                13d     192.168.56.109    worker-w006   <none>           <none>[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,756 [INFO]   kube-system   calico-node-7lkdq                                          1/1     Running     0                13d     192.168.56.104    worker-w001   <none>           <none>[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,756 [INFO]   kube-system   calico-node-85f8c                                          1/1     Running     0                13d     192.168.56.102    master-m002   <none>           <none>[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,756 [INFO]   kube-system   calico-node-j8nb9                                          1/1     Running     0                13d     192.168.56.101    master-m001   <none>           <none>[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,756 [INFO]   kube-system   calico-node-lbcb2                                          1/1     Running     0                13d     192.168.56.106    worker-w003   <none>           <none>[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,756 [INFO]   kube-system   calico-node-wlr5v                                          1/1     Running     0                13d     192.168.56.107    worker-w004   <none>           <none>[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,756 [INFO]   kube-system   calico-node-xnzjw                                          1/1     Running     0                13d     192.168.56.103    master-m003   <none>           <none>[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,757 [INFO]   kube-system   calico-node-xsltn                                          1/1     Running     0                13d     192.168.56.108    worker-w005   <none>           <none>[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,757 [INFO]   kube-system   coredns-668d6bf9bc-8dqlk                                   1/1     Running     0                135m    192.168.15.200    worker-w005   <none>           <none>[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,757 [INFO]   kube-system   coredns-668d6bf9bc-8nn7w                                   1/1     Running     0                28m     192.168.132.179   worker-w001   <none>           <none>[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,757 [INFO]   kube-system   etcd-master-m001                                           1/1     Running     0                13d     192.168.56.101    master-m001   <none>           <none>[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,757 [INFO]   kube-system   kube-apiserver-master-m001                                 1/1     Running     1 (5d4h ago)     13d     192.168.56.101    master-m001   <none>           <none>[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,757 [INFO]   kube-system   kube-controller-manager-master-m001                        1/1     Running     16 (8m38s ago)   13d     192.168.56.101    master-m001   <none>           <none>[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,757 [INFO]   kube-system   kube-proxy-5lzhj                                           1/1     Running     0                13d     192.168.56.102    master-m002   <none>           <none>[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,757 [INFO]   kube-system   kube-proxy-7jqkv                                           1/1     Running     0                13d     192.168.56.109    worker-w006   <none>           <none>[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,757 [INFO]   kube-system   kube-proxy-cv5dt                                           1/1     Running     0                13d     192.168.56.108    worker-w005   <none>           <none>[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,757 [INFO]   kube-system   kube-proxy-fvpmr                                           1/1     Running     0                13d     192.168.56.101    master-m001   <none>           <none>[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,757 [INFO]   kube-system   kube-proxy-hgs5z                                           1/1     Running     0                13d     192.168.56.106    worker-w003   <none>           <none>[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,757 [INFO]   kube-system   kube-proxy-kmgqr                                           1/1     Running     0                13d     192.168.56.105    worker-w002   <none>           <none>[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,757 [INFO]   kube-system   kube-proxy-rdbz5                                           1/1     Running     0                13d     192.168.56.107    worker-w004   <none>           <none>[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,757 [INFO]   kube-system   kube-proxy-w8mnb                                           1/1     Running     0                13d     192.168.56.103    master-m003   <none>           <none>[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,757 [INFO]   kube-system   kube-proxy-x9jxr                                           1/1     Running     0                13d     192.168.56.104    worker-w001   <none>           <none>[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,757 [INFO]   kube-system   kube-scheduler-master-m001                                 1/1     Running     15 (8m33s ago)   13d     192.168.56.101    master-m001   <none>           <none>[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,758 [INFO] [0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: Pod distribution by node:[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,899 [INFO]   Node worker-w004: 3 pods[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,899 [INFO]   Node master-m003: 16 pods[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,899 [INFO]   Node 54m: 1 pods[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,899 [INFO]   Node 28m: 1 pods[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,899 [INFO]   Node worker-w002: 2 pods[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,899 [INFO]   Node worker-w006: 2 pods[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,899 [INFO]   Node worker-w001: 3 pods[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,900 [INFO]   Node master-m002: 2 pods[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,900 [INFO]   Node master-m001: 3 pods[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,900 [INFO]   Node worker-w003: 2 pods[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,900 [INFO]   Node worker-w005: 3 pods[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,900 [INFO]   Node 13d: 3 pods[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:02,900 [INFO] [0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: Filtering for simulation services:[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:03,182 [INFO] Node master-m001 is Ready[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:03,182 [INFO] Node master-m002 is Ready[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:03,182 [INFO] Node master-m003 is Ready[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:03,182 [INFO] Node worker-w001 is Ready[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:03,182 [INFO] Node worker-w002 is Ready[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:03,182 [INFO] Node worker-w003 is Ready[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:03,182 [INFO] Node worker-w004 is Ready[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:03,182 [INFO] Node worker-w005 is Ready[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:03,182 [INFO] Node worker-w006 is Ready[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:03,190 [WARNING] No pods found for etcd-sim[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:03,192 [WARNING] No pods found for postgres-sim[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:03,194 [WARNING] No pods found for redis-sim[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:03,197 [WARNING] No pods found for nginx-sim[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:03,199 [WARNING] No pods found for auth-sim[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: 2026-01-08 07:56:03,199 [INFO] Completed full health check[0m
[35mresilience-scaled-heft-pplkg-health-check-2427202394: time="2026-01-08T07:56:04.922Z" level=info msg="sub-process exited" argo=true error="<nil>"[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: Log directory created/verified: /app/logs[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: File logging configured successfully[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:30,075 [INFO] Loaded in-cluster Kubernetes config[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:30,075 [INFO] Running on host: resilience-scaled-heft-pplkg-health-check-2410424775[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:30,075 [INFO] Detected current node: master-m003, zone: R3[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:30,075 [INFO] Checking if we have permissions to modify nodes...[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:30,100 [INFO] Testing permissions using node: master-m001[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:30,107 [INFO] Permission check successful - we can modify nodes[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:30,110 [INFO] Using real Kubernetes API for node control[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:30,111 [INFO] Action received: health-check[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:30,111 [INFO] Stabilization time: 10 seconds[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:30,112 [INFO] Starting full health check[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:30,112 [INFO] [0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: ============ DETAILED NODE STATUS ============[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:30,112 [INFO] Basic Node Information (kubectl get nodes -o wide):[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:30,557 [INFO]   NAME          STATUS   ROLES           AGE   VERSION    INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:30,557 [INFO]   master-m001   Ready    control-plane   13d   v1.32.11   192.168.56.101   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:30,557 [INFO]   master-m002   Ready    control-plane   13d   v1.32.11   192.168.56.102   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:30,558 [INFO]   master-m003   Ready    control-plane   13d   v1.32.11   192.168.56.103   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:30,558 [INFO]   worker-w001   Ready    <none>          13d   v1.32.11   192.168.56.104   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:30,558 [INFO]   worker-w002   Ready    <none>          13d   v1.32.11   192.168.56.105   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:30,558 [INFO]   worker-w003   Ready    <none>          13d   v1.32.11   192.168.56.106   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:30,558 [INFO]   worker-w004   Ready    <none>          13d   v1.32.11   192.168.56.107   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:30,558 [INFO]   worker-w005   Ready    <none>          13d   v1.32.11   192.168.56.108   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:30,559 [INFO]   worker-w006   Ready    <none>          13d   v1.32.11   192.168.56.109   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:30,559 [INFO] [0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: Enhanced Node Status (with taint and cordon indicators):[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:30,559 [INFO]   NAME                STATUS    ROLES           ZONE   CORDONED   TAINTS[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:30,990 [INFO]   master-m001     Ready  ‚úì worker         R1    No       node-role.kubernetes.io/control-plane [0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:31,267 [INFO]   master-m002     Ready  ‚úì worker         R2    No       None [0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:31,573 [INFO]   master-m003     Ready  ‚úì worker         R3    No       None [0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:31,890 [INFO]   worker-w001     Ready  ‚úì worker         R1    No       None [0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:32,179 [INFO]   worker-w002     Ready  ‚úì worker         R1    No       None [0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:32,561 [INFO]   worker-w003     Ready  ‚úì worker         R2    No       None [0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:32,857 [INFO]   worker-w004     Ready  ‚úì worker         R2    No       None [0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,184 [INFO]   worker-w005     Ready  ‚úì worker         R3    No       None [0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,439 [INFO]   worker-w006     Ready  ‚úì worker         R3    No       None [0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,440 [INFO] [0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: Legend:[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,440 [INFO]   ‚úì = Node is Ready[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,440 [INFO]   ‚ö†Ô∏è = Warning indicator (NotReady, Cordoned, or has simulated-failure taint)[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,440 [INFO] ============ DETAILED POD INFORMATION ============[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,440 [INFO] Running 'kubectl get pods -o wide' to show detailed pod placement:[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,542 [INFO]   NAMESPACE     NAME                                                       READY   STATUS      RESTARTS        AGE     IP                NODE          NOMINATED NODE   READINESS GATES[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,542 [INFO]   argo          argo-server-5c69cb69db-qk7vj                               1/1     Running     0               3m22s   192.168.153.248   worker-w004   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,542 [INFO]   argo          resilience-bench-ptngm-initialize-metrics-940927476        0/2     Completed   0               5d9h    192.168.221.125   master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,542 [INFO]   argo          resilience-bench-ptngm-run-health-check-1168793163         0/2     Completed   0               5d9h    192.168.221.67    master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,542 [INFO]   argo          resilience-bench-ptngm-run-health-check-1185570782         0/2     Completed   0               5d9h    192.168.221.66    master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,543 [INFO]   argo          resilience-bench-ptngm-run-health-check-1202348401         0/2     Completed   0               5d9h    192.168.221.122   master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,543 [INFO]   argo          resilience-bench-wf87f-initialize-metrics-3677018408       0/2     Completed   0               5d11h   192.168.221.100   master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,543 [INFO]   argo          resilience-bench-wf87f-run-health-check-2914150102         0/2     Completed   0               5d11h   192.168.221.105   master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,543 [INFO]   argo          resilience-bench-wf87f-run-health-check-3931060015         0/2     Completed   0               5d11h   192.168.221.104   master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,543 [INFO]   argo          resilience-bench-wf87f-run-health-check-3947837634         0/2     Completed   0               5d11h   192.168.221.98    master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,543 [INFO]   argo          resilience-bench-wf87f-run-health-check-3964615253         0/2     Completed   0               5d11h   192.168.221.103   master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,543 [INFO]   argo          resilience-bench-wf87f-run-node-simulation-4173146970      0/2     Completed   0               5d11h   192.168.221.101   master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,543 [INFO]   argo          resilience-bench-wf87f-run-rack-simulation-1356070019      0/2     Completed   0               5d11h   192.168.221.109   master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,544 [INFO]   argo          resilience-scaled-heft-pplkg-health-check-2410424775       2/2     Running     0               14s     192.168.221.67    master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,544 [INFO]   argo          resilience-scaled-heft-pplkg-health-check-2427202394       0/2     Completed   0               48s     192.168.221.122   master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,544 [INFO]   argo          resilience-scaled-heft-pplkg-node-failure-sim-2649153575   0/2     Completed   0               3m34s   192.168.221.66    master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,544 [INFO]   argo          resilience-scaled-heft-pplkg-node-failure-sim-2665931194   0/2     Completed   0               10m     192.168.221.69    master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,544 [INFO]   argo          workflow-controller-ccbd949dc-ghwvq                        1/1     Running     4 (9m7s ago)    55m     192.168.191.65    worker-w006   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,544 [INFO]   kube-system   calico-kube-controllers-7498b9bb4c-df4xv                   1/1     Running     1 (8m15s ago)   29m     192.168.195.242   worker-w002   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,544 [INFO]   kube-system   calico-node-4zhd4                                          1/1     Running     0               13d     192.168.56.105    worker-w002   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,544 [INFO]   kube-system   calico-node-75nx6                                          1/1     Running     0               13d     192.168.56.109    worker-w006   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,545 [INFO]   kube-system   calico-node-7lkdq                                          1/1     Running     0               13d     192.168.56.104    worker-w001   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,545 [INFO]   kube-system   calico-node-85f8c                                          1/1     Running     0               13d     192.168.56.102    master-m002   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,545 [INFO]   kube-system   calico-node-j8nb9                                          1/1     Running     0               13d     192.168.56.101    master-m001   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,545 [INFO]   kube-system   calico-node-lbcb2                                          1/1     Running     0               13d     192.168.56.106    worker-w003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,545 [INFO]   kube-system   calico-node-wlr5v                                          1/1     Running     0               13d     192.168.56.107    worker-w004   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,545 [INFO]   kube-system   calico-node-xnzjw                                          1/1     Running     0               13d     192.168.56.103    master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,545 [INFO]   kube-system   calico-node-xsltn                                          1/1     Running     0               13d     192.168.56.108    worker-w005   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,545 [INFO]   kube-system   coredns-668d6bf9bc-8dqlk                                   1/1     Running     0               136m    192.168.15.200    worker-w005   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,546 [INFO]   kube-system   coredns-668d6bf9bc-8nn7w                                   1/1     Running     0               29m     192.168.132.179   worker-w001   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,546 [INFO]   kube-system   etcd-master-m001                                           1/1     Running     0               13d     192.168.56.101    master-m001   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,546 [INFO]   kube-system   kube-apiserver-master-m001                                 1/1     Running     1 (5d4h ago)    13d     192.168.56.101    master-m001   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,546 [INFO]   kube-system   kube-controller-manager-master-m001                        1/1     Running     16 (9m9s ago)   13d     192.168.56.101    master-m001   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,546 [INFO]   kube-system   kube-proxy-5lzhj                                           1/1     Running     0               13d     192.168.56.102    master-m002   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,546 [INFO]   kube-system   kube-proxy-7jqkv                                           1/1     Running     0               13d     192.168.56.109    worker-w006   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,546 [INFO]   kube-system   kube-proxy-cv5dt                                           1/1     Running     0               13d     192.168.56.108    worker-w005   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,546 [INFO]   kube-system   kube-proxy-fvpmr                                           1/1     Running     0               13d     192.168.56.101    master-m001   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,546 [INFO]   kube-system   kube-proxy-hgs5z                                           1/1     Running     0               13d     192.168.56.106    worker-w003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,546 [INFO]   kube-system   kube-proxy-kmgqr                                           1/1     Running     0               13d     192.168.56.105    worker-w002   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,547 [INFO]   kube-system   kube-proxy-rdbz5                                           1/1     Running     0               13d     192.168.56.107    worker-w004   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,547 [INFO]   kube-system   kube-proxy-w8mnb                                           1/1     Running     0               13d     192.168.56.103    master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,547 [INFO]   kube-system   kube-proxy-x9jxr                                           1/1     Running     0               13d     192.168.56.104    worker-w001   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,547 [INFO]   kube-system   kube-scheduler-master-m001                                 1/1     Running     15 (9m4s ago)   13d     192.168.56.101    master-m001   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,547 [INFO] [0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: Pod distribution by node:[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,724 [INFO]   Node worker-w004: 3 pods[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,724 [INFO]   Node master-m003: 17 pods[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,724 [INFO]   Node 55m: 1 pods[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,725 [INFO]   Node 29m: 1 pods[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,725 [INFO]   Node worker-w002: 2 pods[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,725 [INFO]   Node worker-w006: 2 pods[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,725 [INFO]   Node worker-w001: 3 pods[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,725 [INFO]   Node master-m002: 2 pods[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,725 [INFO]   Node master-m001: 3 pods[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,726 [INFO]   Node worker-w003: 2 pods[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,726 [INFO]   Node worker-w005: 3 pods[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,726 [INFO]   Node 13d: 3 pods[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,726 [INFO] [0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: Filtering for simulation services:[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,878 [INFO] Node master-m001 is Ready[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,879 [INFO] Node master-m002 is Ready[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,879 [INFO] Node master-m003 is Ready[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,879 [INFO] Node worker-w001 is Ready[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,879 [INFO] Node worker-w002 is Ready[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,879 [INFO] Node worker-w003 is Ready[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,880 [INFO] Node worker-w004 is Ready[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,880 [INFO] Node worker-w005 is Ready[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,880 [INFO] Node worker-w006 is Ready[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,889 [WARNING] No pods found for etcd-sim[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,892 [WARNING] No pods found for postgres-sim[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,896 [WARNING] No pods found for redis-sim[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,898 [WARNING] No pods found for nginx-sim[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,902 [WARNING] No pods found for auth-sim[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: 2026-01-08 07:56:33,902 [INFO] Completed full health check[0m
[36mresilience-scaled-heft-pplkg-health-check-2410424775: time="2026-01-08T07:56:34.324Z" level=info msg="sub-process exited" argo=true error="<nil>"[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: Log directory created/verified: /app/logs[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: File logging configured successfully[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:56:48,036 [INFO] Loaded in-cluster Kubernetes config[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:56:48,037 [INFO] Running on host: resilience-scaled-heft-pplkg-rack-failure-sim-1468935587[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:56:48,037 [INFO] Detected current node: master-m003, zone: R3[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:56:48,037 [INFO] Checking if we have permissions to modify nodes...[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:56:48,059 [INFO] Testing permissions using node: master-m001[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:56:48,064 [INFO] Permission check successful - we can modify nodes[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:56:48,065 [INFO] Using real Kubernetes API for node control[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:56:48,065 [INFO] Action received: simulate-rack[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:56:48,065 [INFO] Stabilization time: 60 seconds[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:56:48,065 [INFO] Current node: master-m003 in zone: R3[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:56:48,066 [INFO] Safe zones for rack simulation: ['R1', 'R2'][0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:56:48,066 [INFO] Simulating full rack (zone) failure: R2 with nodes: ['master-m002', 'worker-w003', 'worker-w004'][0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:56:48,066 [INFO] Simulating node failure for master-m002 using Kubernetes API[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:56:48,080 [INFO] Node master-m002 cordoned[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:56:48,107 [INFO] Node master-m002 tainted with NoExecute[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:56:48,107 [INFO] Node master-m002 powered off (delay 5s)[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:56:48,108 [INFO] Node master-m002 powered off (delay 5s)[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:56:53,115 [INFO] Simulating node failure for worker-w003 using Kubernetes API[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:56:53,129 [INFO] Node worker-w003 cordoned[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:56:53,141 [INFO] Node worker-w003 tainted with NoExecute[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:56:53,141 [INFO] Node worker-w003 powered off (delay 5s)[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:56:53,141 [INFO] Node worker-w003 powered off (delay 5s)[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:56:58,147 [INFO] Simulating node failure for worker-w004 using Kubernetes API[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:56:58,164 [INFO] Node worker-w004 cordoned[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:56:58,177 [INFO] Node worker-w004 tainted with NoExecute[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:56:58,177 [INFO] Node worker-w004 powered off (delay 5s)[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:56:58,177 [INFO] Node worker-w004 powered off (delay 5s)[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:57:03,183 [INFO] Waiting 60 seconds for the cluster to stabilize before health check...[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:03,240 [INFO] Running health check after rack power off[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:03,240 [INFO] Starting full health check[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:03,241 [INFO] [0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: ============ DETAILED NODE STATUS ============[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:03,241 [INFO] Basic Node Information (kubectl get nodes -o wide):[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:03,320 [INFO]   NAME          STATUS                     ROLES           AGE   VERSION    INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:03,320 [INFO]   master-m001   Ready                      control-plane   13d   v1.32.11   192.168.56.101   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:03,321 [INFO]   master-m002   Ready,SchedulingDisabled   control-plane   13d   v1.32.11   192.168.56.102   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:03,321 [INFO]   master-m003   Ready                      control-plane   13d   v1.32.11   192.168.56.103   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:03,321 [INFO]   worker-w001   Ready                      <none>          13d   v1.32.11   192.168.56.104   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:03,321 [INFO]   worker-w002   Ready                      <none>          13d   v1.32.11   192.168.56.105   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:03,321 [INFO]   worker-w003   Ready,SchedulingDisabled   <none>          13d   v1.32.11   192.168.56.106   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:03,321 [INFO]   worker-w004   Ready,SchedulingDisabled   <none>          13d   v1.32.11   192.168.56.107   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:03,321 [INFO]   worker-w005   Ready                      <none>          13d   v1.32.11   192.168.56.108   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:03,322 [INFO]   worker-w006   Ready                      <none>          13d   v1.32.11   192.168.56.109   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:03,322 [INFO] [0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: Enhanced Node Status (with taint and cordon indicators):[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:03,322 [INFO]   NAME                STATUS    ROLES           ZONE   CORDONED   TAINTS[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:03,760 [INFO]   master-m001     Ready  ‚úì worker         R1    No       node-role.kubernetes.io/control-plane [0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:04,052 [INFO]   master-m002     Ready  ‚úì worker         R2    YES     ‚ö†Ô∏è simulated-failure, node.kubernetes.io/unschedulable ‚ö†Ô∏è[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:04,438 [INFO]   master-m003     Ready  ‚úì worker         R3    No       None [0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:04,734 [INFO]   worker-w001     Ready  ‚úì worker         R1    No       None [0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:05,105 [INFO]   worker-w002     Ready  ‚úì worker         R1    No       None [0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:05,504 [INFO]   worker-w003     Ready  ‚úì worker         R2    YES     ‚ö†Ô∏è simulated-failure, node.kubernetes.io/unschedulable ‚ö†Ô∏è[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:05,818 [INFO]   worker-w004     Ready  ‚úì worker         R2    YES     ‚ö†Ô∏è simulated-failure, node.kubernetes.io/unschedulable ‚ö†Ô∏è[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,124 [INFO]   worker-w005     Ready  ‚úì worker         R3    No       None [0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,492 [INFO]   worker-w006     Ready  ‚úì worker         R3    No       None [0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,492 [INFO] [0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: Legend:[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,492 [INFO]   ‚úì = Node is Ready[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,492 [INFO]   ‚ö†Ô∏è = Warning indicator (NotReady, Cordoned, or has simulated-failure taint)[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,493 [INFO] ============ DETAILED POD INFORMATION ============[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,493 [INFO] Running 'kubectl get pods -o wide' to show detailed pod placement:[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,641 [INFO]   NAMESPACE     NAME                                                       READY   STATUS      RESTARTS        AGE     IP                NODE          NOMINATED NODE   READINESS GATES[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,642 [INFO]   argo          argo-server-5c69cb69db-qjhjz                               1/1     Running     0               68s     192.168.15.201    worker-w005   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,642 [INFO]   argo          resilience-bench-ptngm-initialize-metrics-940927476        0/2     Completed   0               5d10h   192.168.221.125   master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,642 [INFO]   argo          resilience-bench-ptngm-run-health-check-1168793163         0/2     Completed   0               5d10h   192.168.221.67    master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,642 [INFO]   argo          resilience-bench-ptngm-run-health-check-1185570782         0/2     Completed   0               5d10h   192.168.221.66    master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,642 [INFO]   argo          resilience-bench-ptngm-run-health-check-1202348401         0/2     Completed   0               5d10h   192.168.221.122   master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,643 [INFO]   argo          resilience-bench-wf87f-initialize-metrics-3677018408       0/2     Completed   0               5d11h   192.168.221.100   master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,643 [INFO]   argo          resilience-bench-wf87f-run-health-check-2914150102         0/2     Completed   0               5d11h   192.168.221.105   master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,643 [INFO]   argo          resilience-bench-wf87f-run-health-check-3931060015         0/2     Completed   0               5d11h   192.168.221.104   master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,643 [INFO]   argo          resilience-bench-wf87f-run-health-check-3947837634         0/2     Completed   0               5d11h   192.168.221.98    master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,643 [INFO]   argo          resilience-bench-wf87f-run-health-check-3964615253         0/2     Completed   0               5d11h   192.168.221.103   master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,643 [INFO]   argo          resilience-bench-wf87f-run-node-simulation-4173146970      0/2     Completed   0               5d11h   192.168.221.101   master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,643 [INFO]   argo          resilience-bench-wf87f-run-rack-simulation-1356070019      0/2     Completed   0               5d11h   192.168.221.109   master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,644 [INFO]   argo          resilience-scaled-heft-pplkg-health-check-2410424775       0/2     Completed   0               107s    192.168.221.67    master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,644 [INFO]   argo          resilience-scaled-heft-pplkg-health-check-2427202394       0/2     Completed   0               2m21s   192.168.221.122   master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,644 [INFO]   argo          resilience-scaled-heft-pplkg-node-failure-sim-2649153575   0/2     Completed   0               5m7s    192.168.221.66    master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,644 [INFO]   argo          resilience-scaled-heft-pplkg-node-failure-sim-2665931194   0/2     Completed   0               12m     192.168.221.69    master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,644 [INFO]   argo          resilience-scaled-heft-pplkg-rack-failure-sim-1468935587   2/2     Running     0               87s     192.168.221.70    master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,644 [INFO]   argo          workflow-controller-ccbd949dc-ghwvq                        1/1     Running     4 (10m ago)     56m     192.168.191.65    worker-w006   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,644 [INFO]   kube-system   calico-kube-controllers-7498b9bb4c-df4xv                   1/1     Running     1 (9m48s ago)   30m     192.168.195.242   worker-w002   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,644 [INFO]   kube-system   calico-node-4zhd4                                          1/1     Running     0               13d     192.168.56.105    worker-w002   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,645 [INFO]   kube-system   calico-node-75nx6                                          1/1     Running     0               13d     192.168.56.109    worker-w006   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,645 [INFO]   kube-system   calico-node-7lkdq                                          1/1     Running     0               13d     192.168.56.104    worker-w001   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,645 [INFO]   kube-system   calico-node-85f8c                                          1/1     Running     0               13d     192.168.56.102    master-m002   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,645 [INFO]   kube-system   calico-node-j8nb9                                          1/1     Running     0               13d     192.168.56.101    master-m001   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,645 [INFO]   kube-system   calico-node-lbcb2                                          1/1     Running     0               13d     192.168.56.106    worker-w003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,645 [INFO]   kube-system   calico-node-wlr5v                                          1/1     Running     0               13d     192.168.56.107    worker-w004   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,645 [INFO]   kube-system   calico-node-xnzjw                                          1/1     Running     0               13d     192.168.56.103    master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,646 [INFO]   kube-system   calico-node-xsltn                                          1/1     Running     0               13d     192.168.56.108    worker-w005   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,646 [INFO]   kube-system   coredns-668d6bf9bc-8dqlk                                   1/1     Running     0               137m    192.168.15.200    worker-w005   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,646 [INFO]   kube-system   coredns-668d6bf9bc-8nn7w                                   1/1     Running     0               30m     192.168.132.179   worker-w001   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,646 [INFO]   kube-system   etcd-master-m001                                           1/1     Running     0               13d     192.168.56.101    master-m001   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,646 [INFO]   kube-system   kube-apiserver-master-m001                                 1/1     Running     1 (5d4h ago)    13d     192.168.56.101    master-m001   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,646 [INFO]   kube-system   kube-controller-manager-master-m001                        1/1     Running     16 (10m ago)    13d     192.168.56.101    master-m001   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,646 [INFO]   kube-system   kube-proxy-5lzhj                                           1/1     Running     0               13d     192.168.56.102    master-m002   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,647 [INFO]   kube-system   kube-proxy-7jqkv                                           1/1     Running     0               13d     192.168.56.109    worker-w006   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,647 [INFO]   kube-system   kube-proxy-cv5dt                                           1/1     Running     0               13d     192.168.56.108    worker-w005   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,647 [INFO]   kube-system   kube-proxy-fvpmr                                           1/1     Running     0               13d     192.168.56.101    master-m001   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,647 [INFO]   kube-system   kube-proxy-hgs5z                                           1/1     Running     0               13d     192.168.56.106    worker-w003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,647 [INFO]   kube-system   kube-proxy-kmgqr                                           1/1     Running     0               13d     192.168.56.105    worker-w002   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,647 [INFO]   kube-system   kube-proxy-rdbz5                                           1/1     Running     0               13d     192.168.56.107    worker-w004   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,647 [INFO]   kube-system   kube-proxy-w8mnb                                           1/1     Running     0               13d     192.168.56.103    master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,648 [INFO]   kube-system   kube-proxy-x9jxr                                           1/1     Running     0               13d     192.168.56.104    worker-w001   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,648 [INFO]   kube-system   kube-scheduler-master-m001                                 1/1     Running     15 (10m ago)    13d     192.168.56.101    master-m001   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,648 [INFO] [0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: Pod distribution by node:[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,889 [INFO]   Node worker-w005: 4 pods[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,890 [INFO]   Node master-m003: 18 pods[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,890 [INFO]   Node 56m: 1 pods[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,890 [INFO]   Node 30m: 1 pods[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,890 [INFO]   Node worker-w002: 2 pods[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,890 [INFO]   Node worker-w006: 2 pods[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,890 [INFO]   Node worker-w001: 3 pods[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,890 [INFO]   Node master-m002: 2 pods[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,891 [INFO]   Node master-m001: 3 pods[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,891 [INFO]   Node worker-w003: 2 pods[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,891 [INFO]   Node worker-w004: 2 pods[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,891 [INFO]   Node 13d: 3 pods[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:06,891 [INFO] [0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: Filtering for simulation services:[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:07,162 [INFO] Node master-m001 is Ready[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:07,162 [INFO] Node master-m002 is Ready[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:07,162 [INFO] Node master-m003 is Ready[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:07,163 [INFO] Node worker-w001 is Ready[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:07,163 [INFO] Node worker-w002 is Ready[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:07,163 [INFO] Node worker-w003 is Ready[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:07,163 [INFO] Node worker-w004 is Ready[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:07,163 [INFO] Node worker-w005 is Ready[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:07,163 [INFO] Node worker-w006 is Ready[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:07,190 [WARNING] No pods found for etcd-sim[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:07,194 [WARNING] No pods found for postgres-sim[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:07,197 [WARNING] No pods found for redis-sim[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:07,199 [WARNING] No pods found for nginx-sim[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:07,202 [WARNING] No pods found for auth-sim[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:07,203 [INFO] Completed full health check[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:07,203 [INFO] Zone R2 remains down for 10 seconds[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:17,213 [INFO] Running health check before rack power on[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:17,213 [INFO] Starting full health check[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:17,213 [INFO] [0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: ============ DETAILED NODE STATUS ============[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:17,213 [INFO] Basic Node Information (kubectl get nodes -o wide):[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:19,310 [INFO]   NAME          STATUS                     ROLES           AGE   VERSION    INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:19,310 [INFO]   master-m001   Ready                      control-plane   13d   v1.32.11   192.168.56.101   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:19,310 [INFO]   master-m002   Ready,SchedulingDisabled   control-plane   13d   v1.32.11   192.168.56.102   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:19,310 [INFO]   master-m003   Ready                      control-plane   13d   v1.32.11   192.168.56.103   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:19,310 [INFO]   worker-w001   Ready                      <none>          13d   v1.32.11   192.168.56.104   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:19,310 [INFO]   worker-w002   Ready                      <none>          13d   v1.32.11   192.168.56.105   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:19,310 [INFO]   worker-w003   Ready,SchedulingDisabled   <none>          13d   v1.32.11   192.168.56.106   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:19,311 [INFO]   worker-w004   Ready,SchedulingDisabled   <none>          13d   v1.32.11   192.168.56.107   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:19,311 [INFO]   worker-w005   Ready                      <none>          13d   v1.32.11   192.168.56.108   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:19,311 [INFO]   worker-w006   Ready                      <none>          13d   v1.32.11   192.168.56.109   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:19,311 [INFO] [0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: Enhanced Node Status (with taint and cordon indicators):[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:19,311 [INFO]   NAME                STATUS    ROLES           ZONE   CORDONED   TAINTS[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:19,631 [INFO]   master-m001     Ready  ‚úì worker         R1    No       node-role.kubernetes.io/control-plane [0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:19,893 [INFO]   master-m002     Ready  ‚úì worker         R2    YES     ‚ö†Ô∏è simulated-failure, node.kubernetes.io/unschedulable ‚ö†Ô∏è[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:20,108 [INFO]   master-m003     Ready  ‚úì worker         R3    No       None [0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:20,306 [INFO]   worker-w001     Ready  ‚úì worker         R1    No       None [0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:20,488 [INFO]   worker-w002     Ready  ‚úì worker         R1    No       None [0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:21,289 [INFO]   worker-w003     Ready  ‚úì worker         R2    YES     ‚ö†Ô∏è simulated-failure, node.kubernetes.io/unschedulable ‚ö†Ô∏è[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:21,578 [INFO]   worker-w004     Ready  ‚úì worker         R2    YES     ‚ö†Ô∏è simulated-failure, node.kubernetes.io/unschedulable ‚ö†Ô∏è[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:21,840 [INFO]   worker-w005     Ready  ‚úì worker         R3    No       None [0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,054 [INFO]   worker-w006     Ready  ‚úì worker         R3    No       None [0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,055 [INFO] [0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: Legend:[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,055 [INFO]   ‚úì = Node is Ready[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,056 [INFO]   ‚ö†Ô∏è = Warning indicator (NotReady, Cordoned, or has simulated-failure taint)[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,056 [INFO] ============ DETAILED POD INFORMATION ============[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,056 [INFO] Running 'kubectl get pods -o wide' to show detailed pod placement:[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,190 [INFO]   NAMESPACE     NAME                                                       READY   STATUS      RESTARTS       AGE     IP                NODE          NOMINATED NODE   READINESS GATES[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,190 [INFO]   argo          argo-server-5c69cb69db-qjhjz                               1/1     Running     0              84s     192.168.15.201    worker-w005   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,191 [INFO]   argo          resilience-bench-ptngm-initialize-metrics-940927476        0/2     Completed   0              5d10h   192.168.221.125   master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,191 [INFO]   argo          resilience-bench-ptngm-run-health-check-1168793163         0/2     Completed   0              5d10h   192.168.221.67    master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,191 [INFO]   argo          resilience-bench-ptngm-run-health-check-1185570782         0/2     Completed   0              5d10h   192.168.221.66    master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,191 [INFO]   argo          resilience-bench-ptngm-run-health-check-1202348401         0/2     Completed   0              5d10h   192.168.221.122   master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,191 [INFO]   argo          resilience-bench-wf87f-initialize-metrics-3677018408       0/2     Completed   0              5d11h   192.168.221.100   master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,191 [INFO]   argo          resilience-bench-wf87f-run-health-check-2914150102         0/2     Completed   0              5d11h   192.168.221.105   master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,192 [INFO]   argo          resilience-bench-wf87f-run-health-check-3931060015         0/2     Completed   0              5d11h   192.168.221.104   master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,192 [INFO]   argo          resilience-bench-wf87f-run-health-check-3947837634         0/2     Completed   0              5d11h   192.168.221.98    master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,192 [INFO]   argo          resilience-bench-wf87f-run-health-check-3964615253         0/2     Completed   0              5d11h   192.168.221.103   master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,192 [INFO]   argo          resilience-bench-wf87f-run-node-simulation-4173146970      0/2     Completed   0              5d11h   192.168.221.101   master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,192 [INFO]   argo          resilience-bench-wf87f-run-rack-simulation-1356070019      0/2     Completed   0              5d11h   192.168.221.109   master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,192 [INFO]   argo          resilience-scaled-heft-pplkg-health-check-2410424775       0/2     Completed   0              2m3s    192.168.221.67    master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,193 [INFO]   argo          resilience-scaled-heft-pplkg-health-check-2427202394       0/2     Completed   0              2m37s   192.168.221.122   master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,193 [INFO]   argo          resilience-scaled-heft-pplkg-node-failure-sim-2649153575   0/2     Completed   0              5m23s   192.168.221.66    master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,193 [INFO]   argo          resilience-scaled-heft-pplkg-node-failure-sim-2665931194   0/2     Completed   0              12m     192.168.221.69    master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,193 [INFO]   argo          resilience-scaled-heft-pplkg-rack-failure-sim-1468935587   2/2     Running     0              103s    192.168.221.70    master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,193 [INFO]   argo          workflow-controller-ccbd949dc-ghwvq                        1/1     Running     4 (10m ago)    56m     192.168.191.65    worker-w006   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,193 [INFO]   kube-system   calico-kube-controllers-7498b9bb4c-df4xv                   1/1     Running     1 (10m ago)    31m     192.168.195.242   worker-w002   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,193 [INFO]   kube-system   calico-node-4zhd4                                          1/1     Running     0              13d     192.168.56.105    worker-w002   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,194 [INFO]   kube-system   calico-node-75nx6                                          1/1     Running     0              13d     192.168.56.109    worker-w006   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,194 [INFO]   kube-system   calico-node-7lkdq                                          1/1     Running     0              13d     192.168.56.104    worker-w001   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,194 [INFO]   kube-system   calico-node-85f8c                                          1/1     Running     0              13d     192.168.56.102    master-m002   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,194 [INFO]   kube-system   calico-node-j8nb9                                          1/1     Running     0              13d     192.168.56.101    master-m001   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,194 [INFO]   kube-system   calico-node-lbcb2                                          1/1     Running     0              13d     192.168.56.106    worker-w003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,194 [INFO]   kube-system   calico-node-wlr5v                                          1/1     Running     0              13d     192.168.56.107    worker-w004   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,195 [INFO]   kube-system   calico-node-xnzjw                                          1/1     Running     0              13d     192.168.56.103    master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,195 [INFO]   kube-system   calico-node-xsltn                                          1/1     Running     0              13d     192.168.56.108    worker-w005   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,195 [INFO]   kube-system   coredns-668d6bf9bc-8dqlk                                   1/1     Running     0              137m    192.168.15.200    worker-w005   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,195 [INFO]   kube-system   coredns-668d6bf9bc-8nn7w                                   1/1     Running     0              31m     192.168.132.179   worker-w001   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,195 [INFO]   kube-system   etcd-master-m001                                           1/1     Running     0              13d     192.168.56.101    master-m001   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,195 [INFO]   kube-system   kube-apiserver-master-m001                                 1/1     Running     1 (5d4h ago)   13d     192.168.56.101    master-m001   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,195 [INFO]   kube-system   kube-controller-manager-master-m001                        1/1     Running     16 (10m ago)   13d     192.168.56.101    master-m001   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,196 [INFO]   kube-system   kube-proxy-5lzhj                                           1/1     Running     0              13d     192.168.56.102    master-m002   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,196 [INFO]   kube-system   kube-proxy-7jqkv                                           1/1     Running     0              13d     192.168.56.109    worker-w006   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,196 [INFO]   kube-system   kube-proxy-cv5dt                                           1/1     Running     0              13d     192.168.56.108    worker-w005   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,196 [INFO]   kube-system   kube-proxy-fvpmr                                           1/1     Running     0              13d     192.168.56.101    master-m001   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,196 [INFO]   kube-system   kube-proxy-hgs5z                                           1/1     Running     0              13d     192.168.56.106    worker-w003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,196 [INFO]   kube-system   kube-proxy-kmgqr                                           1/1     Running     0              13d     192.168.56.105    worker-w002   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,197 [INFO]   kube-system   kube-proxy-rdbz5                                           1/1     Running     0              13d     192.168.56.107    worker-w004   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,197 [INFO]   kube-system   kube-proxy-w8mnb                                           1/1     Running     0              13d     192.168.56.103    master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,197 [INFO]   kube-system   kube-proxy-x9jxr                                           1/1     Running     0              13d     192.168.56.104    worker-w001   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,197 [INFO]   kube-system   kube-scheduler-master-m001                                 1/1     Running     15 (10m ago)   13d     192.168.56.101    master-m001   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,197 [INFO] [0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: Pod distribution by node:[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,292 [INFO]   Node worker-w005: 4 pods[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,293 [INFO]   Node master-m003: 18 pods[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,293 [INFO]   Node 56m: 1 pods[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,293 [INFO]   Node 31m: 1 pods[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,293 [INFO]   Node worker-w002: 2 pods[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,293 [INFO]   Node worker-w006: 2 pods[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,293 [INFO]   Node worker-w001: 3 pods[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,293 [INFO]   Node master-m002: 2 pods[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,293 [INFO]   Node master-m001: 3 pods[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,293 [INFO]   Node worker-w003: 2 pods[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,294 [INFO]   Node worker-w004: 2 pods[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,294 [INFO]   Node 13d: 3 pods[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,294 [INFO] [0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: Filtering for simulation services:[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,428 [INFO] Node master-m001 is Ready[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,429 [INFO] Node master-m002 is Ready[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,429 [INFO] Node master-m003 is Ready[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,429 [INFO] Node worker-w001 is Ready[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,429 [INFO] Node worker-w002 is Ready[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,429 [INFO] Node worker-w003 is Ready[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,429 [INFO] Node worker-w004 is Ready[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,429 [INFO] Node worker-w005 is Ready[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,429 [INFO] Node worker-w006 is Ready[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,432 [WARNING] No pods found for etcd-sim[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,434 [WARNING] No pods found for postgres-sim[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,436 [WARNING] No pods found for redis-sim[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,441 [WARNING] No pods found for nginx-sim[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,452 [WARNING] No pods found for auth-sim[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,454 [INFO] Completed full health check[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,454 [INFO] Simulating node recovery for master-m002 using Kubernetes API[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,474 [INFO] Removed simulated-failure taint from node master-m002[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,490 [INFO] Node master-m002 uncordoned and ready[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:22,491 [INFO] Node master-m002 powered on (delay 5s)[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:27,495 [INFO] Simulating node recovery for worker-w003 using Kubernetes API[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:27,519 [INFO] Removed simulated-failure taint from node worker-w003[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:27,534 [INFO] Node worker-w003 uncordoned and ready[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:27,535 [INFO] Node worker-w003 powered on (delay 5s)[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:32,540 [INFO] Simulating node recovery for worker-w004 using Kubernetes API[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:32,556 [INFO] Removed simulated-failure taint from node worker-w004[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:32,570 [INFO] Node worker-w004 uncordoned and ready[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:32,570 [INFO] Node worker-w004 powered on (delay 5s)[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:58:37,574 [INFO] Waiting 60 seconds for the cluster to stabilize after recovery...[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:37,610 [INFO] Running final health check[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:37,611 [INFO] Starting full health check[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:37,611 [INFO] [0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: ============ DETAILED NODE STATUS ============[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:37,611 [INFO] Basic Node Information (kubectl get nodes -o wide):[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:37,680 [INFO]   NAME          STATUS   ROLES           AGE   VERSION    INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:37,681 [INFO]   master-m001   Ready    control-plane   13d   v1.32.11   192.168.56.101   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:37,681 [INFO]   master-m002   Ready    control-plane   13d   v1.32.11   192.168.56.102   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:37,681 [INFO]   master-m003   Ready    control-plane   13d   v1.32.11   192.168.56.103   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:37,681 [INFO]   worker-w001   Ready    <none>          13d   v1.32.11   192.168.56.104   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:37,681 [INFO]   worker-w002   Ready    <none>          13d   v1.32.11   192.168.56.105   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:37,681 [INFO]   worker-w003   Ready    <none>          13d   v1.32.11   192.168.56.106   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:37,681 [INFO]   worker-w004   Ready    <none>          13d   v1.32.11   192.168.56.107   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:37,681 [INFO]   worker-w005   Ready    <none>          13d   v1.32.11   192.168.56.108   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:37,681 [INFO]   worker-w006   Ready    <none>          13d   v1.32.11   192.168.56.109   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:37,682 [INFO] [0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: Enhanced Node Status (with taint and cordon indicators):[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:37,682 [INFO]   NAME                STATUS    ROLES           ZONE   CORDONED   TAINTS[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:37,902 [INFO]   master-m001     Ready  ‚úì worker         R1    No       node-role.kubernetes.io/control-plane [0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:38,076 [INFO]   master-m002     Ready  ‚úì worker         R2    No       None [0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:38,247 [INFO]   master-m003     Ready  ‚úì worker         R3    No       None [0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:38,387 [INFO]   worker-w001     Ready  ‚úì worker         R1    No       None [0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:38,530 [INFO]   worker-w002     Ready  ‚úì worker         R1    No       None [0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:38,669 [INFO]   worker-w003     Ready  ‚úì worker         R2    No       None [0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:38,794 [INFO]   worker-w004     Ready  ‚úì worker         R2    No       None [0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:38,958 [INFO]   worker-w005     Ready  ‚úì worker         R3    No       None [0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,133 [INFO]   worker-w006     Ready  ‚úì worker         R3    No       None [0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,133 [INFO] [0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: Legend:[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,133 [INFO]   ‚úì = Node is Ready[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,133 [INFO]   ‚ö†Ô∏è = Warning indicator (NotReady, Cordoned, or has simulated-failure taint)[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,133 [INFO] ============ DETAILED POD INFORMATION ============[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,133 [INFO] Running 'kubectl get pods -o wide' to show detailed pod placement:[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,221 [INFO]   NAMESPACE     NAME                                                       READY   STATUS      RESTARTS       AGE     IP                NODE          NOMINATED NODE   READINESS GATES[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,221 [INFO]   argo          argo-server-5c69cb69db-qjhjz                               1/1     Running     0              2m41s   192.168.15.201    worker-w005   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,221 [INFO]   argo          resilience-bench-ptngm-initialize-metrics-940927476        0/2     Completed   0              5d10h   192.168.221.125   master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,221 [INFO]   argo          resilience-bench-ptngm-run-health-check-1168793163         0/2     Completed   0              5d10h   192.168.221.67    master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,221 [INFO]   argo          resilience-bench-ptngm-run-health-check-1185570782         0/2     Completed   0              5d10h   192.168.221.66    master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,221 [INFO]   argo          resilience-bench-ptngm-run-health-check-1202348401         0/2     Completed   0              5d10h   192.168.221.122   master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,222 [INFO]   argo          resilience-bench-wf87f-initialize-metrics-3677018408       0/2     Completed   0              5d11h   192.168.221.100   master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,222 [INFO]   argo          resilience-bench-wf87f-run-health-check-2914150102         0/2     Completed   0              5d11h   192.168.221.105   master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,222 [INFO]   argo          resilience-bench-wf87f-run-health-check-3931060015         0/2     Completed   0              5d11h   192.168.221.104   master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,222 [INFO]   argo          resilience-bench-wf87f-run-health-check-3947837634         0/2     Completed   0              5d11h   192.168.221.98    master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,222 [INFO]   argo          resilience-bench-wf87f-run-health-check-3964615253         0/2     Completed   0              5d11h   192.168.221.103   master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,222 [INFO]   argo          resilience-bench-wf87f-run-node-simulation-4173146970      0/2     Completed   0              5d11h   192.168.221.101   master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,222 [INFO]   argo          resilience-bench-wf87f-run-rack-simulation-1356070019      0/2     Completed   0              5d11h   192.168.221.109   master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,222 [INFO]   argo          resilience-scaled-heft-pplkg-health-check-2410424775       0/2     Completed   0              3m20s   192.168.221.67    master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,222 [INFO]   argo          resilience-scaled-heft-pplkg-health-check-2427202394       0/2     Completed   0              3m54s   192.168.221.122   master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,222 [INFO]   argo          resilience-scaled-heft-pplkg-node-failure-sim-2649153575   0/2     Completed   0              6m40s   192.168.221.66    master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,222 [INFO]   argo          resilience-scaled-heft-pplkg-node-failure-sim-2665931194   0/2     Completed   0              13m     192.168.221.69    master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,222 [INFO]   argo          resilience-scaled-heft-pplkg-rack-failure-sim-1468935587   2/2     Running     0              3m      192.168.221.70    master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,222 [INFO]   argo          workflow-controller-ccbd949dc-ghwvq                        1/1     Running     4 (12m ago)    58m     192.168.191.65    worker-w006   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,222 [INFO]   kube-system   calico-kube-controllers-7498b9bb4c-df4xv                   1/1     Running     1 (11m ago)    32m     192.168.195.242   worker-w002   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,222 [INFO]   kube-system   calico-node-4zhd4                                          1/1     Running     0              13d     192.168.56.105    worker-w002   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,222 [INFO]   kube-system   calico-node-75nx6                                          1/1     Running     0              13d     192.168.56.109    worker-w006   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,222 [INFO]   kube-system   calico-node-7lkdq                                          1/1     Running     0              13d     192.168.56.104    worker-w001   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,222 [INFO]   kube-system   calico-node-85f8c                                          1/1     Running     0              13d     192.168.56.102    master-m002   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,222 [INFO]   kube-system   calico-node-j8nb9                                          1/1     Running     0              13d     192.168.56.101    master-m001   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,222 [INFO]   kube-system   calico-node-lbcb2                                          1/1     Running     0              13d     192.168.56.106    worker-w003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,222 [INFO]   kube-system   calico-node-wlr5v                                          1/1     Running     0              13d     192.168.56.107    worker-w004   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,222 [INFO]   kube-system   calico-node-xnzjw                                          1/1     Running     0              13d     192.168.56.103    master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,222 [INFO]   kube-system   calico-node-xsltn                                          1/1     Running     0              13d     192.168.56.108    worker-w005   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,222 [INFO]   kube-system   coredns-668d6bf9bc-8dqlk                                   1/1     Running     0              139m    192.168.15.200    worker-w005   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,222 [INFO]   kube-system   coredns-668d6bf9bc-8nn7w                                   1/1     Running     0              32m     192.168.132.179   worker-w001   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,222 [INFO]   kube-system   etcd-master-m001                                           1/1     Running     0              13d     192.168.56.101    master-m001   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,222 [INFO]   kube-system   kube-apiserver-master-m001                                 1/1     Running     1 (5d4h ago)   13d     192.168.56.101    master-m001   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,222 [INFO]   kube-system   kube-controller-manager-master-m001                        1/1     Running     16 (12m ago)   13d     192.168.56.101    master-m001   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,223 [INFO]   kube-system   kube-proxy-5lzhj                                           1/1     Running     0              13d     192.168.56.102    master-m002   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,223 [INFO]   kube-system   kube-proxy-7jqkv                                           1/1     Running     0              13d     192.168.56.109    worker-w006   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,223 [INFO]   kube-system   kube-proxy-cv5dt                                           1/1     Running     0              13d     192.168.56.108    worker-w005   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,223 [INFO]   kube-system   kube-proxy-fvpmr                                           1/1     Running     0              13d     192.168.56.101    master-m001   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,223 [INFO]   kube-system   kube-proxy-hgs5z                                           1/1     Running     0              13d     192.168.56.106    worker-w003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,223 [INFO]   kube-system   kube-proxy-kmgqr                                           1/1     Running     0              13d     192.168.56.105    worker-w002   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,223 [INFO]   kube-system   kube-proxy-rdbz5                                           1/1     Running     0              13d     192.168.56.107    worker-w004   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,223 [INFO]   kube-system   kube-proxy-w8mnb                                           1/1     Running     0              13d     192.168.56.103    master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,223 [INFO]   kube-system   kube-proxy-x9jxr                                           1/1     Running     0              13d     192.168.56.104    worker-w001   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,223 [INFO]   kube-system   kube-scheduler-master-m001                                 1/1     Running     15 (12m ago)   13d     192.168.56.101    master-m001   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,223 [INFO] [0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: Pod distribution by node:[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,320 [INFO]   Node worker-w005: 4 pods[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,320 [INFO]   Node master-m003: 18 pods[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,320 [INFO]   Node 58m: 1 pods[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,320 [INFO]   Node 32m: 1 pods[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,320 [INFO]   Node worker-w002: 2 pods[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,320 [INFO]   Node worker-w006: 2 pods[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,320 [INFO]   Node worker-w001: 3 pods[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,320 [INFO]   Node master-m002: 2 pods[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,321 [INFO]   Node master-m001: 3 pods[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,321 [INFO]   Node worker-w003: 2 pods[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,321 [INFO]   Node worker-w004: 2 pods[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,321 [INFO]   Node 13d: 3 pods[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,321 [INFO] [0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: Filtering for simulation services:[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,425 [INFO] Node master-m001 is Ready[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,426 [INFO] Node master-m002 is Ready[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,426 [INFO] Node master-m003 is Ready[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,426 [INFO] Node worker-w001 is Ready[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,426 [INFO] Node worker-w002 is Ready[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,426 [INFO] Node worker-w003 is Ready[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,426 [INFO] Node worker-w004 is Ready[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,426 [INFO] Node worker-w005 is Ready[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,426 [INFO] Node worker-w006 is Ready[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,483 [WARNING] No pods found for etcd-sim[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,485 [WARNING] No pods found for postgres-sim[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,487 [WARNING] No pods found for redis-sim[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,488 [WARNING] No pods found for nginx-sim[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,490 [WARNING] No pods found for auth-sim[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,490 [INFO] Completed full health check[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: 2026-01-08 07:59:39,490 [INFO] Rack R2 has been fully restored[0m
[37mresilience-scaled-heft-pplkg-rack-failure-sim-1468935587: time="2026-01-08T07:59:39.762Z" level=info msg="sub-process exited" argo=true error="<nil>"[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: Log directory created/verified: /app/logs[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: File logging configured successfully[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 07:59:57,708 [INFO] Loaded in-cluster Kubernetes config[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 07:59:57,709 [INFO] Running on host: resilience-scaled-heft-pplkg-rack-failure-sim-1485713206[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 07:59:57,709 [INFO] Detected current node: master-m003, zone: R3[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 07:59:57,709 [INFO] Checking if we have permissions to modify nodes...[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 07:59:57,742 [INFO] Testing permissions using node: master-m001[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 07:59:57,750 [INFO] Permission check successful - we can modify nodes[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 07:59:57,750 [INFO] Using real Kubernetes API for node control[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 07:59:57,751 [INFO] Action received: simulate-rack[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 07:59:57,751 [INFO] Stabilization time: 60 seconds[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 07:59:57,751 [INFO] Current node: master-m003 in zone: R3[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 07:59:57,751 [INFO] Safe zones for rack simulation: ['R1', 'R2'][0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 07:59:57,752 [INFO] Simulating full rack (zone) failure: R1 with nodes: ['master-m001', 'worker-w001', 'worker-w002'][0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 07:59:57,752 [INFO] Simulating node failure for master-m001 using Kubernetes API[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 07:59:57,767 [INFO] Node master-m001 cordoned[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 07:59:57,782 [INFO] Node master-m001 tainted with NoExecute[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 07:59:57,783 [INFO] Node master-m001 powered off (delay 5s)[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 07:59:57,783 [INFO] Node master-m001 powered off (delay 5s)[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:00:02,787 [INFO] Simulating node failure for worker-w001 using Kubernetes API[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:00:02,802 [INFO] Node worker-w001 cordoned[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:00:02,826 [INFO] Node worker-w001 tainted with NoExecute[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:00:02,826 [INFO] Node worker-w001 powered off (delay 5s)[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:00:02,826 [INFO] Node worker-w001 powered off (delay 5s)[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:00:07,830 [INFO] Simulating node failure for worker-w002 using Kubernetes API[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:00:07,848 [INFO] Node worker-w002 cordoned[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:00:07,874 [INFO] Node worker-w002 tainted with NoExecute[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:00:07,875 [INFO] Node worker-w002 powered off (delay 5s)[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:00:07,876 [INFO] Node worker-w002 powered off (delay 5s)[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:00:12,890 [INFO] Waiting 60 seconds for the cluster to stabilize before health check...[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:12,941 [INFO] Running health check after rack power off[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:12,941 [INFO] Starting full health check[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:12,941 [INFO] [0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: ============ DETAILED NODE STATUS ============[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:12,941 [INFO] Basic Node Information (kubectl get nodes -o wide):[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:13,054 [INFO]   NAME          STATUS                     ROLES           AGE   VERSION    INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:13,055 [INFO]   master-m001   Ready,SchedulingDisabled   control-plane   13d   v1.32.11   192.168.56.101   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:13,055 [INFO]   master-m002   Ready                      control-plane   13d   v1.32.11   192.168.56.102   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:13,055 [INFO]   master-m003   Ready                      control-plane   13d   v1.32.11   192.168.56.103   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:13,055 [INFO]   worker-w001   Ready,SchedulingDisabled   <none>          13d   v1.32.11   192.168.56.104   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:13,055 [INFO]   worker-w002   Ready,SchedulingDisabled   <none>          13d   v1.32.11   192.168.56.105   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:13,055 [INFO]   worker-w003   Ready                      <none>          13d   v1.32.11   192.168.56.106   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:13,055 [INFO]   worker-w004   Ready                      <none>          13d   v1.32.11   192.168.56.107   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:13,055 [INFO]   worker-w005   Ready                      <none>          13d   v1.32.11   192.168.56.108   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:13,056 [INFO]   worker-w006   Ready                      <none>          13d   v1.32.11   192.168.56.109   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:13,056 [INFO] [0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: Enhanced Node Status (with taint and cordon indicators):[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:13,056 [INFO]   NAME                STATUS    ROLES           ZONE   CORDONED   TAINTS[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:13,323 [INFO]   master-m001     Ready  ‚úì worker         R1    YES     ‚ö†Ô∏è node-role.kubernetes.io/control-plane, simulated-failure, node.kubernetes.io/unschedulable ‚ö†Ô∏è[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:13,504 [INFO]   master-m002     Ready  ‚úì worker         R2    No       None [0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:13,695 [INFO]   master-m003     Ready  ‚úì worker         R3    No       None [0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:13,867 [INFO]   worker-w001     Ready  ‚úì worker         R1    YES     ‚ö†Ô∏è simulated-failure, node.kubernetes.io/unschedulable ‚ö†Ô∏è[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,047 [INFO]   worker-w002     Ready  ‚úì worker         R1    YES     ‚ö†Ô∏è simulated-failure, node.kubernetes.io/unschedulable ‚ö†Ô∏è[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,222 [INFO]   worker-w003     Ready  ‚úì worker         R2    No       None [0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,477 [INFO]   worker-w004     Ready  ‚úì worker         R2    No       None [0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,658 [INFO]   worker-w005     Ready  ‚úì worker         R3    No       None [0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,842 [INFO]   worker-w006     Ready  ‚úì worker         R3    No       None [0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,843 [INFO] [0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: Legend:[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,843 [INFO]   ‚úì = Node is Ready[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,843 [INFO]   ‚ö†Ô∏è = Warning indicator (NotReady, Cordoned, or has simulated-failure taint)[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,843 [INFO] ============ DETAILED POD INFORMATION ============[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,843 [INFO] Running 'kubectl get pods -o wide' to show detailed pod placement:[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,934 [INFO]   NAMESPACE     NAME                                                       READY   STATUS      RESTARTS       AGE     IP                NODE          NOMINATED NODE   READINESS GATES[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,934 [INFO]   argo          argo-server-5c69cb69db-qjhjz                               1/1     Running     0              4m16s   192.168.15.201    worker-w005   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,934 [INFO]   argo          resilience-bench-ptngm-initialize-metrics-940927476        0/2     Completed   0              5d10h   192.168.221.125   master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,934 [INFO]   argo          resilience-bench-ptngm-run-health-check-1168793163         0/2     Completed   0              5d10h   192.168.221.67    master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,934 [INFO]   argo          resilience-bench-ptngm-run-health-check-1185570782         0/2     Completed   0              5d10h   192.168.221.66    master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,935 [INFO]   argo          resilience-bench-ptngm-run-health-check-1202348401         0/2     Completed   0              5d10h   192.168.221.122   master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,935 [INFO]   argo          resilience-bench-wf87f-initialize-metrics-3677018408       0/2     Completed   0              5d11h   192.168.221.100   master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,935 [INFO]   argo          resilience-bench-wf87f-run-health-check-2914150102         0/2     Completed   0              5d11h   192.168.221.105   master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,935 [INFO]   argo          resilience-bench-wf87f-run-health-check-3931060015         0/2     Completed   0              5d11h   192.168.221.104   master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,935 [INFO]   argo          resilience-bench-wf87f-run-health-check-3947837634         0/2     Completed   0              5d11h   192.168.221.98    master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,935 [INFO]   argo          resilience-bench-wf87f-run-health-check-3964615253         0/2     Completed   0              5d11h   192.168.221.103   master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,935 [INFO]   argo          resilience-bench-wf87f-run-node-simulation-4173146970      0/2     Completed   0              5d11h   192.168.221.101   master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,935 [INFO]   argo          resilience-bench-wf87f-run-rack-simulation-1356070019      0/2     Completed   0              5d11h   192.168.221.109   master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,936 [INFO]   argo          resilience-scaled-heft-pplkg-health-check-2410424775       0/2     Completed   0              4m55s   192.168.221.67    master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,936 [INFO]   argo          resilience-scaled-heft-pplkg-health-check-2427202394       0/2     Completed   0              5m29s   192.168.221.122   master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,936 [INFO]   argo          resilience-scaled-heft-pplkg-node-failure-sim-2649153575   0/2     Completed   0              8m15s   192.168.221.66    master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,936 [INFO]   argo          resilience-scaled-heft-pplkg-node-failure-sim-2665931194   0/2     Completed   0              15m     192.168.221.69    master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,936 [INFO]   argo          resilience-scaled-heft-pplkg-rack-failure-sim-1468935587   0/2     Completed   0              4m35s   192.168.221.70    master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,936 [INFO]   argo          resilience-scaled-heft-pplkg-rack-failure-sim-1485713206   2/2     Running     0              84s     192.168.221.68    master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,936 [INFO]   argo          workflow-controller-ccbd949dc-ghwvq                        1/1     Running     4 (13m ago)    59m     192.168.191.65    worker-w006   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,936 [INFO]   kube-system   calico-kube-controllers-7498b9bb4c-b49jr                   1/1     Running     0              67s     192.168.153.249   worker-w004   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,937 [INFO]   kube-system   calico-node-4zhd4                                          1/1     Running     0              13d     192.168.56.105    worker-w002   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,937 [INFO]   kube-system   calico-node-75nx6                                          1/1     Running     0              13d     192.168.56.109    worker-w006   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,937 [INFO]   kube-system   calico-node-7lkdq                                          1/1     Running     0              13d     192.168.56.104    worker-w001   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,937 [INFO]   kube-system   calico-node-85f8c                                          1/1     Running     0              13d     192.168.56.102    master-m002   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,937 [INFO]   kube-system   calico-node-j8nb9                                          1/1     Running     0              13d     192.168.56.101    master-m001   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,937 [INFO]   kube-system   calico-node-lbcb2                                          1/1     Running     0              13d     192.168.56.106    worker-w003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,937 [INFO]   kube-system   calico-node-wlr5v                                          1/1     Running     0              13d     192.168.56.107    worker-w004   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,937 [INFO]   kube-system   calico-node-xnzjw                                          1/1     Running     0              13d     192.168.56.103    master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,938 [INFO]   kube-system   calico-node-xsltn                                          1/1     Running     0              13d     192.168.56.108    worker-w005   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,938 [INFO]   kube-system   coredns-668d6bf9bc-8dqlk                                   1/1     Running     0              140m    192.168.15.200    worker-w005   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,938 [INFO]   kube-system   coredns-668d6bf9bc-pbq7d                                   1/1     Running     0              72s     192.168.227.254   master-m002   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,938 [INFO]   kube-system   etcd-master-m001                                           1/1     Running     0              13d     192.168.56.101    master-m001   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,938 [INFO]   kube-system   kube-apiserver-master-m001                                 1/1     Running     1 (5d4h ago)   13d     192.168.56.101    master-m001   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,938 [INFO]   kube-system   kube-controller-manager-master-m001                        1/1     Running     16 (13m ago)   13d     192.168.56.101    master-m001   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,938 [INFO]   kube-system   kube-proxy-5lzhj                                           1/1     Running     0              13d     192.168.56.102    master-m002   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,938 [INFO]   kube-system   kube-proxy-7jqkv                                           1/1     Running     0              13d     192.168.56.109    worker-w006   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,939 [INFO]   kube-system   kube-proxy-cv5dt                                           1/1     Running     0              13d     192.168.56.108    worker-w005   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,939 [INFO]   kube-system   kube-proxy-fvpmr                                           1/1     Running     0              13d     192.168.56.101    master-m001   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,939 [INFO]   kube-system   kube-proxy-hgs5z                                           1/1     Running     0              13d     192.168.56.106    worker-w003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,939 [INFO]   kube-system   kube-proxy-kmgqr                                           1/1     Running     0              13d     192.168.56.105    worker-w002   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,939 [INFO]   kube-system   kube-proxy-rdbz5                                           1/1     Running     0              13d     192.168.56.107    worker-w004   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,939 [INFO]   kube-system   kube-proxy-w8mnb                                           1/1     Running     0              13d     192.168.56.103    master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,939 [INFO]   kube-system   kube-proxy-x9jxr                                           1/1     Running     0              13d     192.168.56.104    worker-w001   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,939 [INFO]   kube-system   kube-scheduler-master-m001                                 1/1     Running     15 (13m ago)   13d     192.168.56.101    master-m001   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:14,939 [INFO] [0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: Pod distribution by node:[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:15,042 [INFO]   Node worker-w005: 4 pods[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:15,042 [INFO]   Node master-m003: 19 pods[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:15,042 [INFO]   Node 59m: 1 pods[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:15,042 [INFO]   Node worker-w004: 3 pods[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:15,042 [INFO]   Node worker-w002: 2 pods[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:15,043 [INFO]   Node worker-w006: 2 pods[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:15,043 [INFO]   Node worker-w001: 2 pods[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:15,043 [INFO]   Node master-m002: 3 pods[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:15,043 [INFO]   Node master-m001: 3 pods[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:15,043 [INFO]   Node worker-w003: 2 pods[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:15,043 [INFO]   Node 13d: 3 pods[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:15,043 [INFO] [0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: Filtering for simulation services:[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:15,152 [INFO] Node master-m001 is Ready[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:15,153 [INFO] Node master-m002 is Ready[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:15,153 [INFO] Node master-m003 is Ready[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:15,153 [INFO] Node worker-w001 is Ready[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:15,153 [INFO] Node worker-w002 is Ready[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:15,153 [INFO] Node worker-w003 is Ready[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:15,153 [INFO] Node worker-w004 is Ready[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:15,153 [INFO] Node worker-w005 is Ready[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:15,154 [INFO] Node worker-w006 is Ready[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:15,157 [WARNING] No pods found for etcd-sim[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:15,159 [WARNING] No pods found for postgres-sim[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:15,161 [WARNING] No pods found for redis-sim[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:15,163 [WARNING] No pods found for nginx-sim[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:15,165 [WARNING] No pods found for auth-sim[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:15,165 [INFO] Completed full health check[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:15,165 [INFO] Zone R1 remains down for 10 seconds[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:25,176 [INFO] Running health check before rack power on[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:25,176 [INFO] Starting full health check[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:25,176 [INFO] [0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: ============ DETAILED NODE STATUS ============[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:25,176 [INFO] Basic Node Information (kubectl get nodes -o wide):[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:25,243 [INFO]   NAME          STATUS                     ROLES           AGE   VERSION    INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:25,243 [INFO]   master-m001   Ready,SchedulingDisabled   control-plane   13d   v1.32.11   192.168.56.101   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:25,243 [INFO]   master-m002   Ready                      control-plane   13d   v1.32.11   192.168.56.102   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:25,243 [INFO]   master-m003   Ready                      control-plane   13d   v1.32.11   192.168.56.103   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:25,243 [INFO]   worker-w001   Ready,SchedulingDisabled   <none>          13d   v1.32.11   192.168.56.104   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:25,243 [INFO]   worker-w002   Ready,SchedulingDisabled   <none>          13d   v1.32.11   192.168.56.105   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:25,243 [INFO]   worker-w003   Ready                      <none>          13d   v1.32.11   192.168.56.106   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:25,243 [INFO]   worker-w004   Ready                      <none>          13d   v1.32.11   192.168.56.107   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:25,244 [INFO]   worker-w005   Ready                      <none>          13d   v1.32.11   192.168.56.108   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:25,244 [INFO]   worker-w006   Ready                      <none>          13d   v1.32.11   192.168.56.109   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:25,244 [INFO] [0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: Enhanced Node Status (with taint and cordon indicators):[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:25,244 [INFO]   NAME                STATUS    ROLES           ZONE   CORDONED   TAINTS[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:25,497 [INFO]   master-m001     Ready  ‚úì worker         R1    YES     ‚ö†Ô∏è node-role.kubernetes.io/control-plane, simulated-failure, node.kubernetes.io/unschedulable ‚ö†Ô∏è[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:25,734 [INFO]   master-m002     Ready  ‚úì worker         R2    No       None [0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:25,867 [INFO]   master-m003     Ready  ‚úì worker         R3    No       None [0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:26,007 [INFO]   worker-w001     Ready  ‚úì worker         R1    YES     ‚ö†Ô∏è simulated-failure, node.kubernetes.io/unschedulable ‚ö†Ô∏è[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:26,213 [INFO]   worker-w002     Ready  ‚úì worker         R1    YES     ‚ö†Ô∏è simulated-failure, node.kubernetes.io/unschedulable ‚ö†Ô∏è[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:26,426 [INFO]   worker-w003     Ready  ‚úì worker         R2    No       None [0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:26,606 [INFO]   worker-w004     Ready  ‚úì worker         R2    No       None [0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:26,885 [INFO]   worker-w005     Ready  ‚úì worker         R3    No       None [0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,027 [INFO]   worker-w006     Ready  ‚úì worker         R3    No       None [0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,028 [INFO] [0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: Legend:[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,028 [INFO]   ‚úì = Node is Ready[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,028 [INFO]   ‚ö†Ô∏è = Warning indicator (NotReady, Cordoned, or has simulated-failure taint)[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,028 [INFO] ============ DETAILED POD INFORMATION ============[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,028 [INFO] Running 'kubectl get pods -o wide' to show detailed pod placement:[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,144 [INFO]   NAMESPACE     NAME                                                       READY   STATUS      RESTARTS       AGE     IP                NODE          NOMINATED NODE   READINESS GATES[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,145 [INFO]   argo          argo-server-5c69cb69db-qjhjz                               1/1     Running     0              4m29s   192.168.15.201    worker-w005   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,145 [INFO]   argo          resilience-bench-ptngm-initialize-metrics-940927476        0/2     Completed   0              5d10h   192.168.221.125   master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,145 [INFO]   argo          resilience-bench-ptngm-run-health-check-1168793163         0/2     Completed   0              5d10h   192.168.221.67    master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,145 [INFO]   argo          resilience-bench-ptngm-run-health-check-1185570782         0/2     Completed   0              5d10h   192.168.221.66    master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,145 [INFO]   argo          resilience-bench-ptngm-run-health-check-1202348401         0/2     Completed   0              5d10h   192.168.221.122   master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,145 [INFO]   argo          resilience-bench-wf87f-initialize-metrics-3677018408       0/2     Completed   0              5d11h   192.168.221.100   master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,145 [INFO]   argo          resilience-bench-wf87f-run-health-check-2914150102         0/2     Completed   0              5d11h   192.168.221.105   master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,145 [INFO]   argo          resilience-bench-wf87f-run-health-check-3931060015         0/2     Completed   0              5d11h   192.168.221.104   master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,145 [INFO]   argo          resilience-bench-wf87f-run-health-check-3947837634         0/2     Completed   0              5d11h   192.168.221.98    master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,145 [INFO]   argo          resilience-bench-wf87f-run-health-check-3964615253         0/2     Completed   0              5d11h   192.168.221.103   master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,145 [INFO]   argo          resilience-bench-wf87f-run-node-simulation-4173146970      0/2     Completed   0              5d11h   192.168.221.101   master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,146 [INFO]   argo          resilience-bench-wf87f-run-rack-simulation-1356070019      0/2     Completed   0              5d11h   192.168.221.109   master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,146 [INFO]   argo          resilience-scaled-heft-pplkg-health-check-2410424775       0/2     Completed   0              5m8s    192.168.221.67    master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,146 [INFO]   argo          resilience-scaled-heft-pplkg-health-check-2427202394       0/2     Completed   0              5m42s   192.168.221.122   master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,146 [INFO]   argo          resilience-scaled-heft-pplkg-node-failure-sim-2649153575   0/2     Completed   0              8m28s   192.168.221.66    master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,146 [INFO]   argo          resilience-scaled-heft-pplkg-node-failure-sim-2665931194   0/2     Completed   0              15m     192.168.221.69    master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,146 [INFO]   argo          resilience-scaled-heft-pplkg-rack-failure-sim-1468935587   0/2     Completed   0              4m48s   192.168.221.70    master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,146 [INFO]   argo          resilience-scaled-heft-pplkg-rack-failure-sim-1485713206   2/2     Running     0              97s     192.168.221.68    master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,146 [INFO]   argo          workflow-controller-ccbd949dc-ghwvq                        1/1     Running     4 (14m ago)    59m     192.168.191.65    worker-w006   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,146 [INFO]   kube-system   calico-kube-controllers-7498b9bb4c-b49jr                   1/1     Running     0              80s     192.168.153.249   worker-w004   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,146 [INFO]   kube-system   calico-node-4zhd4                                          1/1     Running     0              13d     192.168.56.105    worker-w002   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,146 [INFO]   kube-system   calico-node-75nx6                                          1/1     Running     0              13d     192.168.56.109    worker-w006   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,147 [INFO]   kube-system   calico-node-7lkdq                                          1/1     Running     0              13d     192.168.56.104    worker-w001   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,147 [INFO]   kube-system   calico-node-85f8c                                          1/1     Running     0              13d     192.168.56.102    master-m002   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,147 [INFO]   kube-system   calico-node-j8nb9                                          1/1     Running     0              13d     192.168.56.101    master-m001   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,147 [INFO]   kube-system   calico-node-lbcb2                                          1/1     Running     0              13d     192.168.56.106    worker-w003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,147 [INFO]   kube-system   calico-node-wlr5v                                          1/1     Running     0              13d     192.168.56.107    worker-w004   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,147 [INFO]   kube-system   calico-node-xnzjw                                          1/1     Running     0              13d     192.168.56.103    master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,147 [INFO]   kube-system   calico-node-xsltn                                          1/1     Running     0              13d     192.168.56.108    worker-w005   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,147 [INFO]   kube-system   coredns-668d6bf9bc-8dqlk                                   1/1     Running     0              140m    192.168.15.200    worker-w005   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,147 [INFO]   kube-system   coredns-668d6bf9bc-pbq7d                                   1/1     Running     0              85s     192.168.227.254   master-m002   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,147 [INFO]   kube-system   etcd-master-m001                                           1/1     Running     0              13d     192.168.56.101    master-m001   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,147 [INFO]   kube-system   kube-apiserver-master-m001                                 1/1     Running     1 (5d4h ago)   13d     192.168.56.101    master-m001   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,147 [INFO]   kube-system   kube-controller-manager-master-m001                        1/1     Running     16 (14m ago)   13d     192.168.56.101    master-m001   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,147 [INFO]   kube-system   kube-proxy-5lzhj                                           1/1     Running     0              13d     192.168.56.102    master-m002   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,147 [INFO]   kube-system   kube-proxy-7jqkv                                           1/1     Running     0              13d     192.168.56.109    worker-w006   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,147 [INFO]   kube-system   kube-proxy-cv5dt                                           1/1     Running     0              13d     192.168.56.108    worker-w005   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,148 [INFO]   kube-system   kube-proxy-fvpmr                                           1/1     Running     0              13d     192.168.56.101    master-m001   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,148 [INFO]   kube-system   kube-proxy-hgs5z                                           1/1     Running     0              13d     192.168.56.106    worker-w003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,148 [INFO]   kube-system   kube-proxy-kmgqr                                           1/1     Running     0              13d     192.168.56.105    worker-w002   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,148 [INFO]   kube-system   kube-proxy-rdbz5                                           1/1     Running     0              13d     192.168.56.107    worker-w004   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,148 [INFO]   kube-system   kube-proxy-w8mnb                                           1/1     Running     0              13d     192.168.56.103    master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,148 [INFO]   kube-system   kube-proxy-x9jxr                                           1/1     Running     0              13d     192.168.56.104    worker-w001   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,148 [INFO]   kube-system   kube-scheduler-master-m001                                 1/1     Running     15 (13m ago)   13d     192.168.56.101    master-m001   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,148 [INFO] [0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: Pod distribution by node:[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,250 [INFO]   Node worker-w005: 4 pods[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,250 [INFO]   Node master-m003: 19 pods[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,250 [INFO]   Node 59m: 1 pods[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,251 [INFO]   Node worker-w004: 3 pods[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,251 [INFO]   Node worker-w002: 2 pods[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,251 [INFO]   Node worker-w006: 2 pods[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,251 [INFO]   Node worker-w001: 2 pods[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,251 [INFO]   Node master-m002: 3 pods[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,251 [INFO]   Node master-m001: 3 pods[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,251 [INFO]   Node worker-w003: 2 pods[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,251 [INFO]   Node 13d: 3 pods[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,251 [INFO] [0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: Filtering for simulation services:[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,356 [INFO] Node master-m001 is Ready[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,356 [INFO] Node master-m002 is Ready[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,356 [INFO] Node master-m003 is Ready[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,356 [INFO] Node worker-w001 is Ready[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,357 [INFO] Node worker-w002 is Ready[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,357 [INFO] Node worker-w003 is Ready[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,357 [INFO] Node worker-w004 is Ready[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,357 [INFO] Node worker-w005 is Ready[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,357 [INFO] Node worker-w006 is Ready[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,361 [WARNING] No pods found for etcd-sim[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,363 [WARNING] No pods found for postgres-sim[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,365 [WARNING] No pods found for redis-sim[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,367 [WARNING] No pods found for nginx-sim[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,369 [WARNING] No pods found for auth-sim[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,369 [INFO] Completed full health check[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,369 [INFO] Simulating node recovery for master-m001 using Kubernetes API[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,385 [INFO] Removed simulated-failure taint from node master-m001[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,397 [INFO] Node master-m001 uncordoned and ready[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:27,398 [INFO] Node master-m001 powered on (delay 5s)[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:32,400 [INFO] Simulating node recovery for worker-w001 using Kubernetes API[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:32,416 [INFO] Removed simulated-failure taint from node worker-w001[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:32,426 [INFO] Node worker-w001 uncordoned and ready[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:32,427 [INFO] Node worker-w001 powered on (delay 5s)[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:37,467 [INFO] Simulating node recovery for worker-w002 using Kubernetes API[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:37,482 [INFO] Removed simulated-failure taint from node worker-w002[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:37,495 [INFO] Node worker-w002 uncordoned and ready[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:37,495 [INFO] Node worker-w002 powered on (delay 5s)[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:01:42,500 [INFO] Waiting 60 seconds for the cluster to stabilize after recovery...[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:42,540 [INFO] Running final health check[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:42,540 [INFO] Starting full health check[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:42,540 [INFO] [0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: ============ DETAILED NODE STATUS ============[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:42,541 [INFO] Basic Node Information (kubectl get nodes -o wide):[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:42,602 [INFO]   NAME          STATUS   ROLES           AGE   VERSION    INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:42,602 [INFO]   master-m001   Ready    control-plane   13d   v1.32.11   192.168.56.101   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:42,602 [INFO]   master-m002   Ready    control-plane   13d   v1.32.11   192.168.56.102   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:42,602 [INFO]   master-m003   Ready    control-plane   13d   v1.32.11   192.168.56.103   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:42,602 [INFO]   worker-w001   Ready    <none>          13d   v1.32.11   192.168.56.104   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:42,603 [INFO]   worker-w002   Ready    <none>          13d   v1.32.11   192.168.56.105   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:42,603 [INFO]   worker-w003   Ready    <none>          13d   v1.32.11   192.168.56.106   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:42,603 [INFO]   worker-w004   Ready    <none>          13d   v1.32.11   192.168.56.107   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:42,603 [INFO]   worker-w005   Ready    <none>          13d   v1.32.11   192.168.56.108   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:42,603 [INFO]   worker-w006   Ready    <none>          13d   v1.32.11   192.168.56.109   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:42,603 [INFO] [0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: Enhanced Node Status (with taint and cordon indicators):[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:42,603 [INFO]   NAME                STATUS    ROLES           ZONE   CORDONED   TAINTS[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:42,836 [INFO]   master-m001     Ready  ‚úì worker         R1    No       node-role.kubernetes.io/control-plane [0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:42,989 [INFO]   master-m002     Ready  ‚úì worker         R2    No       None [0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:43,116 [INFO]   master-m003     Ready  ‚úì worker         R3    No       None [0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:43,257 [INFO]   worker-w001     Ready  ‚úì worker         R1    No       None [0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:43,383 [INFO]   worker-w002     Ready  ‚úì worker         R1    No       None [0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:43,507 [INFO]   worker-w003     Ready  ‚úì worker         R2    No       None [0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:43,736 [INFO]   worker-w004     Ready  ‚úì worker         R2    No       None [0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:43,909 [INFO]   worker-w005     Ready  ‚úì worker         R3    No       None [0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,091 [INFO]   worker-w006     Ready  ‚úì worker         R3    No       None [0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,092 [INFO] [0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: Legend:[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,092 [INFO]   ‚úì = Node is Ready[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,092 [INFO]   ‚ö†Ô∏è = Warning indicator (NotReady, Cordoned, or has simulated-failure taint)[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,092 [INFO] ============ DETAILED POD INFORMATION ============[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,092 [INFO] Running 'kubectl get pods -o wide' to show detailed pod placement:[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,190 [INFO]   NAMESPACE     NAME                                                       READY   STATUS      RESTARTS       AGE     IP                NODE          NOMINATED NODE   READINESS GATES[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,190 [INFO]   argo          argo-server-5c69cb69db-qjhjz                               1/1     Running     0              5m46s   192.168.15.201    worker-w005   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,190 [INFO]   argo          resilience-bench-ptngm-initialize-metrics-940927476        0/2     Completed   0              5d10h   192.168.221.125   master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,190 [INFO]   argo          resilience-bench-ptngm-run-health-check-1168793163         0/2     Completed   0              5d10h   192.168.221.67    master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,190 [INFO]   argo          resilience-bench-ptngm-run-health-check-1185570782         0/2     Completed   0              5d10h   192.168.221.66    master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,191 [INFO]   argo          resilience-bench-ptngm-run-health-check-1202348401         0/2     Completed   0              5d10h   192.168.221.122   master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,191 [INFO]   argo          resilience-bench-wf87f-initialize-metrics-3677018408       0/2     Completed   0              5d11h   192.168.221.100   master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,191 [INFO]   argo          resilience-bench-wf87f-run-health-check-2914150102         0/2     Completed   0              5d11h   192.168.221.105   master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,191 [INFO]   argo          resilience-bench-wf87f-run-health-check-3931060015         0/2     Completed   0              5d11h   192.168.221.104   master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,191 [INFO]   argo          resilience-bench-wf87f-run-health-check-3947837634         0/2     Completed   0              5d11h   192.168.221.98    master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,191 [INFO]   argo          resilience-bench-wf87f-run-health-check-3964615253         0/2     Completed   0              5d11h   192.168.221.103   master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,191 [INFO]   argo          resilience-bench-wf87f-run-node-simulation-4173146970      0/2     Completed   0              5d11h   192.168.221.101   master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,191 [INFO]   argo          resilience-bench-wf87f-run-rack-simulation-1356070019      0/2     Completed   0              5d11h   192.168.221.109   master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,191 [INFO]   argo          resilience-scaled-heft-pplkg-health-check-2410424775       0/2     Completed   0              6m25s   192.168.221.67    master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,191 [INFO]   argo          resilience-scaled-heft-pplkg-health-check-2427202394       0/2     Completed   0              6m59s   192.168.221.122   master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,192 [INFO]   argo          resilience-scaled-heft-pplkg-node-failure-sim-2649153575   0/2     Completed   0              9m45s   192.168.221.66    master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,192 [INFO]   argo          resilience-scaled-heft-pplkg-node-failure-sim-2665931194   0/2     Completed   0              16m     192.168.221.69    master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,192 [INFO]   argo          resilience-scaled-heft-pplkg-rack-failure-sim-1468935587   0/2     Completed   0              6m5s    192.168.221.70    master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,192 [INFO]   argo          resilience-scaled-heft-pplkg-rack-failure-sim-1485713206   2/2     Running     0              2m54s   192.168.221.68    master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,192 [INFO]   argo          workflow-controller-ccbd949dc-ghwvq                        1/1     Running     4 (15m ago)    61m     192.168.191.65    worker-w006   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,192 [INFO]   kube-system   calico-kube-controllers-7498b9bb4c-b49jr                   1/1     Running     0              2m37s   192.168.153.249   worker-w004   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,192 [INFO]   kube-system   calico-node-4zhd4                                          1/1     Running     0              13d     192.168.56.105    worker-w002   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,192 [INFO]   kube-system   calico-node-75nx6                                          1/1     Running     0              13d     192.168.56.109    worker-w006   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,192 [INFO]   kube-system   calico-node-7lkdq                                          1/1     Running     0              13d     192.168.56.104    worker-w001   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,192 [INFO]   kube-system   calico-node-85f8c                                          1/1     Running     0              13d     192.168.56.102    master-m002   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,193 [INFO]   kube-system   calico-node-j8nb9                                          1/1     Running     0              13d     192.168.56.101    master-m001   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,193 [INFO]   kube-system   calico-node-lbcb2                                          1/1     Running     0              13d     192.168.56.106    worker-w003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,193 [INFO]   kube-system   calico-node-wlr5v                                          1/1     Running     0              13d     192.168.56.107    worker-w004   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,193 [INFO]   kube-system   calico-node-xnzjw                                          1/1     Running     0              13d     192.168.56.103    master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,193 [INFO]   kube-system   calico-node-xsltn                                          1/1     Running     0              13d     192.168.56.108    worker-w005   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,194 [INFO]   kube-system   coredns-668d6bf9bc-8dqlk                                   1/1     Running     0              142m    192.168.15.200    worker-w005   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,194 [INFO]   kube-system   coredns-668d6bf9bc-pbq7d                                   1/1     Running     0              2m42s   192.168.227.254   master-m002   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,194 [INFO]   kube-system   etcd-master-m001                                           1/1     Running     0              13d     192.168.56.101    master-m001   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,194 [INFO]   kube-system   kube-apiserver-master-m001                                 1/1     Running     1 (5d4h ago)   13d     192.168.56.101    master-m001   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,194 [INFO]   kube-system   kube-controller-manager-master-m001                        1/1     Running     16 (15m ago)   13d     192.168.56.101    master-m001   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,194 [INFO]   kube-system   kube-proxy-5lzhj                                           1/1     Running     0              13d     192.168.56.102    master-m002   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,194 [INFO]   kube-system   kube-proxy-7jqkv                                           1/1     Running     0              13d     192.168.56.109    worker-w006   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,194 [INFO]   kube-system   kube-proxy-cv5dt                                           1/1     Running     0              13d     192.168.56.108    worker-w005   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,194 [INFO]   kube-system   kube-proxy-fvpmr                                           1/1     Running     0              13d     192.168.56.101    master-m001   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,194 [INFO]   kube-system   kube-proxy-hgs5z                                           1/1     Running     0              13d     192.168.56.106    worker-w003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,195 [INFO]   kube-system   kube-proxy-kmgqr                                           1/1     Running     0              13d     192.168.56.105    worker-w002   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,195 [INFO]   kube-system   kube-proxy-rdbz5                                           1/1     Running     0              13d     192.168.56.107    worker-w004   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,195 [INFO]   kube-system   kube-proxy-w8mnb                                           1/1     Running     0              13d     192.168.56.103    master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,195 [INFO]   kube-system   kube-proxy-x9jxr                                           1/1     Running     0              13d     192.168.56.104    worker-w001   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,195 [INFO]   kube-system   kube-scheduler-master-m001                                 1/1     Running     15 (15m ago)   13d     192.168.56.101    master-m001   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,195 [INFO] [0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: Pod distribution by node:[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,302 [INFO]   Node worker-w005: 4 pods[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,302 [INFO]   Node master-m003: 19 pods[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,302 [INFO]   Node 61m: 1 pods[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,302 [INFO]   Node worker-w004: 3 pods[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,302 [INFO]   Node worker-w002: 2 pods[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,302 [INFO]   Node worker-w006: 2 pods[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,303 [INFO]   Node worker-w001: 2 pods[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,303 [INFO]   Node master-m002: 3 pods[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,303 [INFO]   Node master-m001: 3 pods[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,303 [INFO]   Node worker-w003: 2 pods[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,303 [INFO]   Node 13d: 3 pods[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,303 [INFO] [0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: Filtering for simulation services:[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,436 [INFO] Node master-m001 is Ready[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,436 [INFO] Node master-m002 is Ready[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,436 [INFO] Node master-m003 is Ready[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,437 [INFO] Node worker-w001 is Ready[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,437 [INFO] Node worker-w002 is Ready[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,437 [INFO] Node worker-w003 is Ready[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,437 [INFO] Node worker-w004 is Ready[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,437 [INFO] Node worker-w005 is Ready[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,438 [INFO] Node worker-w006 is Ready[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,440 [WARNING] No pods found for etcd-sim[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,442 [WARNING] No pods found for postgres-sim[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,444 [WARNING] No pods found for redis-sim[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,446 [WARNING] No pods found for nginx-sim[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,448 [WARNING] No pods found for auth-sim[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,448 [INFO] Completed full health check[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: 2026-01-08 08:02:44,448 [INFO] Rack R1 has been fully restored[0m
[36mresilience-scaled-heft-pplkg-rack-failure-sim-1485713206: time="2026-01-08T08:02:44.787Z" level=info msg="sub-process exited" argo=true error="<nil>"[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: Log directory created/verified: /app/logs[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: File logging configured successfully[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:06,781 [INFO] Loaded in-cluster Kubernetes config[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:06,782 [INFO] Running on host: resilience-scaled-heft-pplkg-health-check-277206534[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:06,782 [INFO] Detected current node: master-m003, zone: R3[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:06,782 [INFO] Checking if we have permissions to modify nodes...[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:06,834 [INFO] Testing permissions using node: master-m001[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:06,843 [INFO] Permission check successful - we can modify nodes[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:06,843 [INFO] Using real Kubernetes API for node control[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:06,844 [INFO] Action received: health-check[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:06,844 [INFO] Stabilization time: 10 seconds[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:06,845 [INFO] Starting full health check[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:06,845 [INFO] [0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: ============ DETAILED NODE STATUS ============[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:06,845 [INFO] Basic Node Information (kubectl get nodes -o wide):[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:06,926 [INFO]   NAME          STATUS   ROLES           AGE   VERSION    INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:06,926 [INFO]   master-m001   Ready    control-plane   13d   v1.32.11   192.168.56.101   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:06,926 [INFO]   master-m002   Ready    control-plane   13d   v1.32.11   192.168.56.102   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:06,926 [INFO]   master-m003   Ready    control-plane   13d   v1.32.11   192.168.56.103   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:06,927 [INFO]   worker-w001   Ready    <none>          13d   v1.32.11   192.168.56.104   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:06,927 [INFO]   worker-w002   Ready    <none>          13d   v1.32.11   192.168.56.105   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:06,927 [INFO]   worker-w003   Ready    <none>          13d   v1.32.11   192.168.56.106   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:06,927 [INFO]   worker-w004   Ready    <none>          13d   v1.32.11   192.168.56.107   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:06,927 [INFO]   worker-w005   Ready    <none>          13d   v1.32.11   192.168.56.108   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:06,927 [INFO]   worker-w006   Ready    <none>          13d   v1.32.11   192.168.56.109   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:06,927 [INFO] [0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: Enhanced Node Status (with taint and cordon indicators):[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:06,927 [INFO]   NAME                STATUS    ROLES           ZONE   CORDONED   TAINTS[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:07,168 [INFO]   master-m001     Ready  ‚úì worker         R1    No       node-role.kubernetes.io/control-plane [0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:07,355 [INFO]   master-m002     Ready  ‚úì worker         R2    No       None [0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:07,538 [INFO]   master-m003     Ready  ‚úì worker         R3    No       None [0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:07,727 [INFO]   worker-w001     Ready  ‚úì worker         R1    No       None [0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,080 [INFO]   worker-w002     Ready  ‚úì worker         R1    No       None [0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,220 [INFO]   worker-w003     Ready  ‚úì worker         R2    No       None [0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,333 [INFO]   worker-w004     Ready  ‚úì worker         R2    No       None [0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,475 [INFO]   worker-w005     Ready  ‚úì worker         R3    No       None [0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,612 [INFO]   worker-w006     Ready  ‚úì worker         R3    No       None [0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,612 [INFO] [0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: Legend:[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,612 [INFO]   ‚úì = Node is Ready[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,612 [INFO]   ‚ö†Ô∏è = Warning indicator (NotReady, Cordoned, or has simulated-failure taint)[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,612 [INFO] ============ DETAILED POD INFORMATION ============[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,612 [INFO] Running 'kubectl get pods -o wide' to show detailed pod placement:[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,674 [INFO]   NAMESPACE     NAME                                                       READY   STATUS      RESTARTS       AGE     IP                NODE          NOMINATED NODE   READINESS GATES[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,674 [INFO]   argo          argo-server-5c69cb69db-qjhjz                               1/1     Running     0              6m10s   192.168.15.201    worker-w005   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,674 [INFO]   argo          resilience-bench-ptngm-initialize-metrics-940927476        0/2     Completed   0              5d10h   192.168.221.125   master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,674 [INFO]   argo          resilience-bench-ptngm-run-health-check-1168793163         0/2     Completed   0              5d10h   192.168.221.67    master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,674 [INFO]   argo          resilience-bench-ptngm-run-health-check-1185570782         0/2     Completed   0              5d10h   192.168.221.66    master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,674 [INFO]   argo          resilience-bench-ptngm-run-health-check-1202348401         0/2     Completed   0              5d10h   192.168.221.122   master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,674 [INFO]   argo          resilience-bench-wf87f-initialize-metrics-3677018408       0/2     Completed   0              5d11h   192.168.221.100   master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,674 [INFO]   argo          resilience-bench-wf87f-run-health-check-2914150102         0/2     Completed   0              5d11h   192.168.221.105   master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,674 [INFO]   argo          resilience-bench-wf87f-run-health-check-3931060015         0/2     Completed   0              5d11h   192.168.221.104   master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,674 [INFO]   argo          resilience-bench-wf87f-run-health-check-3947837634         0/2     Completed   0              5d11h   192.168.221.98    master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,674 [INFO]   argo          resilience-bench-wf87f-run-health-check-3964615253         0/2     Completed   0              5d11h   192.168.221.103   master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,674 [INFO]   argo          resilience-bench-wf87f-run-node-simulation-4173146970      0/2     Completed   0              5d11h   192.168.221.101   master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,674 [INFO]   argo          resilience-bench-wf87f-run-rack-simulation-1356070019      0/2     Completed   0              5d11h   192.168.221.109   master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,674 [INFO]   argo          resilience-scaled-heft-pplkg-health-check-2410424775       0/2     Completed   0              6m49s   192.168.221.67    master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,674 [INFO]   argo          resilience-scaled-heft-pplkg-health-check-2427202394       0/2     Completed   0              7m23s   192.168.221.122   master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,674 [INFO]   argo          resilience-scaled-heft-pplkg-health-check-277206534        2/2     Running     0              13s     192.168.221.73    master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,674 [INFO]   argo          resilience-scaled-heft-pplkg-node-failure-sim-2649153575   0/2     Completed   0              10m     192.168.221.66    master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,674 [INFO]   argo          resilience-scaled-heft-pplkg-node-failure-sim-2665931194   0/2     Completed   0              17m     192.168.221.69    master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,674 [INFO]   argo          resilience-scaled-heft-pplkg-rack-failure-sim-1468935587   0/2     Completed   0              6m29s   192.168.221.70    master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,674 [INFO]   argo          resilience-scaled-heft-pplkg-rack-failure-sim-1485713206   0/2     Completed   0              3m18s   192.168.221.68    master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,674 [INFO]   argo          workflow-controller-ccbd949dc-ghwvq                        1/1     Running     4 (15m ago)    61m     192.168.191.65    worker-w006   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,674 [INFO]   kube-system   calico-kube-controllers-7498b9bb4c-b49jr                   1/1     Running     0              3m1s    192.168.153.249   worker-w004   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,674 [INFO]   kube-system   calico-node-4zhd4                                          1/1     Running     0              13d     192.168.56.105    worker-w002   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,674 [INFO]   kube-system   calico-node-75nx6                                          1/1     Running     0              13d     192.168.56.109    worker-w006   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,674 [INFO]   kube-system   calico-node-7lkdq                                          1/1     Running     0              13d     192.168.56.104    worker-w001   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,674 [INFO]   kube-system   calico-node-85f8c                                          1/1     Running     0              13d     192.168.56.102    master-m002   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,674 [INFO]   kube-system   calico-node-j8nb9                                          1/1     Running     0              13d     192.168.56.101    master-m001   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,674 [INFO]   kube-system   calico-node-lbcb2                                          1/1     Running     0              13d     192.168.56.106    worker-w003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,674 [INFO]   kube-system   calico-node-wlr5v                                          1/1     Running     0              13d     192.168.56.107    worker-w004   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,674 [INFO]   kube-system   calico-node-xnzjw                                          1/1     Running     0              13d     192.168.56.103    master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,674 [INFO]   kube-system   calico-node-xsltn                                          1/1     Running     0              13d     192.168.56.108    worker-w005   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,674 [INFO]   kube-system   coredns-668d6bf9bc-8dqlk                                   1/1     Running     0              142m    192.168.15.200    worker-w005   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,674 [INFO]   kube-system   coredns-668d6bf9bc-pbq7d                                   1/1     Running     0              3m6s    192.168.227.254   master-m002   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,674 [INFO]   kube-system   etcd-master-m001                                           1/1     Running     0              13d     192.168.56.101    master-m001   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,674 [INFO]   kube-system   kube-apiserver-master-m001                                 1/1     Running     1 (5d4h ago)   13d     192.168.56.101    master-m001   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,675 [INFO]   kube-system   kube-controller-manager-master-m001                        1/1     Running     16 (15m ago)   13d     192.168.56.101    master-m001   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,675 [INFO]   kube-system   kube-proxy-5lzhj                                           1/1     Running     0              13d     192.168.56.102    master-m002   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,675 [INFO]   kube-system   kube-proxy-7jqkv                                           1/1     Running     0              13d     192.168.56.109    worker-w006   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,675 [INFO]   kube-system   kube-proxy-cv5dt                                           1/1     Running     0              13d     192.168.56.108    worker-w005   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,675 [INFO]   kube-system   kube-proxy-fvpmr                                           1/1     Running     0              13d     192.168.56.101    master-m001   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,675 [INFO]   kube-system   kube-proxy-hgs5z                                           1/1     Running     0              13d     192.168.56.106    worker-w003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,675 [INFO]   kube-system   kube-proxy-kmgqr                                           1/1     Running     0              13d     192.168.56.105    worker-w002   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,675 [INFO]   kube-system   kube-proxy-rdbz5                                           1/1     Running     0              13d     192.168.56.107    worker-w004   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,675 [INFO]   kube-system   kube-proxy-w8mnb                                           1/1     Running     0              13d     192.168.56.103    master-m003   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,675 [INFO]   kube-system   kube-proxy-x9jxr                                           1/1     Running     0              13d     192.168.56.104    worker-w001   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,675 [INFO]   kube-system   kube-scheduler-master-m001                                 1/1     Running     15 (15m ago)   13d     192.168.56.101    master-m001   <none>           <none>[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,675 [INFO] [0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: Pod distribution by node:[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,750 [INFO]   Node worker-w005: 4 pods[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,750 [INFO]   Node master-m003: 20 pods[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,750 [INFO]   Node 61m: 1 pods[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,750 [INFO]   Node worker-w004: 3 pods[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,750 [INFO]   Node worker-w002: 2 pods[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,750 [INFO]   Node worker-w006: 2 pods[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,750 [INFO]   Node worker-w001: 2 pods[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,750 [INFO]   Node master-m002: 3 pods[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,750 [INFO]   Node master-m001: 3 pods[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,750 [INFO]   Node worker-w003: 2 pods[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,750 [INFO]   Node 13d: 3 pods[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,750 [INFO] [0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: Filtering for simulation services:[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,817 [INFO] Node master-m001 is Ready[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,817 [INFO] Node master-m002 is Ready[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,817 [INFO] Node master-m003 is Ready[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,817 [INFO] Node worker-w001 is Ready[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,817 [INFO] Node worker-w002 is Ready[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,817 [INFO] Node worker-w003 is Ready[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,817 [INFO] Node worker-w004 is Ready[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,818 [INFO] Node worker-w005 is Ready[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,818 [INFO] Node worker-w006 is Ready[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,820 [WARNING] No pods found for etcd-sim[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,821 [WARNING] No pods found for postgres-sim[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,823 [WARNING] No pods found for redis-sim[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,825 [WARNING] No pods found for nginx-sim[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,826 [WARNING] No pods found for auth-sim[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: 2026-01-08 08:03:08,826 [INFO] Completed full health check[0m
[36mresilience-scaled-heft-pplkg-health-check-277206534: time="2026-01-08T08:03:09.377Z" level=info msg="sub-process exited" argo=true error="<nil>"[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: Log directory created/verified: /app/logs[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: File logging configured successfully[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:23,666 [INFO] Loaded in-cluster Kubernetes config[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:23,666 [INFO] Running on host: resilience-scaled-heft-pplkg-health-check-260428915[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:23,666 [INFO] Detected current node: master-m003, zone: R3[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:23,666 [INFO] Checking if we have permissions to modify nodes...[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:23,687 [INFO] Testing permissions using node: master-m001[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:23,694 [INFO] Permission check successful - we can modify nodes[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:23,694 [INFO] Using real Kubernetes API for node control[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:23,694 [INFO] Action received: health-check[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:23,694 [INFO] Stabilization time: 10 seconds[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:23,695 [INFO] Starting full health check[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:23,695 [INFO] [0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: ============ DETAILED NODE STATUS ============[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:23,695 [INFO] Basic Node Information (kubectl get nodes -o wide):[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:23,773 [INFO]   NAME          STATUS   ROLES           AGE   VERSION    INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:23,774 [INFO]   master-m001   Ready    control-plane   13d   v1.32.11   192.168.56.101   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:23,774 [INFO]   master-m002   Ready    control-plane   13d   v1.32.11   192.168.56.102   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:23,774 [INFO]   master-m003   Ready    control-plane   13d   v1.32.11   192.168.56.103   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:23,774 [INFO]   worker-w001   Ready    <none>          13d   v1.32.11   192.168.56.104   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:23,774 [INFO]   worker-w002   Ready    <none>          13d   v1.32.11   192.168.56.105   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:23,774 [INFO]   worker-w003   Ready    <none>          13d   v1.32.11   192.168.56.106   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:23,775 [INFO]   worker-w004   Ready    <none>          13d   v1.32.11   192.168.56.107   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:23,775 [INFO]   worker-w005   Ready    <none>          13d   v1.32.11   192.168.56.108   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:23,775 [INFO]   worker-w006   Ready    <none>          13d   v1.32.11   192.168.56.109   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:23,775 [INFO] [0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: Enhanced Node Status (with taint and cordon indicators):[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:23,775 [INFO]   NAME                STATUS    ROLES           ZONE   CORDONED   TAINTS[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:23,986 [INFO]   master-m001     Ready  ‚úì worker         R1    No       node-role.kubernetes.io/control-plane [0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:24,134 [INFO]   master-m002     Ready  ‚úì worker         R2    No       None [0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:24,317 [INFO]   master-m003     Ready  ‚úì worker         R3    No       None [0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:24,460 [INFO]   worker-w001     Ready  ‚úì worker         R1    No       None [0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:24,612 [INFO]   worker-w002     Ready  ‚úì worker         R1    No       None [0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:24,817 [INFO]   worker-w003     Ready  ‚úì worker         R2    No       None [0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,025 [INFO]   worker-w004     Ready  ‚úì worker         R2    No       None [0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,172 [INFO]   worker-w005     Ready  ‚úì worker         R3    No       None [0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,328 [INFO]   worker-w006     Ready  ‚úì worker         R3    No       None [0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,328 [INFO] [0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: Legend:[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,328 [INFO]   ‚úì = Node is Ready[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,328 [INFO]   ‚ö†Ô∏è = Warning indicator (NotReady, Cordoned, or has simulated-failure taint)[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,328 [INFO] ============ DETAILED POD INFORMATION ============[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,328 [INFO] Running 'kubectl get pods -o wide' to show detailed pod placement:[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,386 [INFO]   NAMESPACE     NAME                                                       READY   STATUS      RESTARTS       AGE     IP                NODE          NOMINATED NODE   READINESS GATES[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,387 [INFO]   argo          argo-server-5c69cb69db-qjhjz                               1/1     Running     0              6m27s   192.168.15.201    worker-w005   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,387 [INFO]   argo          resilience-bench-ptngm-initialize-metrics-940927476        0/2     Completed   0              5d10h   192.168.221.125   master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,387 [INFO]   argo          resilience-bench-ptngm-run-health-check-1168793163         0/2     Completed   0              5d10h   192.168.221.67    master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,387 [INFO]   argo          resilience-bench-ptngm-run-health-check-1185570782         0/2     Completed   0              5d10h   192.168.221.66    master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,387 [INFO]   argo          resilience-bench-ptngm-run-health-check-1202348401         0/2     Completed   0              5d10h   192.168.221.122   master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,387 [INFO]   argo          resilience-bench-wf87f-initialize-metrics-3677018408       0/2     Completed   0              5d11h   192.168.221.100   master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,387 [INFO]   argo          resilience-bench-wf87f-run-health-check-2914150102         0/2     Completed   0              5d11h   192.168.221.105   master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,387 [INFO]   argo          resilience-bench-wf87f-run-health-check-3931060015         0/2     Completed   0              5d11h   192.168.221.104   master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,388 [INFO]   argo          resilience-bench-wf87f-run-health-check-3947837634         0/2     Completed   0              5d11h   192.168.221.98    master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,388 [INFO]   argo          resilience-bench-wf87f-run-health-check-3964615253         0/2     Completed   0              5d11h   192.168.221.103   master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,388 [INFO]   argo          resilience-bench-wf87f-run-node-simulation-4173146970      0/2     Completed   0              5d11h   192.168.221.101   master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,388 [INFO]   argo          resilience-bench-wf87f-run-rack-simulation-1356070019      0/2     Completed   0              5d11h   192.168.221.109   master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,388 [INFO]   argo          resilience-scaled-heft-pplkg-health-check-2410424775       0/2     Completed   0              7m6s    192.168.221.67    master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,388 [INFO]   argo          resilience-scaled-heft-pplkg-health-check-2427202394       0/2     Completed   0              7m40s   192.168.221.122   master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,388 [INFO]   argo          resilience-scaled-heft-pplkg-health-check-260428915        2/2     Running     0              9s      192.168.221.71    master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,388 [INFO]   argo          resilience-scaled-heft-pplkg-health-check-277206534        0/2     Completed   0              30s     192.168.221.73    master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,388 [INFO]   argo          resilience-scaled-heft-pplkg-node-failure-sim-2649153575   0/2     Completed   0              10m     192.168.221.66    master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,388 [INFO]   argo          resilience-scaled-heft-pplkg-node-failure-sim-2665931194   0/2     Completed   0              17m     192.168.221.69    master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,389 [INFO]   argo          resilience-scaled-heft-pplkg-rack-failure-sim-1468935587   0/2     Completed   0              6m46s   192.168.221.70    master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,389 [INFO]   argo          resilience-scaled-heft-pplkg-rack-failure-sim-1485713206   0/2     Completed   0              3m35s   192.168.221.68    master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,389 [INFO]   argo          workflow-controller-ccbd949dc-ghwvq                        1/1     Running     4 (15m ago)    61m     192.168.191.65    worker-w006   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,389 [INFO]   kube-system   calico-kube-controllers-7498b9bb4c-b49jr                   1/1     Running     0              3m18s   192.168.153.249   worker-w004   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,389 [INFO]   kube-system   calico-node-4zhd4                                          1/1     Running     0              13d     192.168.56.105    worker-w002   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,389 [INFO]   kube-system   calico-node-75nx6                                          1/1     Running     0              13d     192.168.56.109    worker-w006   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,389 [INFO]   kube-system   calico-node-7lkdq                                          1/1     Running     0              13d     192.168.56.104    worker-w001   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,389 [INFO]   kube-system   calico-node-85f8c                                          1/1     Running     0              13d     192.168.56.102    master-m002   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,389 [INFO]   kube-system   calico-node-j8nb9                                          1/1     Running     0              13d     192.168.56.101    master-m001   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,389 [INFO]   kube-system   calico-node-lbcb2                                          1/1     Running     0              13d     192.168.56.106    worker-w003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,389 [INFO]   kube-system   calico-node-wlr5v                                          1/1     Running     0              13d     192.168.56.107    worker-w004   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,390 [INFO]   kube-system   calico-node-xnzjw                                          1/1     Running     0              13d     192.168.56.103    master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,390 [INFO]   kube-system   calico-node-xsltn                                          1/1     Running     0              13d     192.168.56.108    worker-w005   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,390 [INFO]   kube-system   coredns-668d6bf9bc-8dqlk                                   1/1     Running     0              142m    192.168.15.200    worker-w005   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,390 [INFO]   kube-system   coredns-668d6bf9bc-pbq7d                                   1/1     Running     0              3m23s   192.168.227.254   master-m002   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,390 [INFO]   kube-system   etcd-master-m001                                           1/1     Running     0              13d     192.168.56.101    master-m001   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,390 [INFO]   kube-system   kube-apiserver-master-m001                                 1/1     Running     1 (5d4h ago)   13d     192.168.56.101    master-m001   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,390 [INFO]   kube-system   kube-controller-manager-master-m001                        1/1     Running     16 (16m ago)   13d     192.168.56.101    master-m001   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,390 [INFO]   kube-system   kube-proxy-5lzhj                                           1/1     Running     0              13d     192.168.56.102    master-m002   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,390 [INFO]   kube-system   kube-proxy-7jqkv                                           1/1     Running     0              13d     192.168.56.109    worker-w006   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,391 [INFO]   kube-system   kube-proxy-cv5dt                                           1/1     Running     0              13d     192.168.56.108    worker-w005   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,391 [INFO]   kube-system   kube-proxy-fvpmr                                           1/1     Running     0              13d     192.168.56.101    master-m001   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,391 [INFO]   kube-system   kube-proxy-hgs5z                                           1/1     Running     0              13d     192.168.56.106    worker-w003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,391 [INFO]   kube-system   kube-proxy-kmgqr                                           1/1     Running     0              13d     192.168.56.105    worker-w002   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,391 [INFO]   kube-system   kube-proxy-rdbz5                                           1/1     Running     0              13d     192.168.56.107    worker-w004   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,391 [INFO]   kube-system   kube-proxy-w8mnb                                           1/1     Running     0              13d     192.168.56.103    master-m003   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,391 [INFO]   kube-system   kube-proxy-x9jxr                                           1/1     Running     0              13d     192.168.56.104    worker-w001   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,391 [INFO]   kube-system   kube-scheduler-master-m001                                 1/1     Running     15 (15m ago)   13d     192.168.56.101    master-m001   <none>           <none>[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,391 [INFO] [0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: Pod distribution by node:[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,498 [INFO]   Node worker-w005: 4 pods[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,498 [INFO]   Node master-m003: 21 pods[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,498 [INFO]   Node 61m: 1 pods[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,498 [INFO]   Node worker-w004: 3 pods[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,498 [INFO]   Node worker-w002: 2 pods[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,498 [INFO]   Node worker-w006: 2 pods[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,498 [INFO]   Node worker-w001: 2 pods[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,499 [INFO]   Node master-m002: 3 pods[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,499 [INFO]   Node master-m001: 3 pods[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,499 [INFO]   Node worker-w003: 2 pods[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,499 [INFO]   Node 13d: 3 pods[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,499 [INFO] [0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: Filtering for simulation services:[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,608 [INFO] Node master-m001 is Ready[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,608 [INFO] Node master-m002 is Ready[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,608 [INFO] Node master-m003 is Ready[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,608 [INFO] Node worker-w001 is Ready[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,608 [INFO] Node worker-w002 is Ready[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,608 [INFO] Node worker-w003 is Ready[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,609 [INFO] Node worker-w004 is Ready[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,609 [INFO] Node worker-w005 is Ready[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,609 [INFO] Node worker-w006 is Ready[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,612 [WARNING] No pods found for etcd-sim[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,614 [WARNING] No pods found for postgres-sim[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,616 [WARNING] No pods found for redis-sim[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,617 [WARNING] No pods found for nginx-sim[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,619 [WARNING] No pods found for auth-sim[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: 2026-01-08 08:03:25,620 [INFO] Completed full health check[0m
[37mresilience-scaled-heft-pplkg-health-check-260428915: time="2026-01-08T08:03:27.397Z" level=info msg="sub-process exited" argo=true error="<nil>"[0m
[34mresilience-scaled-heft-pplkg-finalize-metrics-1254213234: ============================================[0m
[34mresilience-scaled-heft-pplkg-finalize-metrics-1254213234: SCALED (2x) HEFT SIMULATION COMPLETE[0m
[34mresilience-scaled-heft-pplkg-finalize-metrics-1254213234: ============================================[0m
[34mresilience-scaled-heft-pplkg-finalize-metrics-1254213234: Scale: 2x (Double nodes)[0m
[34mresilience-scaled-heft-pplkg-finalize-metrics-1254213234: Scheduler: HEFT[0m
[34mresilience-scaled-heft-pplkg-finalize-metrics-1254213234:   - 6 Health Checks[0m
[34mresilience-scaled-heft-pplkg-finalize-metrics-1254213234:   - 2 Node Simulations (HEFT-aware)[0m
[34mresilience-scaled-heft-pplkg-finalize-metrics-1254213234:   - 2 Interim Health Checks[0m
[34mresilience-scaled-heft-pplkg-finalize-metrics-1254213234:   - 2 Rack Simulations (HEFT-aware)[0m
[34mresilience-scaled-heft-pplkg-finalize-metrics-1254213234:   - 2 Final Health Checks[0m
[34mresilience-scaled-heft-pplkg-finalize-metrics-1254213234: time="2026-01-08T08:03:50.725Z" level=info msg="sub-process exited" argo=true error="<nil>"[0m
