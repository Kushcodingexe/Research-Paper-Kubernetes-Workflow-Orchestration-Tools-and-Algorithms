================================================================
 HEFT-OPTIMIZED NATIVE KUBERNETES RESILIENCE SIMULATION
 Time: Mon Jan  5 14:45:12 UTC 2026
================================================================
[2026-01-05 14:45:12] Run ID: heft-native-20260105-144512
[2026-01-05 14:45:12] Platform: Native Kubernetes (HEFT-Optimized)
[2026-01-05 14:45:12] HEFT Exclusion Node: worker-w005
[2026-01-05 14:45:12] HEFT Exclusion Zone: R3

================================================================
 STEP 1: PARALLEL HEALTH CHECKS (3x)
 Time: Mon Jan  5 14:45:12 UTC 2026
================================================================
Log directory created/verified: /app/logs
File logging configured successfully
2026-01-05 14:45:12,658 [INFO] Loaded in-cluster Kubernetes config
2026-01-05 14:45:12,659 [INFO] Running on host: resilience-heft-1-201507-qflcd
2026-01-05 14:45:12,659 [INFO] Detected current node: master-m003, zone: R3
2026-01-05 14:45:12,659 [INFO] Checking if we have permissions to modify nodes...
Log directory created/verified: /app/logs
File logging configured successfully
2026-01-05 14:45:12,674 [INFO] Loaded in-cluster Kubernetes config
2026-01-05 14:45:12,674 [INFO] Running on host: resilience-heft-1-201507-qflcd
2026-01-05 14:45:12,675 [INFO] Detected current node: master-m003, zone: R3
2026-01-05 14:45:12,675 [INFO] Checking if we have permissions to modify nodes...
2026-01-05 14:45:12,721 [INFO] Testing permissions using node: master-m001
2026-01-05 14:45:12,730 [INFO] Permission check successful - we can modify nodes
2026-01-05 14:45:12,730 [INFO] Using real Kubernetes API for node control
2026-01-05 14:45:12,731 [INFO] Testing permissions using node: master-m001
2026-01-05 14:45:12,731 [INFO] Action received: health-check
2026-01-05 14:45:12,731 [INFO] Stabilization time: 10 seconds
2026-01-05 14:45:12,731 [INFO] Starting full health check
2026-01-05 14:45:12,731 [INFO] 
============ DETAILED NODE STATUS ============
2026-01-05 14:45:12,732 [INFO] Basic Node Information (kubectl get nodes -o wide):
2026-01-05 14:45:12,737 [INFO] Permission check successful - we can modify nodes
2026-01-05 14:45:12,737 [INFO] Using real Kubernetes API for node control
2026-01-05 14:45:12,738 [INFO] Action received: health-check
2026-01-05 14:45:12,738 [INFO] Stabilization time: 10 seconds
2026-01-05 14:45:12,738 [INFO] Starting full health check
2026-01-05 14:45:12,738 [INFO] 
============ DETAILED NODE STATUS ============
2026-01-05 14:45:12,738 [INFO] Basic Node Information (kubectl get nodes -o wide):
Log directory created/verified: /app/logs
File logging configured successfully
2026-01-05 14:45:12,749 [INFO] Loaded in-cluster Kubernetes config
2026-01-05 14:45:12,749 [INFO] Running on host: resilience-heft-1-201507-qflcd
2026-01-05 14:45:12,749 [INFO] Detected current node: master-m003, zone: R3
2026-01-05 14:45:12,750 [INFO] Checking if we have permissions to modify nodes...
2026-01-05 14:45:12,785 [INFO] Testing permissions using node: master-m001
2026-01-05 14:45:12,793 [INFO] Permission check successful - we can modify nodes
2026-01-05 14:45:12,793 [INFO] Using real Kubernetes API for node control
2026-01-05 14:45:12,794 [INFO] Action received: health-check
2026-01-05 14:45:12,794 [INFO] Stabilization time: 10 seconds
2026-01-05 14:45:12,794 [INFO] Starting full health check
2026-01-05 14:45:12,794 [INFO] 
============ DETAILED NODE STATUS ============
2026-01-05 14:45:12,795 [INFO] Basic Node Information (kubectl get nodes -o wide):
2026-01-05 14:45:14,207 [INFO]   NAME          STATUS   ROLES           AGE   VERSION    INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
2026-01-05 14:45:14,208 [INFO]   master-m001   Ready    control-plane   10d   v1.32.11   192.168.56.101   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:45:14,208 [INFO]   master-m002   Ready    control-plane   10d   v1.32.11   192.168.56.102   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:45:14,208 [INFO]   master-m003   Ready    control-plane   10d   v1.32.11   192.168.56.103   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:45:14,208 [INFO]   worker-w001   Ready    <none>          10d   v1.32.11   192.168.56.104   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:45:14,208 [INFO]   worker-w002   Ready    <none>          10d   v1.32.11   192.168.56.105   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:45:14,208 [INFO]   worker-w003   Ready    <none>          10d   v1.32.11   192.168.56.106   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:45:14,208 [INFO]   worker-w004   Ready    <none>          10d   v1.32.11   192.168.56.107   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:45:14,208 [INFO]   worker-w005   Ready    <none>          10d   v1.32.11   192.168.56.108   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:45:14,209 [INFO]   worker-w006   Ready    <none>          10d   v1.32.11   192.168.56.109   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:45:14,209 [INFO] 
Enhanced Node Status (with taint and cordon indicators):
2026-01-05 14:45:14,209 [INFO]   NAME                STATUS    ROLES           ZONE   CORDONED   TAINTS
2026-01-05 14:45:14,210 [INFO]   NAME          STATUS   ROLES           AGE   VERSION    INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
2026-01-05 14:45:14,210 [INFO]   master-m001   Ready    control-plane   10d   v1.32.11   192.168.56.101   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:45:14,210 [INFO]   master-m002   Ready    control-plane   10d   v1.32.11   192.168.56.102   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:45:14,210 [INFO]   master-m003   Ready    control-plane   10d   v1.32.11   192.168.56.103   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:45:14,210 [INFO]   worker-w001   Ready    <none>          10d   v1.32.11   192.168.56.104   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:45:14,211 [INFO]   worker-w002   Ready    <none>          10d   v1.32.11   192.168.56.105   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:45:14,211 [INFO]   worker-w003   Ready    <none>          10d   v1.32.11   192.168.56.106   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:45:14,211 [INFO]   worker-w004   Ready    <none>          10d   v1.32.11   192.168.56.107   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:45:14,211 [INFO]   worker-w005   Ready    <none>          10d   v1.32.11   192.168.56.108   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:45:14,211 [INFO]   worker-w006   Ready    <none>          10d   v1.32.11   192.168.56.109   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:45:14,211 [INFO] 
Enhanced Node Status (with taint and cordon indicators):
2026-01-05 14:45:14,211 [INFO]   NAME                STATUS    ROLES           ZONE   CORDONED   TAINTS
2026-01-05 14:45:14,211 [INFO]   NAME          STATUS   ROLES           AGE   VERSION    INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
2026-01-05 14:45:14,211 [INFO]   master-m001   Ready    control-plane   10d   v1.32.11   192.168.56.101   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:45:14,211 [INFO]   master-m002   Ready    control-plane   10d   v1.32.11   192.168.56.102   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:45:14,211 [INFO]   master-m003   Ready    control-plane   10d   v1.32.11   192.168.56.103   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:45:14,211 [INFO]   worker-w001   Ready    <none>          10d   v1.32.11   192.168.56.104   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:45:14,212 [INFO]   worker-w002   Ready    <none>          10d   v1.32.11   192.168.56.105   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:45:14,212 [INFO]   worker-w003   Ready    <none>          10d   v1.32.11   192.168.56.106   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:45:14,212 [INFO]   worker-w004   Ready    <none>          10d   v1.32.11   192.168.56.107   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:45:14,212 [INFO]   worker-w005   Ready    <none>          10d   v1.32.11   192.168.56.108   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:45:14,212 [INFO]   worker-w006   Ready    <none>          10d   v1.32.11   192.168.56.109   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:45:14,212 [INFO] 
Enhanced Node Status (with taint and cordon indicators):
2026-01-05 14:45:14,212 [INFO]   NAME                STATUS    ROLES           ZONE   CORDONED   TAINTS
2026-01-05 14:45:14,728 [INFO]   master-m001     Ready  ✓ worker         R1    No       node-role.kubernetes.io/control-plane 
2026-01-05 14:45:14,775 [INFO]   master-m001     Ready  ✓ worker         R1    No       node-role.kubernetes.io/control-plane 
2026-01-05 14:45:14,886 [INFO]   master-m001     Ready  ✓ worker         R1    No       node-role.kubernetes.io/control-plane 
2026-01-05 14:45:15,055 [INFO]   master-m002     Ready  ✓ worker         R2    No       None 
2026-01-05 14:45:15,064 [INFO]   master-m002     Ready  ✓ worker         R2    No       None 
2026-01-05 14:45:15,206 [INFO]   master-m002     Ready  ✓ worker         R2    No       None 
2026-01-05 14:45:15,344 [INFO]   master-m003     Ready  ✓ worker         R3    No       None 
2026-01-05 14:45:15,405 [INFO]   master-m003     Ready  ✓ worker         R3    No       None 
2026-01-05 14:45:15,459 [INFO]   master-m003     Ready  ✓ worker         R3    No       None 
2026-01-05 14:45:15,583 [INFO]   worker-w001     Ready  ✓ worker         R1    No       None 
2026-01-05 14:45:15,618 [INFO]   worker-w001     Ready  ✓ worker         R1    No       None 
2026-01-05 14:45:15,788 [INFO]   worker-w002     Ready  ✓ worker         R1    No       None 
2026-01-05 14:45:15,812 [INFO]   worker-w001     Ready  ✓ worker         R1    No       None 
2026-01-05 14:45:15,903 [INFO]   worker-w002     Ready  ✓ worker         R1    No       None 
2026-01-05 14:45:16,004 [INFO]   worker-w002     Ready  ✓ worker         R1    No       None 
2026-01-05 14:45:16,047 [INFO]   worker-w003     Ready  ✓ worker         R2    No       None 
2026-01-05 14:45:16,162 [INFO]   worker-w003     Ready  ✓ worker         R2    No       None 
2026-01-05 14:45:16,295 [INFO]   worker-w003     Ready  ✓ worker         R2    No       None 
2026-01-05 14:45:16,311 [INFO]   worker-w004     Ready  ✓ worker         R2    No       None 
2026-01-05 14:45:16,421 [INFO]   worker-w004     Ready  ✓ worker         R2    No       None 
2026-01-05 14:45:16,618 [INFO]   worker-w004     Ready  ✓ worker         R2    No       None 
2026-01-05 14:45:16,711 [INFO]   worker-w005     Ready  ✓ worker         R3    No       None 
2026-01-05 14:45:16,745 [INFO]   worker-w005     Ready  ✓ worker         R3    No       None 
2026-01-05 14:45:16,911 [INFO]   worker-w005     Ready  ✓ worker         R3    No       None 
2026-01-05 14:45:16,989 [INFO]   worker-w006     Ready  ✓ worker         R3    No       None 
2026-01-05 14:45:16,989 [INFO] 
Legend:
2026-01-05 14:45:16,989 [INFO]   ✓ = Node is Ready
2026-01-05 14:45:16,989 [INFO]   ⚠️ = Warning indicator (NotReady, Cordoned, or has simulated-failure taint)
2026-01-05 14:45:16,989 [INFO] ============ DETAILED POD INFORMATION ============
2026-01-05 14:45:16,989 [INFO] Running 'kubectl get pods -o wide' to show detailed pod placement:
2026-01-05 14:45:17,047 [INFO]   worker-w006     Ready  ✓ worker         R3    No       None 
2026-01-05 14:45:17,047 [INFO] 
Legend:
2026-01-05 14:45:17,047 [INFO]   ✓ = Node is Ready
2026-01-05 14:45:17,048 [INFO]   ⚠️ = Warning indicator (NotReady, Cordoned, or has simulated-failure taint)
2026-01-05 14:45:17,048 [INFO] ============ DETAILED POD INFORMATION ============
2026-01-05 14:45:17,048 [INFO] Running 'kubectl get pods -o wide' to show detailed pod placement:
2026-01-05 14:45:17,137 [INFO]   NAMESPACE     NAME                                                    READY   STATUS      RESTARTS        AGE     IP                NODE          NOMINATED NODE   READINESS GATES
2026-01-05 14:45:17,137 [INFO]   argo          argo-server-5c69cb69db-gdkl6                            1/1     Running     0               8d      192.168.221.65    master-m003   <none>           <none>
2026-01-05 14:45:17,137 [INFO]   argo          resilience-bench-ptngm-initialize-metrics-940927476     0/2     Completed   0               2d16h   192.168.221.125   master-m003   <none>           <none>
2026-01-05 14:45:17,137 [INFO]   argo          resilience-bench-ptngm-run-health-check-1168793163      0/2     Completed   0               2d16h   192.168.221.67    master-m003   <none>           <none>
2026-01-05 14:45:17,137 [INFO]   argo          resilience-bench-ptngm-run-health-check-1185570782      0/2     Completed   0               2d16h   192.168.221.66    master-m003   <none>           <none>
2026-01-05 14:45:17,137 [INFO]   argo          resilience-bench-ptngm-run-health-check-1202348401      0/2     Completed   0               2d16h   192.168.221.122   master-m003   <none>           <none>
2026-01-05 14:45:17,137 [INFO]   argo          resilience-bench-wf87f-initialize-metrics-3677018408    0/2     Completed   0               2d18h   192.168.221.100   master-m003   <none>           <none>
2026-01-05 14:45:17,137 [INFO]   argo          resilience-bench-wf87f-run-health-check-2914150102      0/2     Completed   0               2d18h   192.168.221.105   master-m003   <none>           <none>
2026-01-05 14:45:17,137 [INFO]   argo          resilience-bench-wf87f-run-health-check-3931060015      0/2     Completed   0               2d18h   192.168.221.104   master-m003   <none>           <none>
2026-01-05 14:45:17,137 [INFO]   argo          resilience-bench-wf87f-run-health-check-3947837634      0/2     Completed   0               2d18h   192.168.221.98    master-m003   <none>           <none>
2026-01-05 14:45:17,137 [INFO]   argo          resilience-bench-wf87f-run-health-check-3964615253      0/2     Completed   0               2d18h   192.168.221.103   master-m003   <none>           <none>
2026-01-05 14:45:17,137 [INFO]   argo          resilience-bench-wf87f-run-node-simulation-4173146970   0/2     Completed   0               2d18h   192.168.221.101   master-m003   <none>           <none>
2026-01-05 14:45:17,137 [INFO]   argo          resilience-bench-wf87f-run-rack-simulation-1356070019   0/2     Completed   0               2d18h   192.168.221.109   master-m003   <none>           <none>
2026-01-05 14:45:17,137 [INFO]   argo          resilience-heft-4cqbx-health-check-1555843180           0/2     Error       0               81m     192.168.132.166   worker-w001   <none>           <none>
2026-01-05 14:45:17,137 [INFO]   argo          resilience-heft-4cqbx-health-check-1589398418           0/2     Error       0               81m     192.168.15.231    worker-w005   <none>           <none>
2026-01-05 14:45:17,138 [INFO]   argo          resilience-heft-4cqbx-health-check-1606176037           0/2     Error       0               81m     192.168.240.157   worker-w003   <none>           <none>
2026-01-05 14:45:17,138 [INFO]   argo          resilience-heft-4cqbx-heft-initialize-2594215169        0/2     Completed   0               82m     192.168.221.79    master-m003   <none>           <none>
2026-01-05 14:45:17,138 [INFO]   argo          resilience-heft-76kbp-health-check-270747540            0/2     Error       0               73m     192.168.15.237    worker-w005   <none>           <none>
2026-01-05 14:45:17,138 [INFO]   argo          resilience-heft-76kbp-health-check-287525159            0/2     Error       0               73m     192.168.240.163   worker-w003   <none>           <none>
2026-01-05 14:45:17,138 [INFO]   argo          resilience-heft-76kbp-health-check-304302778            0/2     Error       0               73m     192.168.132.172   worker-w001   <none>           <none>
2026-01-05 14:45:17,138 [INFO]   argo          resilience-heft-76kbp-heft-initialize-3574960751        0/2     Completed   0               73m     192.168.221.86    master-m003   <none>           <none>
2026-01-05 14:45:17,138 [INFO]   argo          resilience-heft-b85xr-health-check-2807215411           0/2     Error       0               69m     192.168.132.175   worker-w001   <none>           <none>
2026-01-05 14:45:17,138 [INFO]   argo          resilience-heft-b85xr-health-check-2823993030           0/2     Error       0               69m     192.168.240.166   worker-w003   <none>           <none>
2026-01-05 14:45:17,138 [INFO]   argo          resilience-heft-b85xr-health-check-2840770649           0/2     Error       0               69m     192.168.15.240    worker-w005   <none>           <none>
2026-01-05 14:45:17,138 [INFO]   argo          resilience-heft-b85xr-heft-initialize-1754406316        0/2     Completed   0               70m     192.168.221.88    master-m003   <none>           <none>
2026-01-05 14:45:17,138 [INFO]   argo          resilience-heft-fdw9f-health-check-1896357264           0/2     Error       0               75m     192.168.15.235    worker-w005   <none>           <none>
2026-01-05 14:45:17,138 [INFO]   argo          resilience-heft-fdw9f-health-check-1913134883           0/2     Error       0               75m     192.168.240.161   worker-w003   <none>           <none>
2026-01-05 14:45:17,138 [INFO]   argo          resilience-heft-fdw9f-health-check-1929912502           0/2     Error       0               75m     192.168.132.170   worker-w001   <none>           <none>
2026-01-05 14:45:17,138 [INFO]   argo          resilience-heft-fdw9f-heft-initialize-2651619139        0/2     Completed   0               75m     192.168.221.77    master-m003   <none>           <none>
2026-01-05 14:45:17,138 [INFO]   argo          resilience-heft-gf256-health-check-946619120            0/2     Error       0               74m     192.168.132.171   worker-w001   <none>           <none>
2026-01-05 14:45:17,138 [INFO]   argo          resilience-heft-gf256-health-check-980174358            0/2     Error       0               74m     192.168.15.236    worker-w005   <none>           <none>
2026-01-05 14:45:17,138 [INFO]   argo          resilience-heft-gf256-health-check-996951977            0/2     Error       0               74m     192.168.240.162   worker-w003   <none>           <none>
2026-01-05 14:45:17,138 [INFO]   argo          resilience-heft-gf256-heft-initialize-1548293021        0/2     Completed   0               74m     192.168.221.84    master-m003   <none>           <none>
2026-01-05 14:45:17,138 [INFO]   argo          resilience-heft-gxwpg-health-check-3428526804           0/2     Error       0               77m     192.168.240.159   worker-w003   <none>           <none>
2026-01-05 14:45:17,138 [INFO]   argo          resilience-heft-gxwpg-health-check-3445304423           0/2     Error       0               77m     192.168.15.233    worker-w005   <none>           <none>
2026-01-05 14:45:17,138 [INFO]   argo          resilience-heft-gxwpg-health-check-3478859661           0/2     Error       0               77m     192.168.132.168   worker-w001   <none>           <none>
2026-01-05 14:45:17,138 [INFO]   argo          resilience-heft-gxwpg-heft-initialize-2377397022        0/2     Completed   0               77m     192.168.221.81    master-m003   <none>           <none>
2026-01-05 14:45:17,138 [INFO]   argo          resilience-heft-pc2kb-health-check-3258812212           0/2     Error       0               71m     192.168.15.238    worker-w005   <none>           <none>
2026-01-05 14:45:17,138 [INFO]   argo          resilience-heft-pc2kb-health-check-3275589831           0/2     Error       0               71m     192.168.240.164   worker-w003   <none>           <none>
2026-01-05 14:45:17,138 [INFO]   argo          resilience-heft-pc2kb-health-check-3292367450           0/2     Error       0               71m     192.168.132.173   worker-w001   <none>           <none>
2026-01-05 14:45:17,138 [INFO]   argo          resilience-heft-pc2kb-heft-initialize-321333135         0/2     Completed   0               72m     192.168.221.83    master-m003   <none>           <none>
2026-01-05 14:45:17,138 [INFO]   argo          resilience-heft-qsmch-health-check-283908068            0/2     Error       0               78m     192.168.15.232    worker-w005   <none>           <none>
2026-01-05 14:45:17,138 [INFO]   argo          resilience-heft-qsmch-health-check-300685687            0/2     Error       0               78m     192.168.240.158   worker-w003   <none>           <none>
2026-01-05 14:45:17,138 [INFO]   argo          resilience-heft-qsmch-health-check-317463306            0/2     Error       0               78m     192.168.132.167   worker-w001   <none>           <none>
2026-01-05 14:45:17,138 [INFO]   argo          resilience-heft-qsmch-heft-initialize-1908810015        0/2     Completed   0               79m     192.168.221.82    master-m003   <none>           <none>
2026-01-05 14:45:17,138 [INFO]   argo          resilience-heft-r2w8l-health-check-350149736            0/2     Error       0               71m     192.168.240.165   worker-w003   <none>           <none>
2026-01-05 14:45:17,138 [INFO]   argo          resilience-heft-r2w8l-health-check-366927355            0/2     Error       0               71m     192.168.15.239    worker-w005   <none>           <none>
2026-01-05 14:45:17,138 [INFO]   argo          resilience-heft-r2w8l-health-check-400482593            0/2     Error       0               71m     192.168.132.174   worker-w001   <none>           <none>
2026-01-05 14:45:17,139 [INFO]   argo          resilience-heft-r2w8l-heft-initialize-3054048250        0/2     Completed   0               71m     192.168.221.85    master-m003   <none>           <none>
2026-01-05 14:45:17,139 [INFO]   argo          resilience-heft-t988k-health-check-612195296            0/2     Error       0               76m     192.168.15.234    worker-w005   <none>           <none>
2026-01-05 14:45:17,139 [INFO]   argo          resilience-heft-t988k-health-check-628972915            0/2     Error       0               76m     192.168.240.160   worker-w003   <none>           <none>
2026-01-05 14:45:17,139 [INFO]   argo          resilience-heft-t988k-health-check-645750534            0/2     Error       0               76m     192.168.132.169   worker-w001   <none>           <none>
2026-01-05 14:45:17,139 [INFO]   argo          resilience-heft-t988k-heft-initialize-164752883         0/2     Completed   0               76m     192.168.221.80    master-m003   <none>           <none>
2026-01-05 14:45:17,139 [INFO]   argo          resilience-heft-vlt2m-health-check-2709447919           0/2     Error       0               32m     192.168.221.119   master-m003   <none>           <none>
2026-01-05 14:45:17,139 [INFO]   argo          resilience-heft-vlt2m-health-check-2726225538           0/2     Error       0               32m     192.168.221.120   master-m003   <none>           <none>
2026-01-05 14:45:17,139 [INFO]   argo          resilience-heft-vlt2m-health-check-2743003157           0/2     Error       0               32m     192.168.221.121   master-m003   <none>           <none>
2026-01-05 14:45:17,139 [INFO]   argo          resilience-heft-vlt2m-heft-initialize-840404112         0/2     Completed   0               33m     192.168.221.118   master-m003   <none>           <none>
2026-01-05 14:45:17,139 [INFO]   argo          resilience-heft-x86wd-health-check-2184182048           0/2     Error       0               31m     192.168.221.123   master-m003   <none>           <none>
2026-01-05 14:45:17,139 [INFO]   argo          resilience-heft-x86wd-health-check-2200959667           0/2     Error       0               31m     192.168.221.125   master-m003   <none>           <none>
2026-01-05 14:45:17,139 [INFO]   argo          resilience-heft-x86wd-health-check-2234514905           0/2     Error       0               31m     192.168.221.124   master-m003   <none>           <none>
2026-01-05 14:45:17,139 [INFO]   argo          resilience-heft-x86wd-heft-initialize-2152040770        0/2     Completed   0               31m     192.168.221.127   master-m003   <none>           <none>
2026-01-05 14:45:17,139 [INFO]   argo          workflow-controller-ccbd949dc-pxkms                     1/1     Running     3 (53m ago)     39h     192.168.195.242   worker-w002   <none>           <none>
2026-01-05 14:45:17,139 [INFO]   default       resilience-heft-1-201507-qflcd                          1/1     Running     0               9s      192.168.221.71    master-m003   <none>           <none>
2026-01-05 14:45:17,139 [INFO]   kube-system   calico-kube-controllers-7498b9bb4c-xd7tb                1/1     Running     0               38h     192.168.191.108   worker-w006   <none>           <none>
2026-01-05 14:45:17,139 [INFO]   kube-system   calico-node-4zhd4                                       1/1     Running     0               10d     192.168.56.105    worker-w002   <none>           <none>
2026-01-05 14:45:17,139 [INFO]   kube-system   calico-node-75nx6                                       1/1     Running     0               10d     192.168.56.109    worker-w006   <none>           <none>
2026-01-05 14:45:17,139 [INFO]   kube-system   calico-node-7lkdq                                       1/1     Running     0               10d     192.168.56.104    worker-w001   <none>           <none>
2026-01-05 14:45:17,139 [INFO]   kube-system   calico-node-85f8c                                       1/1     Running     0               10d     192.168.56.102    master-m002   <none>           <none>
2026-01-05 14:45:17,139 [INFO]   kube-system   calico-node-j8nb9                                       1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:45:17,139 [INFO]   kube-system   calico-node-lbcb2                                       1/1     Running     0               10d     192.168.56.106    worker-w003   <none>           <none>
2026-01-05 14:45:17,139 [INFO]   kube-system   calico-node-wlr5v                                       1/1     Running     0               10d     192.168.56.107    worker-w004   <none>           <none>
2026-01-05 14:45:17,139 [INFO]   kube-system   calico-node-xnzjw                                       1/1     Running     0               10d     192.168.56.103    master-m003   <none>           <none>
2026-01-05 14:45:17,139 [INFO]   kube-system   calico-node-xsltn                                       1/1     Running     0               10d     192.168.56.108    worker-w005   <none>           <none>
2026-01-05 14:45:17,139 [INFO]   kube-system   coredns-668d6bf9bc-2f74f                                1/1     Running     0               39h     192.168.15.230    worker-w005   <none>           <none>
2026-01-05 14:45:17,139 [INFO]   kube-system   coredns-668d6bf9bc-ssl9h                                1/1     Running     0               39h     192.168.132.165   worker-w001   <none>           <none>
2026-01-05 14:45:17,139 [INFO]   kube-system   etcd-master-m001                                        1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:45:17,140 [INFO]   kube-system   kube-apiserver-master-m001                              1/1     Running     1 (2d11h ago)   10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:45:17,140 [INFO]   kube-system   kube-controller-manager-master-m001                     1/1     Running     12 (29m ago)    10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:45:17,140 [INFO]   kube-system   kube-proxy-5lzhj                                        1/1     Running     0               10d     192.168.56.102    master-m002   <none>           <none>
2026-01-05 14:45:17,140 [INFO]   kube-system   kube-proxy-7jqkv                                        1/1     Running     0               10d     192.168.56.109    worker-w006   <none>           <none>
2026-01-05 14:45:17,140 [INFO]   kube-system   kube-proxy-cv5dt                                        1/1     Running     0               10d     192.168.56.108    worker-w005   <none>           <none>
2026-01-05 14:45:17,140 [INFO]   kube-system   kube-proxy-fvpmr                                        1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:45:17,140 [INFO]   kube-system   kube-proxy-hgs5z                                        1/1     Running     0               10d     192.168.56.106    worker-w003   <none>           <none>
2026-01-05 14:45:17,140 [INFO]   kube-system   kube-proxy-kmgqr                                        1/1     Running     0               10d     192.168.56.105    worker-w002   <none>           <none>
2026-01-05 14:45:17,140 [INFO]   kube-system   kube-proxy-rdbz5                                        1/1     Running     0               10d     192.168.56.107    worker-w004   <none>           <none>
2026-01-05 14:45:17,140 [INFO]   kube-system   kube-proxy-w8mnb                                        1/1     Running     0               10d     192.168.56.103    master-m003   <none>           <none>
2026-01-05 14:45:17,140 [INFO]   kube-system   kube-proxy-x9jxr                                        1/1     Running     0               10d     192.168.56.104    worker-w001   <none>           <none>
2026-01-05 14:45:17,140 [INFO]   kube-system   kube-scheduler-master-m001                              1/1     Running     11 (29m ago)    10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:45:17,140 [INFO] 
Pod distribution by node:
2026-01-05 14:45:17,165 [INFO]   worker-w006     Ready  ✓ worker         R3    No       None 
2026-01-05 14:45:17,165 [INFO] 
Legend:
2026-01-05 14:45:17,166 [INFO]   ✓ = Node is Ready
2026-01-05 14:45:17,166 [INFO]   ⚠️ = Warning indicator (NotReady, Cordoned, or has simulated-failure taint)
2026-01-05 14:45:17,166 [INFO] ============ DETAILED POD INFORMATION ============
2026-01-05 14:45:17,166 [INFO] Running 'kubectl get pods -o wide' to show detailed pod placement:
2026-01-05 14:45:17,219 [INFO]   NAMESPACE     NAME                                                    READY   STATUS      RESTARTS        AGE     IP                NODE          NOMINATED NODE   READINESS GATES
2026-01-05 14:45:17,220 [INFO]   argo          argo-server-5c69cb69db-gdkl6                            1/1     Running     0               8d      192.168.221.65    master-m003   <none>           <none>
2026-01-05 14:45:17,220 [INFO]   argo          resilience-bench-ptngm-initialize-metrics-940927476     0/2     Completed   0               2d16h   192.168.221.125   master-m003   <none>           <none>
2026-01-05 14:45:17,220 [INFO]   argo          resilience-bench-ptngm-run-health-check-1168793163      0/2     Completed   0               2d16h   192.168.221.67    master-m003   <none>           <none>
2026-01-05 14:45:17,220 [INFO]   argo          resilience-bench-ptngm-run-health-check-1185570782      0/2     Completed   0               2d16h   192.168.221.66    master-m003   <none>           <none>
2026-01-05 14:45:17,220 [INFO]   argo          resilience-bench-ptngm-run-health-check-1202348401      0/2     Completed   0               2d16h   192.168.221.122   master-m003   <none>           <none>
2026-01-05 14:45:17,220 [INFO]   argo          resilience-bench-wf87f-initialize-metrics-3677018408    0/2     Completed   0               2d18h   192.168.221.100   master-m003   <none>           <none>
2026-01-05 14:45:17,220 [INFO]   argo          resilience-bench-wf87f-run-health-check-2914150102      0/2     Completed   0               2d18h   192.168.221.105   master-m003   <none>           <none>
2026-01-05 14:45:17,220 [INFO]   argo          resilience-bench-wf87f-run-health-check-3931060015      0/2     Completed   0               2d18h   192.168.221.104   master-m003   <none>           <none>
2026-01-05 14:45:17,220 [INFO]   argo          resilience-bench-wf87f-run-health-check-3947837634      0/2     Completed   0               2d18h   192.168.221.98    master-m003   <none>           <none>
2026-01-05 14:45:17,220 [INFO]   argo          resilience-bench-wf87f-run-health-check-3964615253      0/2     Completed   0               2d18h   192.168.221.103   master-m003   <none>           <none>
2026-01-05 14:45:17,220 [INFO]   argo          resilience-bench-wf87f-run-node-simulation-4173146970   0/2     Completed   0               2d18h   192.168.221.101   master-m003   <none>           <none>
2026-01-05 14:45:17,220 [INFO]   argo          resilience-bench-wf87f-run-rack-simulation-1356070019   0/2     Completed   0               2d18h   192.168.221.109   master-m003   <none>           <none>
2026-01-05 14:45:17,221 [INFO]   argo          resilience-heft-4cqbx-health-check-1555843180           0/2     Error       0               81m     192.168.132.166   worker-w001   <none>           <none>
2026-01-05 14:45:17,221 [INFO]   argo          resilience-heft-4cqbx-health-check-1589398418           0/2     Error       0               81m     192.168.15.231    worker-w005   <none>           <none>
2026-01-05 14:45:17,221 [INFO]   argo          resilience-heft-4cqbx-health-check-1606176037           0/2     Error       0               81m     192.168.240.157   worker-w003   <none>           <none>
2026-01-05 14:45:17,221 [INFO]   argo          resilience-heft-4cqbx-heft-initialize-2594215169        0/2     Completed   0               82m     192.168.221.79    master-m003   <none>           <none>
2026-01-05 14:45:17,221 [INFO]   argo          resilience-heft-76kbp-health-check-270747540            0/2     Error       0               73m     192.168.15.237    worker-w005   <none>           <none>
2026-01-05 14:45:17,221 [INFO]   argo          resilience-heft-76kbp-health-check-287525159            0/2     Error       0               73m     192.168.240.163   worker-w003   <none>           <none>
2026-01-05 14:45:17,222 [INFO]   argo          resilience-heft-76kbp-health-check-304302778            0/2     Error       0               73m     192.168.132.172   worker-w001   <none>           <none>
2026-01-05 14:45:17,222 [INFO]   argo          resilience-heft-76kbp-heft-initialize-3574960751        0/2     Completed   0               73m     192.168.221.86    master-m003   <none>           <none>
2026-01-05 14:45:17,222 [INFO]   argo          resilience-heft-b85xr-health-check-2807215411           0/2     Error       0               69m     192.168.132.175   worker-w001   <none>           <none>
2026-01-05 14:45:17,222 [INFO]   argo          resilience-heft-b85xr-health-check-2823993030           0/2     Error       0               69m     192.168.240.166   worker-w003   <none>           <none>
2026-01-05 14:45:17,222 [INFO]   argo          resilience-heft-b85xr-health-check-2840770649           0/2     Error       0               69m     192.168.15.240    worker-w005   <none>           <none>
2026-01-05 14:45:17,222 [INFO]   argo          resilience-heft-b85xr-heft-initialize-1754406316        0/2     Completed   0               70m     192.168.221.88    master-m003   <none>           <none>
2026-01-05 14:45:17,222 [INFO]   argo          resilience-heft-fdw9f-health-check-1896357264           0/2     Error       0               75m     192.168.15.235    worker-w005   <none>           <none>
2026-01-05 14:45:17,222 [INFO]   argo          resilience-heft-fdw9f-health-check-1913134883           0/2     Error       0               75m     192.168.240.161   worker-w003   <none>           <none>
2026-01-05 14:45:17,222 [INFO]   argo          resilience-heft-fdw9f-health-check-1929912502           0/2     Error       0               75m     192.168.132.170   worker-w001   <none>           <none>
2026-01-05 14:45:17,222 [INFO]   argo          resilience-heft-fdw9f-heft-initialize-2651619139        0/2     Completed   0               75m     192.168.221.77    master-m003   <none>           <none>
2026-01-05 14:45:17,222 [INFO]   argo          resilience-heft-gf256-health-check-946619120            0/2     Error       0               74m     192.168.132.171   worker-w001   <none>           <none>
2026-01-05 14:45:17,222 [INFO]   argo          resilience-heft-gf256-health-check-980174358            0/2     Error       0               74m     192.168.15.236    worker-w005   <none>           <none>
2026-01-05 14:45:17,222 [INFO]   argo          resilience-heft-gf256-health-check-996951977            0/2     Error       0               74m     192.168.240.162   worker-w003   <none>           <none>
2026-01-05 14:45:17,222 [INFO]   argo          resilience-heft-gf256-heft-initialize-1548293021        0/2     Completed   0               74m     192.168.221.84    master-m003   <none>           <none>
2026-01-05 14:45:17,222 [INFO]   argo          resilience-heft-gxwpg-health-check-3428526804           0/2     Error       0               77m     192.168.240.159   worker-w003   <none>           <none>
2026-01-05 14:45:17,222 [INFO]   argo          resilience-heft-gxwpg-health-check-3445304423           0/2     Error       0               77m     192.168.15.233    worker-w005   <none>           <none>
2026-01-05 14:45:17,222 [INFO]   argo          resilience-heft-gxwpg-health-check-3478859661           0/2     Error       0               77m     192.168.132.168   worker-w001   <none>           <none>
2026-01-05 14:45:17,222 [INFO]   argo          resilience-heft-gxwpg-heft-initialize-2377397022        0/2     Completed   0               77m     192.168.221.81    master-m003   <none>           <none>
2026-01-05 14:45:17,222 [INFO]   argo          resilience-heft-pc2kb-health-check-3258812212           0/2     Error       0               71m     192.168.15.238    worker-w005   <none>           <none>
2026-01-05 14:45:17,222 [INFO]   argo          resilience-heft-pc2kb-health-check-3275589831           0/2     Error       0               71m     192.168.240.164   worker-w003   <none>           <none>
2026-01-05 14:45:17,222 [INFO]   argo          resilience-heft-pc2kb-health-check-3292367450           0/2     Error       0               71m     192.168.132.173   worker-w001   <none>           <none>
2026-01-05 14:45:17,223 [INFO]   argo          resilience-heft-pc2kb-heft-initialize-321333135         0/2     Completed   0               72m     192.168.221.83    master-m003   <none>           <none>
2026-01-05 14:45:17,223 [INFO]   argo          resilience-heft-qsmch-health-check-283908068            0/2     Error       0               78m     192.168.15.232    worker-w005   <none>           <none>
2026-01-05 14:45:17,223 [INFO]   argo          resilience-heft-qsmch-health-check-300685687            0/2     Error       0               78m     192.168.240.158   worker-w003   <none>           <none>
2026-01-05 14:45:17,223 [INFO]   argo          resilience-heft-qsmch-health-check-317463306            0/2     Error       0               78m     192.168.132.167   worker-w001   <none>           <none>
2026-01-05 14:45:17,223 [INFO]   argo          resilience-heft-qsmch-heft-initialize-1908810015        0/2     Completed   0               79m     192.168.221.82    master-m003   <none>           <none>
2026-01-05 14:45:17,223 [INFO]   argo          resilience-heft-r2w8l-health-check-350149736            0/2     Error       0               71m     192.168.240.165   worker-w003   <none>           <none>
2026-01-05 14:45:17,223 [INFO]   argo          resilience-heft-r2w8l-health-check-366927355            0/2     Error       0               71m     192.168.15.239    worker-w005   <none>           <none>
2026-01-05 14:45:17,223 [INFO]   argo          resilience-heft-r2w8l-health-check-400482593            0/2     Error       0               71m     192.168.132.174   worker-w001   <none>           <none>
2026-01-05 14:45:17,223 [INFO]   argo          resilience-heft-r2w8l-heft-initialize-3054048250        0/2     Completed   0               71m     192.168.221.85    master-m003   <none>           <none>
2026-01-05 14:45:17,223 [INFO]   argo          resilience-heft-t988k-health-check-612195296            0/2     Error       0               76m     192.168.15.234    worker-w005   <none>           <none>
2026-01-05 14:45:17,223 [INFO]   argo          resilience-heft-t988k-health-check-628972915            0/2     Error       0               76m     192.168.240.160   worker-w003   <none>           <none>
2026-01-05 14:45:17,223 [INFO]   argo          resilience-heft-t988k-health-check-645750534            0/2     Error       0               76m     192.168.132.169   worker-w001   <none>           <none>
2026-01-05 14:45:17,223 [INFO]   argo          resilience-heft-t988k-heft-initialize-164752883         0/2     Completed   0               76m     192.168.221.80    master-m003   <none>           <none>
2026-01-05 14:45:17,223 [INFO]   argo          resilience-heft-vlt2m-health-check-2709447919           0/2     Error       0               32m     192.168.221.119   master-m003   <none>           <none>
2026-01-05 14:45:17,223 [INFO]   argo          resilience-heft-vlt2m-health-check-2726225538           0/2     Error       0               32m     192.168.221.120   master-m003   <none>           <none>
2026-01-05 14:45:17,223 [INFO]   argo          resilience-heft-vlt2m-health-check-2743003157           0/2     Error       0               32m     192.168.221.121   master-m003   <none>           <none>
2026-01-05 14:45:17,223 [INFO]   argo          resilience-heft-vlt2m-heft-initialize-840404112         0/2     Completed   0               33m     192.168.221.118   master-m003   <none>           <none>
2026-01-05 14:45:17,223 [INFO]   argo          resilience-heft-x86wd-health-check-2184182048           0/2     Error       0               31m     192.168.221.123   master-m003   <none>           <none>
2026-01-05 14:45:17,223 [INFO]   argo          resilience-heft-x86wd-health-check-2200959667           0/2     Error       0               31m     192.168.221.125   master-m003   <none>           <none>
2026-01-05 14:45:17,223 [INFO]   argo          resilience-heft-x86wd-health-check-2234514905           0/2     Error       0               31m     192.168.221.124   master-m003   <none>           <none>
2026-01-05 14:45:17,223 [INFO]   argo          resilience-heft-x86wd-heft-initialize-2152040770        0/2     Completed   0               31m     192.168.221.127   master-m003   <none>           <none>
2026-01-05 14:45:17,223 [INFO]   argo          workflow-controller-ccbd949dc-pxkms                     1/1     Running     3 (53m ago)     39h     192.168.195.242   worker-w002   <none>           <none>
2026-01-05 14:45:17,223 [INFO]   default       resilience-heft-1-201507-qflcd                          1/1     Running     0               9s      192.168.221.71    master-m003   <none>           <none>
2026-01-05 14:45:17,223 [INFO]   kube-system   calico-kube-controllers-7498b9bb4c-xd7tb                1/1     Running     0               38h     192.168.191.108   worker-w006   <none>           <none>
2026-01-05 14:45:17,223 [INFO]   kube-system   calico-node-4zhd4                                       1/1     Running     0               10d     192.168.56.105    worker-w002   <none>           <none>
2026-01-05 14:45:17,224 [INFO]   kube-system   calico-node-75nx6                                       1/1     Running     0               10d     192.168.56.109    worker-w006   <none>           <none>
2026-01-05 14:45:17,224 [INFO]   kube-system   calico-node-7lkdq                                       1/1     Running     0               10d     192.168.56.104    worker-w001   <none>           <none>
2026-01-05 14:45:17,224 [INFO]   kube-system   calico-node-85f8c                                       1/1     Running     0               10d     192.168.56.102    master-m002   <none>           <none>
2026-01-05 14:45:17,224 [INFO]   kube-system   calico-node-j8nb9                                       1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:45:17,224 [INFO]   kube-system   calico-node-lbcb2                                       1/1     Running     0               10d     192.168.56.106    worker-w003   <none>           <none>
2026-01-05 14:45:17,224 [INFO]   kube-system   calico-node-wlr5v                                       1/1     Running     0               10d     192.168.56.107    worker-w004   <none>           <none>
2026-01-05 14:45:17,224 [INFO]   kube-system   calico-node-xnzjw                                       1/1     Running     0               10d     192.168.56.103    master-m003   <none>           <none>
2026-01-05 14:45:17,224 [INFO]   kube-system   calico-node-xsltn                                       1/1     Running     0               10d     192.168.56.108    worker-w005   <none>           <none>
2026-01-05 14:45:17,224 [INFO]   kube-system   coredns-668d6bf9bc-2f74f                                1/1     Running     0               39h     192.168.15.230    worker-w005   <none>           <none>
2026-01-05 14:45:17,224 [INFO]   kube-system   coredns-668d6bf9bc-ssl9h                                1/1     Running     0               39h     192.168.132.165   worker-w001   <none>           <none>
2026-01-05 14:45:17,224 [INFO]   kube-system   etcd-master-m001                                        1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:45:17,224 [INFO]   kube-system   kube-apiserver-master-m001                              1/1     Running     1 (2d11h ago)   10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:45:17,224 [INFO]   kube-system   kube-controller-manager-master-m001                     1/1     Running     12 (29m ago)    10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:45:17,224 [INFO]   kube-system   kube-proxy-5lzhj                                        1/1     Running     0               10d     192.168.56.102    master-m002   <none>           <none>
2026-01-05 14:45:17,224 [INFO]   kube-system   kube-proxy-7jqkv                                        1/1     Running     0               10d     192.168.56.109    worker-w006   <none>           <none>
2026-01-05 14:45:17,224 [INFO]   kube-system   kube-proxy-cv5dt                                        1/1     Running     0               10d     192.168.56.108    worker-w005   <none>           <none>
2026-01-05 14:45:17,224 [INFO]   kube-system   kube-proxy-fvpmr                                        1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:45:17,224 [INFO]   kube-system   kube-proxy-hgs5z                                        1/1     Running     0               10d     192.168.56.106    worker-w003   <none>           <none>
2026-01-05 14:45:17,224 [INFO]   kube-system   kube-proxy-kmgqr                                        1/1     Running     0               10d     192.168.56.105    worker-w002   <none>           <none>
2026-01-05 14:45:17,224 [INFO]   kube-system   kube-proxy-rdbz5                                        1/1     Running     0               10d     192.168.56.107    worker-w004   <none>           <none>
2026-01-05 14:45:17,224 [INFO]   kube-system   kube-proxy-w8mnb                                        1/1     Running     0               10d     192.168.56.103    master-m003   <none>           <none>
2026-01-05 14:45:17,224 [INFO]   kube-system   kube-proxy-x9jxr                                        1/1     Running     0               10d     192.168.56.104    worker-w001   <none>           <none>
2026-01-05 14:45:17,224 [INFO]   kube-system   kube-scheduler-master-m001                              1/1     Running     11 (29m ago)    10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:45:17,224 [INFO] 
Pod distribution by node:
2026-01-05 14:45:17,311 [INFO]   Node master-m003: 33 pods
2026-01-05 14:45:17,311 [INFO]   Node worker-w001: 13 pods
2026-01-05 14:45:17,311 [INFO]   Node worker-w005: 13 pods
2026-01-05 14:45:17,311 [INFO]   Node worker-w003: 12 pods
2026-01-05 14:45:17,311 [INFO]   Node 39h: 1 pods
2026-01-05 14:45:17,311 [INFO]   Node worker-w006: 3 pods
2026-01-05 14:45:17,311 [INFO]   Node worker-w002: 2 pods
2026-01-05 14:45:17,311 [INFO]   Node master-m002: 2 pods
2026-01-05 14:45:17,311 [INFO]   Node master-m001: 3 pods
2026-01-05 14:45:17,311 [INFO]   Node worker-w004: 2 pods
2026-01-05 14:45:17,311 [INFO]   Node 10d: 3 pods
2026-01-05 14:45:17,311 [INFO] 
Filtering for simulation services:
2026-01-05 14:45:17,345 [INFO]   NAMESPACE     NAME                                                    READY   STATUS      RESTARTS        AGE     IP                NODE          NOMINATED NODE   READINESS GATES
2026-01-05 14:45:17,345 [INFO]   argo          argo-server-5c69cb69db-gdkl6                            1/1     Running     0               8d      192.168.221.65    master-m003   <none>           <none>
2026-01-05 14:45:17,345 [INFO]   argo          resilience-bench-ptngm-initialize-metrics-940927476     0/2     Completed   0               2d16h   192.168.221.125   master-m003   <none>           <none>
2026-01-05 14:45:17,345 [INFO]   argo          resilience-bench-ptngm-run-health-check-1168793163      0/2     Completed   0               2d16h   192.168.221.67    master-m003   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-bench-ptngm-run-health-check-1185570782      0/2     Completed   0               2d16h   192.168.221.66    master-m003   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-bench-ptngm-run-health-check-1202348401      0/2     Completed   0               2d16h   192.168.221.122   master-m003   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-bench-wf87f-initialize-metrics-3677018408    0/2     Completed   0               2d18h   192.168.221.100   master-m003   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-bench-wf87f-run-health-check-2914150102      0/2     Completed   0               2d18h   192.168.221.105   master-m003   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-bench-wf87f-run-health-check-3931060015      0/2     Completed   0               2d18h   192.168.221.104   master-m003   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-bench-wf87f-run-health-check-3947837634      0/2     Completed   0               2d18h   192.168.221.98    master-m003   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-bench-wf87f-run-health-check-3964615253      0/2     Completed   0               2d18h   192.168.221.103   master-m003   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-bench-wf87f-run-node-simulation-4173146970   0/2     Completed   0               2d18h   192.168.221.101   master-m003   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-bench-wf87f-run-rack-simulation-1356070019   0/2     Completed   0               2d18h   192.168.221.109   master-m003   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-heft-4cqbx-health-check-1555843180           0/2     Error       0               81m     192.168.132.166   worker-w001   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-heft-4cqbx-health-check-1589398418           0/2     Error       0               81m     192.168.15.231    worker-w005   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-heft-4cqbx-health-check-1606176037           0/2     Error       0               81m     192.168.240.157   worker-w003   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-heft-4cqbx-heft-initialize-2594215169        0/2     Completed   0               82m     192.168.221.79    master-m003   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-heft-76kbp-health-check-270747540            0/2     Error       0               73m     192.168.15.237    worker-w005   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-heft-76kbp-health-check-287525159            0/2     Error       0               73m     192.168.240.163   worker-w003   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-heft-76kbp-health-check-304302778            0/2     Error       0               73m     192.168.132.172   worker-w001   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-heft-76kbp-heft-initialize-3574960751        0/2     Completed   0               73m     192.168.221.86    master-m003   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-heft-b85xr-health-check-2807215411           0/2     Error       0               69m     192.168.132.175   worker-w001   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-heft-b85xr-health-check-2823993030           0/2     Error       0               69m     192.168.240.166   worker-w003   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-heft-b85xr-health-check-2840770649           0/2     Error       0               69m     192.168.15.240    worker-w005   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-heft-b85xr-heft-initialize-1754406316        0/2     Completed   0               70m     192.168.221.88    master-m003   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-heft-fdw9f-health-check-1896357264           0/2     Error       0               75m     192.168.15.235    worker-w005   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-heft-fdw9f-health-check-1913134883           0/2     Error       0               75m     192.168.240.161   worker-w003   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-heft-fdw9f-health-check-1929912502           0/2     Error       0               75m     192.168.132.170   worker-w001   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-heft-fdw9f-heft-initialize-2651619139        0/2     Completed   0               75m     192.168.221.77    master-m003   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-heft-gf256-health-check-946619120            0/2     Error       0               74m     192.168.132.171   worker-w001   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-heft-gf256-health-check-980174358            0/2     Error       0               74m     192.168.15.236    worker-w005   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-heft-gf256-health-check-996951977            0/2     Error       0               74m     192.168.240.162   worker-w003   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-heft-gf256-heft-initialize-1548293021        0/2     Completed   0               74m     192.168.221.84    master-m003   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-heft-gxwpg-health-check-3428526804           0/2     Error       0               77m     192.168.240.159   worker-w003   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-heft-gxwpg-health-check-3445304423           0/2     Error       0               77m     192.168.15.233    worker-w005   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-heft-gxwpg-health-check-3478859661           0/2     Error       0               77m     192.168.132.168   worker-w001   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-heft-gxwpg-heft-initialize-2377397022        0/2     Completed   0               77m     192.168.221.81    master-m003   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-heft-pc2kb-health-check-3258812212           0/2     Error       0               71m     192.168.15.238    worker-w005   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-heft-pc2kb-health-check-3275589831           0/2     Error       0               71m     192.168.240.164   worker-w003   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-heft-pc2kb-health-check-3292367450           0/2     Error       0               71m     192.168.132.173   worker-w001   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-heft-pc2kb-heft-initialize-321333135         0/2     Completed   0               72m     192.168.221.83    master-m003   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-heft-qsmch-health-check-283908068            0/2     Error       0               78m     192.168.15.232    worker-w005   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-heft-qsmch-health-check-300685687            0/2     Error       0               78m     192.168.240.158   worker-w003   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-heft-qsmch-health-check-317463306            0/2     Error       0               78m     192.168.132.167   worker-w001   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-heft-qsmch-heft-initialize-1908810015        0/2     Completed   0               79m     192.168.221.82    master-m003   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-heft-r2w8l-health-check-350149736            0/2     Error       0               71m     192.168.240.165   worker-w003   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-heft-r2w8l-health-check-366927355            0/2     Error       0               71m     192.168.15.239    worker-w005   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-heft-r2w8l-health-check-400482593            0/2     Error       0               71m     192.168.132.174   worker-w001   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-heft-r2w8l-heft-initialize-3054048250        0/2     Completed   0               71m     192.168.221.85    master-m003   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-heft-t988k-health-check-612195296            0/2     Error       0               76m     192.168.15.234    worker-w005   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-heft-t988k-health-check-628972915            0/2     Error       0               76m     192.168.240.160   worker-w003   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-heft-t988k-health-check-645750534            0/2     Error       0               76m     192.168.132.169   worker-w001   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-heft-t988k-heft-initialize-164752883         0/2     Completed   0               76m     192.168.221.80    master-m003   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-heft-vlt2m-health-check-2709447919           0/2     Error       0               32m     192.168.221.119   master-m003   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-heft-vlt2m-health-check-2726225538           0/2     Error       0               32m     192.168.221.120   master-m003   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-heft-vlt2m-health-check-2743003157           0/2     Error       0               32m     192.168.221.121   master-m003   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-heft-vlt2m-heft-initialize-840404112         0/2     Completed   0               33m     192.168.221.118   master-m003   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-heft-x86wd-health-check-2184182048           0/2     Error       0               31m     192.168.221.123   master-m003   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-heft-x86wd-health-check-2200959667           0/2     Error       0               31m     192.168.221.125   master-m003   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-heft-x86wd-health-check-2234514905           0/2     Error       0               31m     192.168.221.124   master-m003   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          resilience-heft-x86wd-heft-initialize-2152040770        0/2     Completed   0               31m     192.168.221.127   master-m003   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   argo          workflow-controller-ccbd949dc-pxkms                     1/1     Running     3 (53m ago)     39h     192.168.195.242   worker-w002   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   default       resilience-heft-1-201507-qflcd                          1/1     Running     0               9s      192.168.221.71    master-m003   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   kube-system   calico-kube-controllers-7498b9bb4c-xd7tb                1/1     Running     0               38h     192.168.191.108   worker-w006   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   kube-system   calico-node-4zhd4                                       1/1     Running     0               10d     192.168.56.105    worker-w002   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   kube-system   calico-node-75nx6                                       1/1     Running     0               10d     192.168.56.109    worker-w006   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   kube-system   calico-node-7lkdq                                       1/1     Running     0               10d     192.168.56.104    worker-w001   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   kube-system   calico-node-85f8c                                       1/1     Running     0               10d     192.168.56.102    master-m002   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   kube-system   calico-node-j8nb9                                       1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   kube-system   calico-node-lbcb2                                       1/1     Running     0               10d     192.168.56.106    worker-w003   <none>           <none>
2026-01-05 14:45:17,346 [INFO]   kube-system   calico-node-wlr5v                                       1/1     Running     0               10d     192.168.56.107    worker-w004   <none>           <none>
2026-01-05 14:45:17,347 [INFO]   kube-system   calico-node-xnzjw                                       1/1     Running     0               10d     192.168.56.103    master-m003   <none>           <none>
2026-01-05 14:45:17,347 [INFO]   kube-system   calico-node-xsltn                                       1/1     Running     0               10d     192.168.56.108    worker-w005   <none>           <none>
2026-01-05 14:45:17,347 [INFO]   kube-system   coredns-668d6bf9bc-2f74f                                1/1     Running     0               39h     192.168.15.230    worker-w005   <none>           <none>
2026-01-05 14:45:17,347 [INFO]   kube-system   coredns-668d6bf9bc-ssl9h                                1/1     Running     0               39h     192.168.132.165   worker-w001   <none>           <none>
2026-01-05 14:45:17,347 [INFO]   kube-system   etcd-master-m001                                        1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:45:17,347 [INFO]   kube-system   kube-apiserver-master-m001                              1/1     Running     1 (2d11h ago)   10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:45:17,347 [INFO]   kube-system   kube-controller-manager-master-m001                     1/1     Running     12 (29m ago)    10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:45:17,347 [INFO]   kube-system   kube-proxy-5lzhj                                        1/1     Running     0               10d     192.168.56.102    master-m002   <none>           <none>
2026-01-05 14:45:17,347 [INFO]   kube-system   kube-proxy-7jqkv                                        1/1     Running     0               10d     192.168.56.109    worker-w006   <none>           <none>
2026-01-05 14:45:17,347 [INFO]   kube-system   kube-proxy-cv5dt                                        1/1     Running     0               10d     192.168.56.108    worker-w005   <none>           <none>
2026-01-05 14:45:17,347 [INFO]   kube-system   kube-proxy-fvpmr                                        1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:45:17,347 [INFO]   kube-system   kube-proxy-hgs5z                                        1/1     Running     0               10d     192.168.56.106    worker-w003   <none>           <none>
2026-01-05 14:45:17,347 [INFO]   kube-system   kube-proxy-kmgqr                                        1/1     Running     0               10d     192.168.56.105    worker-w002   <none>           <none>
2026-01-05 14:45:17,347 [INFO]   kube-system   kube-proxy-rdbz5                                        1/1     Running     0               10d     192.168.56.107    worker-w004   <none>           <none>
2026-01-05 14:45:17,347 [INFO]   kube-system   kube-proxy-w8mnb                                        1/1     Running     0               10d     192.168.56.103    master-m003   <none>           <none>
2026-01-05 14:45:17,347 [INFO]   kube-system   kube-proxy-x9jxr                                        1/1     Running     0               10d     192.168.56.104    worker-w001   <none>           <none>
2026-01-05 14:45:17,347 [INFO]   kube-system   kube-scheduler-master-m001                              1/1     Running     11 (29m ago)    10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:45:17,347 [INFO] 
Pod distribution by node:
2026-01-05 14:45:17,401 [INFO]   Node master-m003: 33 pods
2026-01-05 14:45:17,401 [INFO]   Node worker-w001: 13 pods
2026-01-05 14:45:17,402 [INFO]   Node worker-w005: 13 pods
2026-01-05 14:45:17,402 [INFO]   Node worker-w003: 12 pods
2026-01-05 14:45:17,402 [INFO]   Node 39h: 1 pods
2026-01-05 14:45:17,402 [INFO]   Node worker-w006: 3 pods
2026-01-05 14:45:17,402 [INFO]   Node worker-w002: 2 pods
2026-01-05 14:45:17,402 [INFO]   Node master-m002: 2 pods
2026-01-05 14:45:17,402 [INFO]   Node master-m001: 3 pods
2026-01-05 14:45:17,402 [INFO]   Node worker-w004: 2 pods
2026-01-05 14:45:17,402 [INFO]   Node 10d: 3 pods
2026-01-05 14:45:17,402 [INFO] 
Filtering for simulation services:
2026-01-05 14:45:17,421 [INFO] Node master-m001 is Ready
2026-01-05 14:45:17,421 [INFO] Node master-m002 is Ready
2026-01-05 14:45:17,421 [INFO] Node master-m003 is Ready
2026-01-05 14:45:17,421 [INFO] Node worker-w001 is Ready
2026-01-05 14:45:17,421 [INFO] Node worker-w002 is Ready
2026-01-05 14:45:17,424 [INFO] Node worker-w003 is Ready
2026-01-05 14:45:17,424 [INFO] Node worker-w004 is Ready
2026-01-05 14:45:17,424 [INFO] Node worker-w005 is Ready
2026-01-05 14:45:17,424 [INFO] Node worker-w006 is Ready
2026-01-05 14:45:17,431 [WARNING] No pods found for etcd-sim
2026-01-05 14:45:17,436 [WARNING] No pods found for postgres-sim
2026-01-05 14:45:17,440 [WARNING] No pods found for redis-sim
2026-01-05 14:45:17,443 [WARNING] No pods found for nginx-sim
2026-01-05 14:45:17,444 [WARNING] No pods found for auth-sim
2026-01-05 14:45:17,445 [INFO] Completed full health check
2026-01-05 14:45:17,531 [INFO]   Node master-m003: 33 pods
2026-01-05 14:45:17,532 [INFO]   Node worker-w001: 13 pods
2026-01-05 14:45:17,532 [INFO]   Node worker-w005: 13 pods
2026-01-05 14:45:17,532 [INFO]   Node worker-w003: 12 pods
2026-01-05 14:45:17,532 [INFO]   Node 39h: 1 pods
2026-01-05 14:45:17,532 [INFO]   Node worker-w006: 3 pods
2026-01-05 14:45:17,532 [INFO]   Node worker-w002: 2 pods
2026-01-05 14:45:17,532 [INFO]   Node master-m002: 2 pods
2026-01-05 14:45:17,532 [INFO]   Node master-m001: 3 pods
2026-01-05 14:45:17,532 [INFO]   Node worker-w004: 2 pods
2026-01-05 14:45:17,532 [INFO]   Node 10d: 3 pods
2026-01-05 14:45:17,532 [INFO] 
Filtering for simulation services:
2026-01-05 14:45:17,546 [INFO] Node master-m001 is Ready
2026-01-05 14:45:17,546 [INFO] Node master-m002 is Ready
2026-01-05 14:45:17,546 [INFO] Node master-m003 is Ready
2026-01-05 14:45:17,546 [INFO] Node worker-w001 is Ready
2026-01-05 14:45:17,546 [INFO] Node worker-w002 is Ready
2026-01-05 14:45:17,546 [INFO] Node worker-w003 is Ready
2026-01-05 14:45:17,546 [INFO] Node worker-w004 is Ready
2026-01-05 14:45:17,547 [INFO] Node worker-w005 is Ready
2026-01-05 14:45:17,547 [INFO] Node worker-w006 is Ready
2026-01-05 14:45:17,550 [WARNING] No pods found for etcd-sim
2026-01-05 14:45:17,552 [WARNING] No pods found for postgres-sim
2026-01-05 14:45:17,557 [WARNING] No pods found for redis-sim
2026-01-05 14:45:17,613 [WARNING] No pods found for nginx-sim
2026-01-05 14:45:17,616 [WARNING] No pods found for auth-sim
2026-01-05 14:45:17,617 [INFO] Completed full health check
2026-01-05 14:45:17,632 [INFO] Node master-m001 is Ready
2026-01-05 14:45:17,632 [INFO] Node master-m002 is Ready
2026-01-05 14:45:17,632 [INFO] Node master-m003 is Ready
2026-01-05 14:45:17,632 [INFO] Node worker-w001 is Ready
2026-01-05 14:45:17,632 [INFO] Node worker-w002 is Ready
2026-01-05 14:45:17,632 [INFO] Node worker-w003 is Ready
2026-01-05 14:45:17,632 [INFO] Node worker-w004 is Ready
2026-01-05 14:45:17,632 [INFO] Node worker-w005 is Ready
2026-01-05 14:45:17,632 [INFO] Node worker-w006 is Ready
2026-01-05 14:45:17,635 [WARNING] No pods found for etcd-sim
2026-01-05 14:45:17,638 [WARNING] No pods found for postgres-sim
2026-01-05 14:45:17,640 [WARNING] No pods found for redis-sim
2026-01-05 14:45:17,641 [WARNING] No pods found for nginx-sim
2026-01-05 14:45:17,643 [WARNING] No pods found for auth-sim
2026-01-05 14:45:17,643 [INFO] Completed full health check
[2026-01-05 14:45:17] TIMING: HEALTH_CHECKS_PARALLEL completed in 5 seconds

================================================================
 STEP 2: NODE FAILURE SIMULATION (HEFT-Aware)
 Excluding node: worker-w005, zone: R3
 Time: Mon Jan  5 14:45:17 UTC 2026
================================================================
Log directory created/verified: /app/logs
File logging configured successfully
2026-01-05 14:45:17,883 [INFO] Loaded in-cluster Kubernetes config
2026-01-05 14:45:17,883 [INFO] Running on host: resilience-heft-1-201507-qflcd
2026-01-05 14:45:17,883 [INFO] Detected current node: master-m003, zone: R3
2026-01-05 14:45:17,883 [INFO] Checking if we have permissions to modify nodes...
2026-01-05 14:45:17,914 [INFO] Testing permissions using node: master-m001
2026-01-05 14:45:17,919 [INFO] Permission check successful - we can modify nodes
2026-01-05 14:45:17,919 [INFO] Using real Kubernetes API for node control
2026-01-05 14:45:17,919 [INFO] Action received: simulate-node
2026-01-05 14:45:17,919 [INFO] Stabilization time: 60 seconds
2026-01-05 14:45:17,919 [INFO] Simulating node failure: worker-w002
2026-01-05 14:45:17,920 [INFO] Simulating node failure for worker-w002 using Kubernetes API
2026-01-05 14:45:17,931 [INFO] Node worker-w002 cordoned
2026-01-05 14:45:17,956 [INFO] Node worker-w002 tainted with NoExecute
2026-01-05 14:45:17,957 [INFO] Node worker-w002 powered off (delay 5s)
2026-01-05 14:45:17,957 [INFO] Node worker-w002 down for 10 seconds
2026-01-05 14:45:17,957 [INFO] Waiting 60 seconds for the cluster to stabilize before health check...
2026-01-05 14:46:18,016 [INFO] Running health check after node power off
2026-01-05 14:46:18,016 [INFO] Starting full health check
2026-01-05 14:46:18,016 [INFO] 
============ DETAILED NODE STATUS ============
2026-01-05 14:46:18,016 [INFO] Basic Node Information (kubectl get nodes -o wide):
2026-01-05 14:46:18,107 [INFO]   NAME          STATUS                     ROLES           AGE   VERSION    INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
2026-01-05 14:46:18,107 [INFO]   master-m001   Ready                      control-plane   10d   v1.32.11   192.168.56.101   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:46:18,107 [INFO]   master-m002   Ready                      control-plane   10d   v1.32.11   192.168.56.102   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:46:18,107 [INFO]   master-m003   Ready                      control-plane   10d   v1.32.11   192.168.56.103   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:46:18,107 [INFO]   worker-w001   Ready                      <none>          10d   v1.32.11   192.168.56.104   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:46:18,108 [INFO]   worker-w002   Ready,SchedulingDisabled   <none>          10d   v1.32.11   192.168.56.105   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:46:18,108 [INFO]   worker-w003   Ready                      <none>          10d   v1.32.11   192.168.56.106   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:46:18,108 [INFO]   worker-w004   Ready                      <none>          10d   v1.32.11   192.168.56.107   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:46:18,108 [INFO]   worker-w005   Ready                      <none>          10d   v1.32.11   192.168.56.108   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:46:18,108 [INFO]   worker-w006   Ready                      <none>          10d   v1.32.11   192.168.56.109   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:46:18,108 [INFO] 
Enhanced Node Status (with taint and cordon indicators):
2026-01-05 14:46:18,108 [INFO]   NAME                STATUS    ROLES           ZONE   CORDONED   TAINTS
2026-01-05 14:46:18,315 [INFO]   master-m001     Ready  ✓ worker         R1    No       node-role.kubernetes.io/control-plane 
2026-01-05 14:46:18,497 [INFO]   master-m002     Ready  ✓ worker         R2    No       None 
2026-01-05 14:46:18,681 [INFO]   master-m003     Ready  ✓ worker         R3    No       None 
2026-01-05 14:46:18,865 [INFO]   worker-w001     Ready  ✓ worker         R1    No       None 
2026-01-05 14:46:19,076 [INFO]   worker-w002     Ready  ✓ worker         R1    YES     ⚠️ simulated-failure, node.kubernetes.io/unschedulable ⚠️
2026-01-05 14:46:19,268 [INFO]   worker-w003     Ready  ✓ worker         R2    No       None 
2026-01-05 14:46:19,448 [INFO]   worker-w004     Ready  ✓ worker         R2    No       None 
2026-01-05 14:46:19,628 [INFO]   worker-w005     Ready  ✓ worker         R3    No       None 
2026-01-05 14:46:19,803 [INFO]   worker-w006     Ready  ✓ worker         R3    No       None 
2026-01-05 14:46:19,803 [INFO] 
Legend:
2026-01-05 14:46:19,804 [INFO]   ✓ = Node is Ready
2026-01-05 14:46:19,804 [INFO]   ⚠️ = Warning indicator (NotReady, Cordoned, or has simulated-failure taint)
2026-01-05 14:46:19,804 [INFO] ============ DETAILED POD INFORMATION ============
2026-01-05 14:46:19,804 [INFO] Running 'kubectl get pods -o wide' to show detailed pod placement:
2026-01-05 14:46:19,935 [INFO]   NAMESPACE     NAME                                                    READY   STATUS      RESTARTS        AGE     IP                NODE          NOMINATED NODE   READINESS GATES
2026-01-05 14:46:19,935 [INFO]   argo          argo-server-5c69cb69db-gdkl6                            1/1     Running     0               8d      192.168.221.65    master-m003   <none>           <none>
2026-01-05 14:46:19,935 [INFO]   argo          resilience-bench-ptngm-initialize-metrics-940927476     0/2     Completed   0               2d16h   192.168.221.125   master-m003   <none>           <none>
2026-01-05 14:46:19,936 [INFO]   argo          resilience-bench-ptngm-run-health-check-1168793163      0/2     Completed   0               2d16h   192.168.221.67    master-m003   <none>           <none>
2026-01-05 14:46:19,936 [INFO]   argo          resilience-bench-ptngm-run-health-check-1185570782      0/2     Completed   0               2d16h   192.168.221.66    master-m003   <none>           <none>
2026-01-05 14:46:19,936 [INFO]   argo          resilience-bench-ptngm-run-health-check-1202348401      0/2     Completed   0               2d16h   192.168.221.122   master-m003   <none>           <none>
2026-01-05 14:46:19,936 [INFO]   argo          resilience-bench-wf87f-initialize-metrics-3677018408    0/2     Completed   0               2d18h   192.168.221.100   master-m003   <none>           <none>
2026-01-05 14:46:19,936 [INFO]   argo          resilience-bench-wf87f-run-health-check-2914150102      0/2     Completed   0               2d18h   192.168.221.105   master-m003   <none>           <none>
2026-01-05 14:46:19,936 [INFO]   argo          resilience-bench-wf87f-run-health-check-3931060015      0/2     Completed   0               2d18h   192.168.221.104   master-m003   <none>           <none>
2026-01-05 14:46:19,937 [INFO]   argo          resilience-bench-wf87f-run-health-check-3947837634      0/2     Completed   0               2d18h   192.168.221.98    master-m003   <none>           <none>
2026-01-05 14:46:19,937 [INFO]   argo          resilience-bench-wf87f-run-health-check-3964615253      0/2     Completed   0               2d18h   192.168.221.103   master-m003   <none>           <none>
2026-01-05 14:46:19,937 [INFO]   argo          resilience-bench-wf87f-run-node-simulation-4173146970   0/2     Completed   0               2d18h   192.168.221.101   master-m003   <none>           <none>
2026-01-05 14:46:19,937 [INFO]   argo          resilience-bench-wf87f-run-rack-simulation-1356070019   0/2     Completed   0               2d18h   192.168.221.109   master-m003   <none>           <none>
2026-01-05 14:46:19,937 [INFO]   argo          resilience-heft-4cqbx-health-check-1555843180           0/2     Error       0               82m     192.168.132.166   worker-w001   <none>           <none>
2026-01-05 14:46:19,937 [INFO]   argo          resilience-heft-4cqbx-health-check-1589398418           0/2     Error       0               82m     192.168.15.231    worker-w005   <none>           <none>
2026-01-05 14:46:19,937 [INFO]   argo          resilience-heft-4cqbx-health-check-1606176037           0/2     Error       0               82m     192.168.240.157   worker-w003   <none>           <none>
2026-01-05 14:46:19,938 [INFO]   argo          resilience-heft-4cqbx-heft-initialize-2594215169        0/2     Completed   0               83m     192.168.221.79    master-m003   <none>           <none>
2026-01-05 14:46:19,938 [INFO]   argo          resilience-heft-76kbp-health-check-270747540            0/2     Error       0               74m     192.168.15.237    worker-w005   <none>           <none>
2026-01-05 14:46:19,938 [INFO]   argo          resilience-heft-76kbp-health-check-287525159            0/2     Error       0               74m     192.168.240.163   worker-w003   <none>           <none>
2026-01-05 14:46:19,938 [INFO]   argo          resilience-heft-76kbp-health-check-304302778            0/2     Error       0               74m     192.168.132.172   worker-w001   <none>           <none>
2026-01-05 14:46:19,938 [INFO]   argo          resilience-heft-76kbp-heft-initialize-3574960751        0/2     Completed   0               74m     192.168.221.86    master-m003   <none>           <none>
2026-01-05 14:46:19,938 [INFO]   argo          resilience-heft-b85xr-health-check-2807215411           0/2     Error       0               70m     192.168.132.175   worker-w001   <none>           <none>
2026-01-05 14:46:19,938 [INFO]   argo          resilience-heft-b85xr-health-check-2823993030           0/2     Error       0               70m     192.168.240.166   worker-w003   <none>           <none>
2026-01-05 14:46:19,938 [INFO]   argo          resilience-heft-b85xr-health-check-2840770649           0/2     Error       0               70m     192.168.15.240    worker-w005   <none>           <none>
2026-01-05 14:46:19,939 [INFO]   argo          resilience-heft-b85xr-heft-initialize-1754406316        0/2     Completed   0               71m     192.168.221.88    master-m003   <none>           <none>
2026-01-05 14:46:19,939 [INFO]   argo          resilience-heft-fdw9f-health-check-1896357264           0/2     Error       0               76m     192.168.15.235    worker-w005   <none>           <none>
2026-01-05 14:46:19,939 [INFO]   argo          resilience-heft-fdw9f-health-check-1913134883           0/2     Error       0               76m     192.168.240.161   worker-w003   <none>           <none>
2026-01-05 14:46:19,939 [INFO]   argo          resilience-heft-fdw9f-health-check-1929912502           0/2     Error       0               76m     192.168.132.170   worker-w001   <none>           <none>
2026-01-05 14:46:19,939 [INFO]   argo          resilience-heft-fdw9f-heft-initialize-2651619139        0/2     Completed   0               76m     192.168.221.77    master-m003   <none>           <none>
2026-01-05 14:46:19,939 [INFO]   argo          resilience-heft-gf256-health-check-946619120            0/2     Error       0               75m     192.168.132.171   worker-w001   <none>           <none>
2026-01-05 14:46:19,940 [INFO]   argo          resilience-heft-gf256-health-check-980174358            0/2     Error       0               75m     192.168.15.236    worker-w005   <none>           <none>
2026-01-05 14:46:19,940 [INFO]   argo          resilience-heft-gf256-health-check-996951977            0/2     Error       0               75m     192.168.240.162   worker-w003   <none>           <none>
2026-01-05 14:46:19,940 [INFO]   argo          resilience-heft-gf256-heft-initialize-1548293021        0/2     Completed   0               75m     192.168.221.84    master-m003   <none>           <none>
2026-01-05 14:46:19,940 [INFO]   argo          resilience-heft-gxwpg-health-check-3428526804           0/2     Error       0               78m     192.168.240.159   worker-w003   <none>           <none>
2026-01-05 14:46:19,940 [INFO]   argo          resilience-heft-gxwpg-health-check-3445304423           0/2     Error       0               78m     192.168.15.233    worker-w005   <none>           <none>
2026-01-05 14:46:19,940 [INFO]   argo          resilience-heft-gxwpg-health-check-3478859661           0/2     Error       0               78m     192.168.132.168   worker-w001   <none>           <none>
2026-01-05 14:46:19,941 [INFO]   argo          resilience-heft-gxwpg-heft-initialize-2377397022        0/2     Completed   0               78m     192.168.221.81    master-m003   <none>           <none>
2026-01-05 14:46:19,941 [INFO]   argo          resilience-heft-pc2kb-health-check-3258812212           0/2     Error       0               72m     192.168.15.238    worker-w005   <none>           <none>
2026-01-05 14:46:19,941 [INFO]   argo          resilience-heft-pc2kb-health-check-3275589831           0/2     Error       0               72m     192.168.240.164   worker-w003   <none>           <none>
2026-01-05 14:46:19,941 [INFO]   argo          resilience-heft-pc2kb-health-check-3292367450           0/2     Error       0               72m     192.168.132.173   worker-w001   <none>           <none>
2026-01-05 14:46:19,941 [INFO]   argo          resilience-heft-pc2kb-heft-initialize-321333135         0/2     Completed   0               73m     192.168.221.83    master-m003   <none>           <none>
2026-01-05 14:46:19,941 [INFO]   argo          resilience-heft-qsmch-health-check-283908068            0/2     Error       0               79m     192.168.15.232    worker-w005   <none>           <none>
2026-01-05 14:46:19,941 [INFO]   argo          resilience-heft-qsmch-health-check-300685687            0/2     Error       0               79m     192.168.240.158   worker-w003   <none>           <none>
2026-01-05 14:46:19,942 [INFO]   argo          resilience-heft-qsmch-health-check-317463306            0/2     Error       0               79m     192.168.132.167   worker-w001   <none>           <none>
2026-01-05 14:46:19,942 [INFO]   argo          resilience-heft-qsmch-heft-initialize-1908810015        0/2     Completed   0               80m     192.168.221.82    master-m003   <none>           <none>
2026-01-05 14:46:19,942 [INFO]   argo          resilience-heft-r2w8l-health-check-350149736            0/2     Error       0               72m     192.168.240.165   worker-w003   <none>           <none>
2026-01-05 14:46:19,942 [INFO]   argo          resilience-heft-r2w8l-health-check-366927355            0/2     Error       0               72m     192.168.15.239    worker-w005   <none>           <none>
2026-01-05 14:46:19,942 [INFO]   argo          resilience-heft-r2w8l-health-check-400482593            0/2     Error       0               72m     192.168.132.174   worker-w001   <none>           <none>
2026-01-05 14:46:19,942 [INFO]   argo          resilience-heft-r2w8l-heft-initialize-3054048250        0/2     Completed   0               72m     192.168.221.85    master-m003   <none>           <none>
2026-01-05 14:46:19,943 [INFO]   argo          resilience-heft-t988k-health-check-612195296            0/2     Error       0               77m     192.168.15.234    worker-w005   <none>           <none>
2026-01-05 14:46:19,943 [INFO]   argo          resilience-heft-t988k-health-check-628972915            0/2     Error       0               77m     192.168.240.160   worker-w003   <none>           <none>
2026-01-05 14:46:19,943 [INFO]   argo          resilience-heft-t988k-health-check-645750534            0/2     Error       0               77m     192.168.132.169   worker-w001   <none>           <none>
2026-01-05 14:46:19,943 [INFO]   argo          resilience-heft-t988k-heft-initialize-164752883         0/2     Completed   0               77m     192.168.221.80    master-m003   <none>           <none>
2026-01-05 14:46:19,943 [INFO]   argo          resilience-heft-vlt2m-health-check-2709447919           0/2     Error       0               33m     192.168.221.119   master-m003   <none>           <none>
2026-01-05 14:46:19,943 [INFO]   argo          resilience-heft-vlt2m-health-check-2726225538           0/2     Error       0               33m     192.168.221.120   master-m003   <none>           <none>
2026-01-05 14:46:19,943 [INFO]   argo          resilience-heft-vlt2m-health-check-2743003157           0/2     Error       0               33m     192.168.221.121   master-m003   <none>           <none>
2026-01-05 14:46:19,943 [INFO]   argo          resilience-heft-vlt2m-heft-initialize-840404112         0/2     Completed   0               34m     192.168.221.118   master-m003   <none>           <none>
2026-01-05 14:46:19,944 [INFO]   argo          resilience-heft-x86wd-health-check-2184182048           0/2     Error       0               32m     192.168.221.123   master-m003   <none>           <none>
2026-01-05 14:46:19,944 [INFO]   argo          resilience-heft-x86wd-health-check-2200959667           0/2     Error       0               32m     192.168.221.125   master-m003   <none>           <none>
2026-01-05 14:46:19,944 [INFO]   argo          resilience-heft-x86wd-health-check-2234514905           0/2     Error       0               32m     192.168.221.124   master-m003   <none>           <none>
2026-01-05 14:46:19,944 [INFO]   argo          resilience-heft-x86wd-heft-initialize-2152040770        0/2     Completed   0               32m     192.168.221.127   master-m003   <none>           <none>
2026-01-05 14:46:19,944 [INFO]   argo          workflow-controller-ccbd949dc-pzf68                     1/1     Running     0               61s     192.168.153.241   worker-w004   <none>           <none>
2026-01-05 14:46:19,944 [INFO]   default       resilience-heft-1-201507-qflcd                          1/1     Running     0               71s     192.168.221.71    master-m003   <none>           <none>
2026-01-05 14:46:19,944 [INFO]   kube-system   calico-kube-controllers-7498b9bb4c-xd7tb                1/1     Running     0               38h     192.168.191.108   worker-w006   <none>           <none>
2026-01-05 14:46:19,945 [INFO]   kube-system   calico-node-4zhd4                                       1/1     Running     0               10d     192.168.56.105    worker-w002   <none>           <none>
2026-01-05 14:46:19,945 [INFO]   kube-system   calico-node-75nx6                                       1/1     Running     0               10d     192.168.56.109    worker-w006   <none>           <none>
2026-01-05 14:46:19,945 [INFO]   kube-system   calico-node-7lkdq                                       1/1     Running     0               10d     192.168.56.104    worker-w001   <none>           <none>
2026-01-05 14:46:19,945 [INFO]   kube-system   calico-node-85f8c                                       1/1     Running     0               10d     192.168.56.102    master-m002   <none>           <none>
2026-01-05 14:46:19,945 [INFO]   kube-system   calico-node-j8nb9                                       1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:46:19,945 [INFO]   kube-system   calico-node-lbcb2                                       1/1     Running     0               10d     192.168.56.106    worker-w003   <none>           <none>
2026-01-05 14:46:19,945 [INFO]   kube-system   calico-node-wlr5v                                       1/1     Running     0               10d     192.168.56.107    worker-w004   <none>           <none>
2026-01-05 14:46:19,946 [INFO]   kube-system   calico-node-xnzjw                                       1/1     Running     0               10d     192.168.56.103    master-m003   <none>           <none>
2026-01-05 14:46:19,946 [INFO]   kube-system   calico-node-xsltn                                       1/1     Running     0               10d     192.168.56.108    worker-w005   <none>           <none>
2026-01-05 14:46:19,946 [INFO]   kube-system   coredns-668d6bf9bc-2f74f                                1/1     Running     0               39h     192.168.15.230    worker-w005   <none>           <none>
2026-01-05 14:46:19,946 [INFO]   kube-system   coredns-668d6bf9bc-ssl9h                                1/1     Running     0               39h     192.168.132.165   worker-w001   <none>           <none>
2026-01-05 14:46:19,946 [INFO]   kube-system   etcd-master-m001                                        1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:46:19,946 [INFO]   kube-system   kube-apiserver-master-m001                              1/1     Running     1 (2d11h ago)   10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:46:19,946 [INFO]   kube-system   kube-controller-manager-master-m001                     1/1     Running     12 (30m ago)    10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:46:19,947 [INFO]   kube-system   kube-proxy-5lzhj                                        1/1     Running     0               10d     192.168.56.102    master-m002   <none>           <none>
2026-01-05 14:46:19,947 [INFO]   kube-system   kube-proxy-7jqkv                                        1/1     Running     0               10d     192.168.56.109    worker-w006   <none>           <none>
2026-01-05 14:46:19,947 [INFO]   kube-system   kube-proxy-cv5dt                                        1/1     Running     0               10d     192.168.56.108    worker-w005   <none>           <none>
2026-01-05 14:46:19,947 [INFO]   kube-system   kube-proxy-fvpmr                                        1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:46:19,947 [INFO]   kube-system   kube-proxy-hgs5z                                        1/1     Running     0               10d     192.168.56.106    worker-w003   <none>           <none>
2026-01-05 14:46:19,947 [INFO]   kube-system   kube-proxy-kmgqr                                        1/1     Running     0               10d     192.168.56.105    worker-w002   <none>           <none>
2026-01-05 14:46:19,947 [INFO]   kube-system   kube-proxy-rdbz5                                        1/1     Running     0               10d     192.168.56.107    worker-w004   <none>           <none>
2026-01-05 14:46:19,947 [INFO]   kube-system   kube-proxy-w8mnb                                        1/1     Running     0               10d     192.168.56.103    master-m003   <none>           <none>
2026-01-05 14:46:19,948 [INFO]   kube-system   kube-proxy-x9jxr                                        1/1     Running     0               10d     192.168.56.104    worker-w001   <none>           <none>
2026-01-05 14:46:19,948 [INFO]   kube-system   kube-scheduler-master-m001                              1/1     Running     11 (30m ago)    10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:46:19,948 [INFO] 
Pod distribution by node:
2026-01-05 14:46:20,600 [INFO]   Node master-m003: 33 pods
2026-01-05 14:46:20,600 [INFO]   Node worker-w001: 13 pods
2026-01-05 14:46:20,600 [INFO]   Node worker-w005: 13 pods
2026-01-05 14:46:20,600 [INFO]   Node worker-w003: 12 pods
2026-01-05 14:46:20,600 [INFO]   Node worker-w004: 3 pods
2026-01-05 14:46:20,600 [INFO]   Node worker-w006: 3 pods
2026-01-05 14:46:20,600 [INFO]   Node worker-w002: 2 pods
2026-01-05 14:46:20,600 [INFO]   Node master-m002: 2 pods
2026-01-05 14:46:20,600 [INFO]   Node master-m001: 3 pods
2026-01-05 14:46:20,600 [INFO]   Node 10d: 3 pods
2026-01-05 14:46:20,600 [INFO] 
Filtering for simulation services:
2026-01-05 14:46:20,873 [INFO] Node master-m001 is Ready
2026-01-05 14:46:20,874 [INFO] Node master-m002 is Ready
2026-01-05 14:46:20,874 [INFO] Node master-m003 is Ready
2026-01-05 14:46:20,874 [INFO] Node worker-w001 is Ready
2026-01-05 14:46:20,874 [INFO] Node worker-w002 is Ready
2026-01-05 14:46:20,874 [INFO] Node worker-w003 is Ready
2026-01-05 14:46:20,874 [INFO] Node worker-w004 is Ready
2026-01-05 14:46:20,874 [INFO] Node worker-w005 is Ready
2026-01-05 14:46:20,874 [INFO] Node worker-w006 is Ready
2026-01-05 14:46:20,877 [WARNING] No pods found for etcd-sim
2026-01-05 14:46:20,879 [WARNING] No pods found for postgres-sim
2026-01-05 14:46:20,881 [WARNING] No pods found for redis-sim
2026-01-05 14:46:20,883 [WARNING] No pods found for nginx-sim
2026-01-05 14:46:20,885 [WARNING] No pods found for auth-sim
2026-01-05 14:46:20,886 [INFO] Completed full health check
2026-01-05 14:46:30,896 [INFO] Running health check before node power on
2026-01-05 14:46:30,896 [INFO] Starting full health check
2026-01-05 14:46:30,897 [INFO] 
============ DETAILED NODE STATUS ============
2026-01-05 14:46:30,897 [INFO] Basic Node Information (kubectl get nodes -o wide):
2026-01-05 14:46:30,962 [INFO]   NAME          STATUS                     ROLES           AGE   VERSION    INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
2026-01-05 14:46:30,963 [INFO]   master-m001   Ready                      control-plane   10d   v1.32.11   192.168.56.101   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:46:30,963 [INFO]   master-m002   Ready                      control-plane   10d   v1.32.11   192.168.56.102   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:46:30,963 [INFO]   master-m003   Ready                      control-plane   10d   v1.32.11   192.168.56.103   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:46:30,963 [INFO]   worker-w001   Ready                      <none>          10d   v1.32.11   192.168.56.104   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:46:30,963 [INFO]   worker-w002   Ready,SchedulingDisabled   <none>          10d   v1.32.11   192.168.56.105   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:46:30,963 [INFO]   worker-w003   Ready                      <none>          10d   v1.32.11   192.168.56.106   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:46:30,964 [INFO]   worker-w004   Ready                      <none>          10d   v1.32.11   192.168.56.107   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:46:30,964 [INFO]   worker-w005   Ready                      <none>          10d   v1.32.11   192.168.56.108   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:46:30,964 [INFO]   worker-w006   Ready                      <none>          10d   v1.32.11   192.168.56.109   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:46:30,964 [INFO] 
Enhanced Node Status (with taint and cordon indicators):
2026-01-05 14:46:30,964 [INFO]   NAME                STATUS    ROLES           ZONE   CORDONED   TAINTS
2026-01-05 14:46:31,241 [INFO]   master-m001     Ready  ✓ worker         R1    No       node-role.kubernetes.io/control-plane 
2026-01-05 14:46:31,420 [INFO]   master-m002     Ready  ✓ worker         R2    No       None 
2026-01-05 14:46:31,596 [INFO]   master-m003     Ready  ✓ worker         R3    No       None 
2026-01-05 14:46:31,770 [INFO]   worker-w001     Ready  ✓ worker         R1    No       None 
2026-01-05 14:46:31,947 [INFO]   worker-w002     Ready  ✓ worker         R1    YES     ⚠️ simulated-failure, node.kubernetes.io/unschedulable ⚠️
2026-01-05 14:46:32,128 [INFO]   worker-w003     Ready  ✓ worker         R2    No       None 
2026-01-05 14:46:32,536 [INFO]   worker-w004     Ready  ✓ worker         R2    No       None 
2026-01-05 14:46:32,716 [INFO]   worker-w005     Ready  ✓ worker         R3    No       None 
2026-01-05 14:46:32,939 [INFO]   worker-w006     Ready  ✓ worker         R3    No       None 
2026-01-05 14:46:32,940 [INFO] 
Legend:
2026-01-05 14:46:32,940 [INFO]   ✓ = Node is Ready
2026-01-05 14:46:32,940 [INFO]   ⚠️ = Warning indicator (NotReady, Cordoned, or has simulated-failure taint)
2026-01-05 14:46:32,940 [INFO] ============ DETAILED POD INFORMATION ============
2026-01-05 14:46:32,940 [INFO] Running 'kubectl get pods -o wide' to show detailed pod placement:
2026-01-05 14:46:33,075 [INFO]   NAMESPACE     NAME                                                    READY   STATUS      RESTARTS        AGE     IP                NODE          NOMINATED NODE   READINESS GATES
2026-01-05 14:46:33,075 [INFO]   argo          argo-server-5c69cb69db-gdkl6                            1/1     Running     0               8d      192.168.221.65    master-m003   <none>           <none>
2026-01-05 14:46:33,075 [INFO]   argo          resilience-bench-ptngm-initialize-metrics-940927476     0/2     Completed   0               2d16h   192.168.221.125   master-m003   <none>           <none>
2026-01-05 14:46:33,075 [INFO]   argo          resilience-bench-ptngm-run-health-check-1168793163      0/2     Completed   0               2d16h   192.168.221.67    master-m003   <none>           <none>
2026-01-05 14:46:33,076 [INFO]   argo          resilience-bench-ptngm-run-health-check-1185570782      0/2     Completed   0               2d16h   192.168.221.66    master-m003   <none>           <none>
2026-01-05 14:46:33,076 [INFO]   argo          resilience-bench-ptngm-run-health-check-1202348401      0/2     Completed   0               2d16h   192.168.221.122   master-m003   <none>           <none>
2026-01-05 14:46:33,076 [INFO]   argo          resilience-bench-wf87f-initialize-metrics-3677018408    0/2     Completed   0               2d18h   192.168.221.100   master-m003   <none>           <none>
2026-01-05 14:46:33,076 [INFO]   argo          resilience-bench-wf87f-run-health-check-2914150102      0/2     Completed   0               2d18h   192.168.221.105   master-m003   <none>           <none>
2026-01-05 14:46:33,076 [INFO]   argo          resilience-bench-wf87f-run-health-check-3931060015      0/2     Completed   0               2d18h   192.168.221.104   master-m003   <none>           <none>
2026-01-05 14:46:33,076 [INFO]   argo          resilience-bench-wf87f-run-health-check-3947837634      0/2     Completed   0               2d18h   192.168.221.98    master-m003   <none>           <none>
2026-01-05 14:46:33,076 [INFO]   argo          resilience-bench-wf87f-run-health-check-3964615253      0/2     Completed   0               2d18h   192.168.221.103   master-m003   <none>           <none>
2026-01-05 14:46:33,076 [INFO]   argo          resilience-bench-wf87f-run-node-simulation-4173146970   0/2     Completed   0               2d18h   192.168.221.101   master-m003   <none>           <none>
2026-01-05 14:46:33,076 [INFO]   argo          resilience-bench-wf87f-run-rack-simulation-1356070019   0/2     Completed   0               2d18h   192.168.221.109   master-m003   <none>           <none>
2026-01-05 14:46:33,076 [INFO]   argo          resilience-heft-4cqbx-health-check-1555843180           0/2     Error       0               82m     192.168.132.166   worker-w001   <none>           <none>
2026-01-05 14:46:33,076 [INFO]   argo          resilience-heft-4cqbx-health-check-1589398418           0/2     Error       0               82m     192.168.15.231    worker-w005   <none>           <none>
2026-01-05 14:46:33,076 [INFO]   argo          resilience-heft-4cqbx-health-check-1606176037           0/2     Error       0               82m     192.168.240.157   worker-w003   <none>           <none>
2026-01-05 14:46:33,076 [INFO]   argo          resilience-heft-4cqbx-heft-initialize-2594215169        0/2     Completed   0               83m     192.168.221.79    master-m003   <none>           <none>
2026-01-05 14:46:33,076 [INFO]   argo          resilience-heft-76kbp-health-check-270747540            0/2     Error       0               74m     192.168.15.237    worker-w005   <none>           <none>
2026-01-05 14:46:33,076 [INFO]   argo          resilience-heft-76kbp-health-check-287525159            0/2     Error       0               74m     192.168.240.163   worker-w003   <none>           <none>
2026-01-05 14:46:33,076 [INFO]   argo          resilience-heft-76kbp-health-check-304302778            0/2     Error       0               74m     192.168.132.172   worker-w001   <none>           <none>
2026-01-05 14:46:33,076 [INFO]   argo          resilience-heft-76kbp-heft-initialize-3574960751        0/2     Completed   0               74m     192.168.221.86    master-m003   <none>           <none>
2026-01-05 14:46:33,076 [INFO]   argo          resilience-heft-b85xr-health-check-2807215411           0/2     Error       0               71m     192.168.132.175   worker-w001   <none>           <none>
2026-01-05 14:46:33,076 [INFO]   argo          resilience-heft-b85xr-health-check-2823993030           0/2     Error       0               71m     192.168.240.166   worker-w003   <none>           <none>
2026-01-05 14:46:33,076 [INFO]   argo          resilience-heft-b85xr-health-check-2840770649           0/2     Error       0               71m     192.168.15.240    worker-w005   <none>           <none>
2026-01-05 14:46:33,076 [INFO]   argo          resilience-heft-b85xr-heft-initialize-1754406316        0/2     Completed   0               71m     192.168.221.88    master-m003   <none>           <none>
2026-01-05 14:46:33,076 [INFO]   argo          resilience-heft-fdw9f-health-check-1896357264           0/2     Error       0               76m     192.168.15.235    worker-w005   <none>           <none>
2026-01-05 14:46:33,076 [INFO]   argo          resilience-heft-fdw9f-health-check-1913134883           0/2     Error       0               76m     192.168.240.161   worker-w003   <none>           <none>
2026-01-05 14:46:33,076 [INFO]   argo          resilience-heft-fdw9f-health-check-1929912502           0/2     Error       0               76m     192.168.132.170   worker-w001   <none>           <none>
2026-01-05 14:46:33,076 [INFO]   argo          resilience-heft-fdw9f-heft-initialize-2651619139        0/2     Completed   0               76m     192.168.221.77    master-m003   <none>           <none>
2026-01-05 14:46:33,076 [INFO]   argo          resilience-heft-gf256-health-check-946619120            0/2     Error       0               75m     192.168.132.171   worker-w001   <none>           <none>
2026-01-05 14:46:33,077 [INFO]   argo          resilience-heft-gf256-health-check-980174358            0/2     Error       0               75m     192.168.15.236    worker-w005   <none>           <none>
2026-01-05 14:46:33,077 [INFO]   argo          resilience-heft-gf256-health-check-996951977            0/2     Error       0               75m     192.168.240.162   worker-w003   <none>           <none>
2026-01-05 14:46:33,077 [INFO]   argo          resilience-heft-gf256-heft-initialize-1548293021        0/2     Completed   0               75m     192.168.221.84    master-m003   <none>           <none>
2026-01-05 14:46:33,077 [INFO]   argo          resilience-heft-gxwpg-health-check-3428526804           0/2     Error       0               78m     192.168.240.159   worker-w003   <none>           <none>
2026-01-05 14:46:33,077 [INFO]   argo          resilience-heft-gxwpg-health-check-3445304423           0/2     Error       0               78m     192.168.15.233    worker-w005   <none>           <none>
2026-01-05 14:46:33,077 [INFO]   argo          resilience-heft-gxwpg-health-check-3478859661           0/2     Error       0               78m     192.168.132.168   worker-w001   <none>           <none>
2026-01-05 14:46:33,077 [INFO]   argo          resilience-heft-gxwpg-heft-initialize-2377397022        0/2     Completed   0               79m     192.168.221.81    master-m003   <none>           <none>
2026-01-05 14:46:33,077 [INFO]   argo          resilience-heft-pc2kb-health-check-3258812212           0/2     Error       0               73m     192.168.15.238    worker-w005   <none>           <none>
2026-01-05 14:46:33,077 [INFO]   argo          resilience-heft-pc2kb-health-check-3275589831           0/2     Error       0               73m     192.168.240.164   worker-w003   <none>           <none>
2026-01-05 14:46:33,077 [INFO]   argo          resilience-heft-pc2kb-health-check-3292367450           0/2     Error       0               73m     192.168.132.173   worker-w001   <none>           <none>
2026-01-05 14:46:33,077 [INFO]   argo          resilience-heft-pc2kb-heft-initialize-321333135         0/2     Completed   0               73m     192.168.221.83    master-m003   <none>           <none>
2026-01-05 14:46:33,077 [INFO]   argo          resilience-heft-qsmch-health-check-283908068            0/2     Error       0               80m     192.168.15.232    worker-w005   <none>           <none>
2026-01-05 14:46:33,077 [INFO]   argo          resilience-heft-qsmch-health-check-300685687            0/2     Error       0               80m     192.168.240.158   worker-w003   <none>           <none>
2026-01-05 14:46:33,077 [INFO]   argo          resilience-heft-qsmch-health-check-317463306            0/2     Error       0               80m     192.168.132.167   worker-w001   <none>           <none>
2026-01-05 14:46:33,077 [INFO]   argo          resilience-heft-qsmch-heft-initialize-1908810015        0/2     Completed   0               80m     192.168.221.82    master-m003   <none>           <none>
2026-01-05 14:46:33,077 [INFO]   argo          resilience-heft-r2w8l-health-check-350149736            0/2     Error       0               72m     192.168.240.165   worker-w003   <none>           <none>
2026-01-05 14:46:33,077 [INFO]   argo          resilience-heft-r2w8l-health-check-366927355            0/2     Error       0               72m     192.168.15.239    worker-w005   <none>           <none>
2026-01-05 14:46:33,077 [INFO]   argo          resilience-heft-r2w8l-health-check-400482593            0/2     Error       0               72m     192.168.132.174   worker-w001   <none>           <none>
2026-01-05 14:46:33,077 [INFO]   argo          resilience-heft-r2w8l-heft-initialize-3054048250        0/2     Completed   0               72m     192.168.221.85    master-m003   <none>           <none>
2026-01-05 14:46:33,077 [INFO]   argo          resilience-heft-t988k-health-check-612195296            0/2     Error       0               77m     192.168.15.234    worker-w005   <none>           <none>
2026-01-05 14:46:33,077 [INFO]   argo          resilience-heft-t988k-health-check-628972915            0/2     Error       0               77m     192.168.240.160   worker-w003   <none>           <none>
2026-01-05 14:46:33,077 [INFO]   argo          resilience-heft-t988k-health-check-645750534            0/2     Error       0               77m     192.168.132.169   worker-w001   <none>           <none>
2026-01-05 14:46:33,078 [INFO]   argo          resilience-heft-t988k-heft-initialize-164752883         0/2     Completed   0               78m     192.168.221.80    master-m003   <none>           <none>
2026-01-05 14:46:33,078 [INFO]   argo          resilience-heft-vlt2m-health-check-2709447919           0/2     Error       0               34m     192.168.221.119   master-m003   <none>           <none>
2026-01-05 14:46:33,078 [INFO]   argo          resilience-heft-vlt2m-health-check-2726225538           0/2     Error       0               34m     192.168.221.120   master-m003   <none>           <none>
2026-01-05 14:46:33,078 [INFO]   argo          resilience-heft-vlt2m-health-check-2743003157           0/2     Error       0               34m     192.168.221.121   master-m003   <none>           <none>
2026-01-05 14:46:33,078 [INFO]   argo          resilience-heft-vlt2m-heft-initialize-840404112         0/2     Completed   0               34m     192.168.221.118   master-m003   <none>           <none>
2026-01-05 14:46:33,078 [INFO]   argo          resilience-heft-x86wd-health-check-2184182048           0/2     Error       0               32m     192.168.221.123   master-m003   <none>           <none>
2026-01-05 14:46:33,078 [INFO]   argo          resilience-heft-x86wd-health-check-2200959667           0/2     Error       0               32m     192.168.221.125   master-m003   <none>           <none>
2026-01-05 14:46:33,078 [INFO]   argo          resilience-heft-x86wd-health-check-2234514905           0/2     Error       0               32m     192.168.221.124   master-m003   <none>           <none>
2026-01-05 14:46:33,078 [INFO]   argo          resilience-heft-x86wd-heft-initialize-2152040770        0/2     Completed   0               32m     192.168.221.127   master-m003   <none>           <none>
2026-01-05 14:46:33,078 [INFO]   argo          workflow-controller-ccbd949dc-pzf68                     1/1     Running     0               74s     192.168.153.241   worker-w004   <none>           <none>
2026-01-05 14:46:33,078 [INFO]   default       resilience-heft-1-201507-qflcd                          1/1     Running     0               84s     192.168.221.71    master-m003   <none>           <none>
2026-01-05 14:46:33,078 [INFO]   kube-system   calico-kube-controllers-7498b9bb4c-xd7tb                1/1     Running     0               38h     192.168.191.108   worker-w006   <none>           <none>
2026-01-05 14:46:33,078 [INFO]   kube-system   calico-node-4zhd4                                       1/1     Running     0               10d     192.168.56.105    worker-w002   <none>           <none>
2026-01-05 14:46:33,078 [INFO]   kube-system   calico-node-75nx6                                       1/1     Running     0               10d     192.168.56.109    worker-w006   <none>           <none>
2026-01-05 14:46:33,078 [INFO]   kube-system   calico-node-7lkdq                                       1/1     Running     0               10d     192.168.56.104    worker-w001   <none>           <none>
2026-01-05 14:46:33,078 [INFO]   kube-system   calico-node-85f8c                                       1/1     Running     0               10d     192.168.56.102    master-m002   <none>           <none>
2026-01-05 14:46:33,078 [INFO]   kube-system   calico-node-j8nb9                                       1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:46:33,078 [INFO]   kube-system   calico-node-lbcb2                                       1/1     Running     0               10d     192.168.56.106    worker-w003   <none>           <none>
2026-01-05 14:46:33,078 [INFO]   kube-system   calico-node-wlr5v                                       1/1     Running     0               10d     192.168.56.107    worker-w004   <none>           <none>
2026-01-05 14:46:33,078 [INFO]   kube-system   calico-node-xnzjw                                       1/1     Running     0               10d     192.168.56.103    master-m003   <none>           <none>
2026-01-05 14:46:33,078 [INFO]   kube-system   calico-node-xsltn                                       1/1     Running     0               10d     192.168.56.108    worker-w005   <none>           <none>
2026-01-05 14:46:33,078 [INFO]   kube-system   coredns-668d6bf9bc-2f74f                                1/1     Running     0               39h     192.168.15.230    worker-w005   <none>           <none>
2026-01-05 14:46:33,078 [INFO]   kube-system   coredns-668d6bf9bc-ssl9h                                1/1     Running     0               39h     192.168.132.165   worker-w001   <none>           <none>
2026-01-05 14:46:33,079 [INFO]   kube-system   etcd-master-m001                                        1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:46:33,079 [INFO]   kube-system   kube-apiserver-master-m001                              1/1     Running     1 (2d11h ago)   10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:46:33,079 [INFO]   kube-system   kube-controller-manager-master-m001                     1/1     Running     12 (30m ago)    10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:46:33,079 [INFO]   kube-system   kube-proxy-5lzhj                                        1/1     Running     0               10d     192.168.56.102    master-m002   <none>           <none>
2026-01-05 14:46:33,079 [INFO]   kube-system   kube-proxy-7jqkv                                        1/1     Running     0               10d     192.168.56.109    worker-w006   <none>           <none>
2026-01-05 14:46:33,079 [INFO]   kube-system   kube-proxy-cv5dt                                        1/1     Running     0               10d     192.168.56.108    worker-w005   <none>           <none>
2026-01-05 14:46:33,079 [INFO]   kube-system   kube-proxy-fvpmr                                        1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:46:33,079 [INFO]   kube-system   kube-proxy-hgs5z                                        1/1     Running     0               10d     192.168.56.106    worker-w003   <none>           <none>
2026-01-05 14:46:33,079 [INFO]   kube-system   kube-proxy-kmgqr                                        1/1     Running     0               10d     192.168.56.105    worker-w002   <none>           <none>
2026-01-05 14:46:33,079 [INFO]   kube-system   kube-proxy-rdbz5                                        1/1     Running     0               10d     192.168.56.107    worker-w004   <none>           <none>
2026-01-05 14:46:33,079 [INFO]   kube-system   kube-proxy-w8mnb                                        1/1     Running     0               10d     192.168.56.103    master-m003   <none>           <none>
2026-01-05 14:46:33,079 [INFO]   kube-system   kube-proxy-x9jxr                                        1/1     Running     0               10d     192.168.56.104    worker-w001   <none>           <none>
2026-01-05 14:46:33,079 [INFO]   kube-system   kube-scheduler-master-m001                              1/1     Running     11 (30m ago)    10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:46:33,079 [INFO] 
Pod distribution by node:
2026-01-05 14:46:33,213 [INFO]   Node master-m003: 33 pods
2026-01-05 14:46:33,214 [INFO]   Node worker-w001: 13 pods
2026-01-05 14:46:33,214 [INFO]   Node worker-w005: 13 pods
2026-01-05 14:46:33,214 [INFO]   Node worker-w003: 12 pods
2026-01-05 14:46:33,214 [INFO]   Node worker-w004: 3 pods
2026-01-05 14:46:33,214 [INFO]   Node worker-w006: 3 pods
2026-01-05 14:46:33,214 [INFO]   Node worker-w002: 2 pods
2026-01-05 14:46:33,214 [INFO]   Node master-m002: 2 pods
2026-01-05 14:46:33,214 [INFO]   Node master-m001: 3 pods
2026-01-05 14:46:33,214 [INFO]   Node 10d: 3 pods
2026-01-05 14:46:33,214 [INFO] 
Filtering for simulation services:
2026-01-05 14:46:33,301 [INFO] Node master-m001 is Ready
2026-01-05 14:46:33,301 [INFO] Node master-m002 is Ready
2026-01-05 14:46:33,301 [INFO] Node master-m003 is Ready
2026-01-05 14:46:33,301 [INFO] Node worker-w001 is Ready
2026-01-05 14:46:33,301 [INFO] Node worker-w002 is Ready
2026-01-05 14:46:33,301 [INFO] Node worker-w003 is Ready
2026-01-05 14:46:33,301 [INFO] Node worker-w004 is Ready
2026-01-05 14:46:33,302 [INFO] Node worker-w005 is Ready
2026-01-05 14:46:33,302 [INFO] Node worker-w006 is Ready
2026-01-05 14:46:33,305 [WARNING] No pods found for etcd-sim
2026-01-05 14:46:33,307 [WARNING] No pods found for postgres-sim
2026-01-05 14:46:33,309 [WARNING] No pods found for redis-sim
2026-01-05 14:46:33,311 [WARNING] No pods found for nginx-sim
2026-01-05 14:46:33,313 [WARNING] No pods found for auth-sim
2026-01-05 14:46:33,313 [INFO] Completed full health check
2026-01-05 14:46:33,313 [INFO] Simulating node recovery for worker-w002 using Kubernetes API
2026-01-05 14:46:33,328 [INFO] Removed simulated-failure taint from node worker-w002
2026-01-05 14:46:33,337 [INFO] Node worker-w002 uncordoned and ready
2026-01-05 14:46:33,337 [INFO] Node worker-w002 has been powered back on
2026-01-05 14:46:33,337 [INFO] Waiting 60 seconds for the cluster to stabilize after recovery...
2026-01-05 14:47:33,340 [INFO] Running final health check
2026-01-05 14:47:33,340 [INFO] Starting full health check
2026-01-05 14:47:33,340 [INFO] 
============ DETAILED NODE STATUS ============
2026-01-05 14:47:33,340 [INFO] Basic Node Information (kubectl get nodes -o wide):
2026-01-05 14:47:33,409 [INFO]   NAME          STATUS   ROLES           AGE   VERSION    INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
2026-01-05 14:47:33,409 [INFO]   master-m001   Ready    control-plane   10d   v1.32.11   192.168.56.101   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:47:33,409 [INFO]   master-m002   Ready    control-plane   10d   v1.32.11   192.168.56.102   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:47:33,409 [INFO]   master-m003   Ready    control-plane   10d   v1.32.11   192.168.56.103   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:47:33,410 [INFO]   worker-w001   Ready    <none>          10d   v1.32.11   192.168.56.104   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:47:33,410 [INFO]   worker-w002   Ready    <none>          10d   v1.32.11   192.168.56.105   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:47:33,410 [INFO]   worker-w003   Ready    <none>          10d   v1.32.11   192.168.56.106   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:47:33,410 [INFO]   worker-w004   Ready    <none>          10d   v1.32.11   192.168.56.107   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:47:33,410 [INFO]   worker-w005   Ready    <none>          10d   v1.32.11   192.168.56.108   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:47:33,410 [INFO]   worker-w006   Ready    <none>          10d   v1.32.11   192.168.56.109   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:47:33,410 [INFO] 
Enhanced Node Status (with taint and cordon indicators):
2026-01-05 14:47:33,410 [INFO]   NAME                STATUS    ROLES           ZONE   CORDONED   TAINTS
2026-01-05 14:47:33,656 [INFO]   master-m001     Ready  ✓ worker         R1    No       node-role.kubernetes.io/control-plane 
2026-01-05 14:47:33,835 [INFO]   master-m002     Ready  ✓ worker         R2    No       None 
2026-01-05 14:47:34,008 [INFO]   master-m003     Ready  ✓ worker         R3    No       None 
2026-01-05 14:47:34,185 [INFO]   worker-w001     Ready  ✓ worker         R1    No       None 
2026-01-05 14:47:34,402 [INFO]   worker-w002     Ready  ✓ worker         R1    No       None 
2026-01-05 14:47:34,584 [INFO]   worker-w003     Ready  ✓ worker         R2    No       None 
2026-01-05 14:47:34,760 [INFO]   worker-w004     Ready  ✓ worker         R2    No       None 
2026-01-05 14:47:34,945 [INFO]   worker-w005     Ready  ✓ worker         R3    No       None 
2026-01-05 14:47:35,122 [INFO]   worker-w006     Ready  ✓ worker         R3    No       None 
2026-01-05 14:47:35,123 [INFO] 
Legend:
2026-01-05 14:47:35,123 [INFO]   ✓ = Node is Ready
2026-01-05 14:47:35,123 [INFO]   ⚠️ = Warning indicator (NotReady, Cordoned, or has simulated-failure taint)
2026-01-05 14:47:35,123 [INFO] ============ DETAILED POD INFORMATION ============
2026-01-05 14:47:35,123 [INFO] Running 'kubectl get pods -o wide' to show detailed pod placement:
2026-01-05 14:47:35,244 [INFO]   NAMESPACE     NAME                                                    READY   STATUS      RESTARTS        AGE     IP                NODE          NOMINATED NODE   READINESS GATES
2026-01-05 14:47:35,244 [INFO]   argo          argo-server-5c69cb69db-gdkl6                            1/1     Running     0               8d      192.168.221.65    master-m003   <none>           <none>
2026-01-05 14:47:35,244 [INFO]   argo          resilience-bench-ptngm-initialize-metrics-940927476     0/2     Completed   0               2d16h   192.168.221.125   master-m003   <none>           <none>
2026-01-05 14:47:35,244 [INFO]   argo          resilience-bench-ptngm-run-health-check-1168793163      0/2     Completed   0               2d16h   192.168.221.67    master-m003   <none>           <none>
2026-01-05 14:47:35,244 [INFO]   argo          resilience-bench-ptngm-run-health-check-1185570782      0/2     Completed   0               2d16h   192.168.221.66    master-m003   <none>           <none>
2026-01-05 14:47:35,244 [INFO]   argo          resilience-bench-ptngm-run-health-check-1202348401      0/2     Completed   0               2d16h   192.168.221.122   master-m003   <none>           <none>
2026-01-05 14:47:35,244 [INFO]   argo          resilience-bench-wf87f-initialize-metrics-3677018408    0/2     Completed   0               2d18h   192.168.221.100   master-m003   <none>           <none>
2026-01-05 14:47:35,244 [INFO]   argo          resilience-bench-wf87f-run-health-check-2914150102      0/2     Completed   0               2d18h   192.168.221.105   master-m003   <none>           <none>
2026-01-05 14:47:35,244 [INFO]   argo          resilience-bench-wf87f-run-health-check-3931060015      0/2     Completed   0               2d18h   192.168.221.104   master-m003   <none>           <none>
2026-01-05 14:47:35,245 [INFO]   argo          resilience-bench-wf87f-run-health-check-3947837634      0/2     Completed   0               2d18h   192.168.221.98    master-m003   <none>           <none>
2026-01-05 14:47:35,245 [INFO]   argo          resilience-bench-wf87f-run-health-check-3964615253      0/2     Completed   0               2d18h   192.168.221.103   master-m003   <none>           <none>
2026-01-05 14:47:35,245 [INFO]   argo          resilience-bench-wf87f-run-node-simulation-4173146970   0/2     Completed   0               2d18h   192.168.221.101   master-m003   <none>           <none>
2026-01-05 14:47:35,245 [INFO]   argo          resilience-bench-wf87f-run-rack-simulation-1356070019   0/2     Completed   0               2d18h   192.168.221.109   master-m003   <none>           <none>
2026-01-05 14:47:35,245 [INFO]   argo          resilience-heft-4cqbx-health-check-1555843180           0/2     Error       0               83m     192.168.132.166   worker-w001   <none>           <none>
2026-01-05 14:47:35,245 [INFO]   argo          resilience-heft-4cqbx-health-check-1589398418           0/2     Error       0               83m     192.168.15.231    worker-w005   <none>           <none>
2026-01-05 14:47:35,245 [INFO]   argo          resilience-heft-4cqbx-health-check-1606176037           0/2     Error       0               83m     192.168.240.157   worker-w003   <none>           <none>
2026-01-05 14:47:35,245 [INFO]   argo          resilience-heft-4cqbx-heft-initialize-2594215169        0/2     Completed   0               84m     192.168.221.79    master-m003   <none>           <none>
2026-01-05 14:47:35,245 [INFO]   argo          resilience-heft-76kbp-health-check-270747540            0/2     Error       0               75m     192.168.15.237    worker-w005   <none>           <none>
2026-01-05 14:47:35,245 [INFO]   argo          resilience-heft-76kbp-health-check-287525159            0/2     Error       0               75m     192.168.240.163   worker-w003   <none>           <none>
2026-01-05 14:47:35,245 [INFO]   argo          resilience-heft-76kbp-health-check-304302778            0/2     Error       0               75m     192.168.132.172   worker-w001   <none>           <none>
2026-01-05 14:47:35,245 [INFO]   argo          resilience-heft-76kbp-heft-initialize-3574960751        0/2     Completed   0               75m     192.168.221.86    master-m003   <none>           <none>
2026-01-05 14:47:35,245 [INFO]   argo          resilience-heft-b85xr-health-check-2807215411           0/2     Error       0               72m     192.168.132.175   worker-w001   <none>           <none>
2026-01-05 14:47:35,245 [INFO]   argo          resilience-heft-b85xr-health-check-2823993030           0/2     Error       0               72m     192.168.240.166   worker-w003   <none>           <none>
2026-01-05 14:47:35,245 [INFO]   argo          resilience-heft-b85xr-health-check-2840770649           0/2     Error       0               72m     192.168.15.240    worker-w005   <none>           <none>
2026-01-05 14:47:35,245 [INFO]   argo          resilience-heft-b85xr-heft-initialize-1754406316        0/2     Completed   0               72m     192.168.221.88    master-m003   <none>           <none>
2026-01-05 14:47:35,245 [INFO]   argo          resilience-heft-fdw9f-health-check-1896357264           0/2     Error       0               77m     192.168.15.235    worker-w005   <none>           <none>
2026-01-05 14:47:35,245 [INFO]   argo          resilience-heft-fdw9f-health-check-1913134883           0/2     Error       0               77m     192.168.240.161   worker-w003   <none>           <none>
2026-01-05 14:47:35,245 [INFO]   argo          resilience-heft-fdw9f-health-check-1929912502           0/2     Error       0               77m     192.168.132.170   worker-w001   <none>           <none>
2026-01-05 14:47:35,245 [INFO]   argo          resilience-heft-fdw9f-heft-initialize-2651619139        0/2     Completed   0               78m     192.168.221.77    master-m003   <none>           <none>
2026-01-05 14:47:35,245 [INFO]   argo          resilience-heft-gf256-health-check-946619120            0/2     Error       0               76m     192.168.132.171   worker-w001   <none>           <none>
2026-01-05 14:47:35,245 [INFO]   argo          resilience-heft-gf256-health-check-980174358            0/2     Error       0               76m     192.168.15.236    worker-w005   <none>           <none>
2026-01-05 14:47:35,246 [INFO]   argo          resilience-heft-gf256-health-check-996951977            0/2     Error       0               76m     192.168.240.162   worker-w003   <none>           <none>
2026-01-05 14:47:35,246 [INFO]   argo          resilience-heft-gf256-heft-initialize-1548293021        0/2     Completed   0               76m     192.168.221.84    master-m003   <none>           <none>
2026-01-05 14:47:35,246 [INFO]   argo          resilience-heft-gxwpg-health-check-3428526804           0/2     Error       0               79m     192.168.240.159   worker-w003   <none>           <none>
2026-01-05 14:47:35,246 [INFO]   argo          resilience-heft-gxwpg-health-check-3445304423           0/2     Error       0               79m     192.168.15.233    worker-w005   <none>           <none>
2026-01-05 14:47:35,246 [INFO]   argo          resilience-heft-gxwpg-health-check-3478859661           0/2     Error       0               79m     192.168.132.168   worker-w001   <none>           <none>
2026-01-05 14:47:35,246 [INFO]   argo          resilience-heft-gxwpg-heft-initialize-2377397022        0/2     Completed   0               80m     192.168.221.81    master-m003   <none>           <none>
2026-01-05 14:47:35,246 [INFO]   argo          resilience-heft-pc2kb-health-check-3258812212           0/2     Error       0               74m     192.168.15.238    worker-w005   <none>           <none>
2026-01-05 14:47:35,246 [INFO]   argo          resilience-heft-pc2kb-health-check-3275589831           0/2     Error       0               74m     192.168.240.164   worker-w003   <none>           <none>
2026-01-05 14:47:35,246 [INFO]   argo          resilience-heft-pc2kb-health-check-3292367450           0/2     Error       0               74m     192.168.132.173   worker-w001   <none>           <none>
2026-01-05 14:47:35,246 [INFO]   argo          resilience-heft-pc2kb-heft-initialize-321333135         0/2     Completed   0               74m     192.168.221.83    master-m003   <none>           <none>
2026-01-05 14:47:35,246 [INFO]   argo          resilience-heft-qsmch-health-check-283908068            0/2     Error       0               81m     192.168.15.232    worker-w005   <none>           <none>
2026-01-05 14:47:35,246 [INFO]   argo          resilience-heft-qsmch-health-check-300685687            0/2     Error       0               81m     192.168.240.158   worker-w003   <none>           <none>
2026-01-05 14:47:35,246 [INFO]   argo          resilience-heft-qsmch-health-check-317463306            0/2     Error       0               81m     192.168.132.167   worker-w001   <none>           <none>
2026-01-05 14:47:35,246 [INFO]   argo          resilience-heft-qsmch-heft-initialize-1908810015        0/2     Completed   0               81m     192.168.221.82    master-m003   <none>           <none>
2026-01-05 14:47:35,246 [INFO]   argo          resilience-heft-r2w8l-health-check-350149736            0/2     Error       0               73m     192.168.240.165   worker-w003   <none>           <none>
2026-01-05 14:47:35,246 [INFO]   argo          resilience-heft-r2w8l-health-check-366927355            0/2     Error       0               73m     192.168.15.239    worker-w005   <none>           <none>
2026-01-05 14:47:35,247 [INFO]   argo          resilience-heft-r2w8l-health-check-400482593            0/2     Error       0               73m     192.168.132.174   worker-w001   <none>           <none>
2026-01-05 14:47:35,247 [INFO]   argo          resilience-heft-r2w8l-heft-initialize-3054048250        0/2     Completed   0               73m     192.168.221.85    master-m003   <none>           <none>
2026-01-05 14:47:35,247 [INFO]   argo          resilience-heft-t988k-health-check-612195296            0/2     Error       0               78m     192.168.15.234    worker-w005   <none>           <none>
2026-01-05 14:47:35,247 [INFO]   argo          resilience-heft-t988k-health-check-628972915            0/2     Error       0               78m     192.168.240.160   worker-w003   <none>           <none>
2026-01-05 14:47:35,247 [INFO]   argo          resilience-heft-t988k-health-check-645750534            0/2     Error       0               78m     192.168.132.169   worker-w001   <none>           <none>
2026-01-05 14:47:35,247 [INFO]   argo          resilience-heft-t988k-heft-initialize-164752883         0/2     Completed   0               79m     192.168.221.80    master-m003   <none>           <none>
2026-01-05 14:47:35,247 [INFO]   argo          resilience-heft-vlt2m-health-check-2709447919           0/2     Error       0               35m     192.168.221.119   master-m003   <none>           <none>
2026-01-05 14:47:35,247 [INFO]   argo          resilience-heft-vlt2m-health-check-2726225538           0/2     Error       0               35m     192.168.221.120   master-m003   <none>           <none>
2026-01-05 14:47:35,247 [INFO]   argo          resilience-heft-vlt2m-health-check-2743003157           0/2     Error       0               35m     192.168.221.121   master-m003   <none>           <none>
2026-01-05 14:47:35,247 [INFO]   argo          resilience-heft-vlt2m-heft-initialize-840404112         0/2     Completed   0               35m     192.168.221.118   master-m003   <none>           <none>
2026-01-05 14:47:35,247 [INFO]   argo          resilience-heft-x86wd-health-check-2184182048           0/2     Error       0               33m     192.168.221.123   master-m003   <none>           <none>
2026-01-05 14:47:35,247 [INFO]   argo          resilience-heft-x86wd-health-check-2200959667           0/2     Error       0               33m     192.168.221.125   master-m003   <none>           <none>
2026-01-05 14:47:35,247 [INFO]   argo          resilience-heft-x86wd-health-check-2234514905           0/2     Error       0               33m     192.168.221.124   master-m003   <none>           <none>
2026-01-05 14:47:35,247 [INFO]   argo          resilience-heft-x86wd-heft-initialize-2152040770        0/2     Completed   0               33m     192.168.221.127   master-m003   <none>           <none>
2026-01-05 14:47:35,247 [INFO]   argo          workflow-controller-ccbd949dc-pzf68                     1/1     Running     0               2m17s   192.168.153.241   worker-w004   <none>           <none>
2026-01-05 14:47:35,247 [INFO]   default       resilience-heft-1-201507-qflcd                          1/1     Running     0               2m27s   192.168.221.71    master-m003   <none>           <none>
2026-01-05 14:47:35,247 [INFO]   kube-system   calico-kube-controllers-7498b9bb4c-xd7tb                1/1     Running     0               38h     192.168.191.108   worker-w006   <none>           <none>
2026-01-05 14:47:35,247 [INFO]   kube-system   calico-node-4zhd4                                       1/1     Running     0               10d     192.168.56.105    worker-w002   <none>           <none>
2026-01-05 14:47:35,247 [INFO]   kube-system   calico-node-75nx6                                       1/1     Running     0               10d     192.168.56.109    worker-w006   <none>           <none>
2026-01-05 14:47:35,247 [INFO]   kube-system   calico-node-7lkdq                                       1/1     Running     0               10d     192.168.56.104    worker-w001   <none>           <none>
2026-01-05 14:47:35,247 [INFO]   kube-system   calico-node-85f8c                                       1/1     Running     0               10d     192.168.56.102    master-m002   <none>           <none>
2026-01-05 14:47:35,248 [INFO]   kube-system   calico-node-j8nb9                                       1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:47:35,248 [INFO]   kube-system   calico-node-lbcb2                                       1/1     Running     0               10d     192.168.56.106    worker-w003   <none>           <none>
2026-01-05 14:47:35,248 [INFO]   kube-system   calico-node-wlr5v                                       1/1     Running     0               10d     192.168.56.107    worker-w004   <none>           <none>
2026-01-05 14:47:35,248 [INFO]   kube-system   calico-node-xnzjw                                       1/1     Running     0               10d     192.168.56.103    master-m003   <none>           <none>
2026-01-05 14:47:35,248 [INFO]   kube-system   calico-node-xsltn                                       1/1     Running     0               10d     192.168.56.108    worker-w005   <none>           <none>
2026-01-05 14:47:35,248 [INFO]   kube-system   coredns-668d6bf9bc-2f74f                                1/1     Running     0               39h     192.168.15.230    worker-w005   <none>           <none>
2026-01-05 14:47:35,248 [INFO]   kube-system   coredns-668d6bf9bc-ssl9h                                1/1     Running     0               39h     192.168.132.165   worker-w001   <none>           <none>
2026-01-05 14:47:35,248 [INFO]   kube-system   etcd-master-m001                                        1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:47:35,248 [INFO]   kube-system   kube-apiserver-master-m001                              1/1     Running     1 (2d11h ago)   10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:47:35,248 [INFO]   kube-system   kube-controller-manager-master-m001                     1/1     Running     12 (31m ago)    10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:47:35,248 [INFO]   kube-system   kube-proxy-5lzhj                                        1/1     Running     0               10d     192.168.56.102    master-m002   <none>           <none>
2026-01-05 14:47:35,248 [INFO]   kube-system   kube-proxy-7jqkv                                        1/1     Running     0               10d     192.168.56.109    worker-w006   <none>           <none>
2026-01-05 14:47:35,248 [INFO]   kube-system   kube-proxy-cv5dt                                        1/1     Running     0               10d     192.168.56.108    worker-w005   <none>           <none>
2026-01-05 14:47:35,248 [INFO]   kube-system   kube-proxy-fvpmr                                        1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:47:35,248 [INFO]   kube-system   kube-proxy-hgs5z                                        1/1     Running     0               10d     192.168.56.106    worker-w003   <none>           <none>
2026-01-05 14:47:35,248 [INFO]   kube-system   kube-proxy-kmgqr                                        1/1     Running     0               10d     192.168.56.105    worker-w002   <none>           <none>
2026-01-05 14:47:35,248 [INFO]   kube-system   kube-proxy-rdbz5                                        1/1     Running     0               10d     192.168.56.107    worker-w004   <none>           <none>
2026-01-05 14:47:35,248 [INFO]   kube-system   kube-proxy-w8mnb                                        1/1     Running     0               10d     192.168.56.103    master-m003   <none>           <none>
2026-01-05 14:47:35,248 [INFO]   kube-system   kube-proxy-x9jxr                                        1/1     Running     0               10d     192.168.56.104    worker-w001   <none>           <none>
2026-01-05 14:47:35,249 [INFO]   kube-system   kube-scheduler-master-m001                              1/1     Running     11 (31m ago)    10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:47:35,249 [INFO] 
Pod distribution by node:
2026-01-05 14:47:35,360 [INFO]   Node master-m003: 33 pods
2026-01-05 14:47:35,361 [INFO]   Node worker-w001: 13 pods
2026-01-05 14:47:35,361 [INFO]   Node worker-w005: 13 pods
2026-01-05 14:47:35,361 [INFO]   Node worker-w003: 12 pods
2026-01-05 14:47:35,361 [INFO]   Node worker-w004: 3 pods
2026-01-05 14:47:35,361 [INFO]   Node worker-w006: 3 pods
2026-01-05 14:47:35,361 [INFO]   Node worker-w002: 2 pods
2026-01-05 14:47:35,361 [INFO]   Node master-m002: 2 pods
2026-01-05 14:47:35,361 [INFO]   Node master-m001: 3 pods
2026-01-05 14:47:35,361 [INFO]   Node 10d: 3 pods
2026-01-05 14:47:35,361 [INFO] 
Filtering for simulation services:
2026-01-05 14:47:35,432 [INFO] Node master-m001 is Ready
2026-01-05 14:47:35,432 [INFO] Node master-m002 is Ready
2026-01-05 14:47:35,432 [INFO] Node master-m003 is Ready
2026-01-05 14:47:35,432 [INFO] Node worker-w001 is Ready
2026-01-05 14:47:35,432 [INFO] Node worker-w002 is Ready
2026-01-05 14:47:35,432 [INFO] Node worker-w003 is Ready
2026-01-05 14:47:35,433 [INFO] Node worker-w004 is Ready
2026-01-05 14:47:35,433 [INFO] Node worker-w005 is Ready
2026-01-05 14:47:35,433 [INFO] Node worker-w006 is Ready
2026-01-05 14:47:35,436 [WARNING] No pods found for etcd-sim
2026-01-05 14:47:35,437 [WARNING] No pods found for postgres-sim
2026-01-05 14:47:35,439 [WARNING] No pods found for redis-sim
2026-01-05 14:47:35,441 [WARNING] No pods found for nginx-sim
2026-01-05 14:47:35,443 [WARNING] No pods found for auth-sim
2026-01-05 14:47:35,443 [INFO] Completed full health check
[2026-01-05 14:47:35] TIMING: NODE_SIMULATION completed in 138 seconds

================================================================
 STEP 3: INTERIM HEALTH CHECK
 Time: Mon Jan  5 14:47:35 UTC 2026
================================================================
Log directory created/verified: /app/logs
File logging configured successfully
2026-01-05 14:47:35,701 [INFO] Loaded in-cluster Kubernetes config
2026-01-05 14:47:35,701 [INFO] Running on host: resilience-heft-1-201507-qflcd
2026-01-05 14:47:35,701 [INFO] Detected current node: master-m003, zone: R3
2026-01-05 14:47:35,701 [INFO] Checking if we have permissions to modify nodes...
2026-01-05 14:47:35,733 [INFO] Testing permissions using node: master-m001
2026-01-05 14:47:35,742 [INFO] Permission check successful - we can modify nodes
2026-01-05 14:47:35,742 [INFO] Using real Kubernetes API for node control
2026-01-05 14:47:35,743 [INFO] Action received: health-check
2026-01-05 14:47:35,743 [INFO] Stabilization time: 10 seconds
2026-01-05 14:47:35,743 [INFO] Starting full health check
2026-01-05 14:47:35,743 [INFO] 
============ DETAILED NODE STATUS ============
2026-01-05 14:47:35,743 [INFO] Basic Node Information (kubectl get nodes -o wide):
2026-01-05 14:47:35,807 [INFO]   NAME          STATUS   ROLES           AGE   VERSION    INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
2026-01-05 14:47:35,807 [INFO]   master-m001   Ready    control-plane   10d   v1.32.11   192.168.56.101   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:47:35,807 [INFO]   master-m002   Ready    control-plane   10d   v1.32.11   192.168.56.102   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:47:35,807 [INFO]   master-m003   Ready    control-plane   10d   v1.32.11   192.168.56.103   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:47:35,807 [INFO]   worker-w001   Ready    <none>          10d   v1.32.11   192.168.56.104   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:47:35,807 [INFO]   worker-w002   Ready    <none>          10d   v1.32.11   192.168.56.105   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:47:35,807 [INFO]   worker-w003   Ready    <none>          10d   v1.32.11   192.168.56.106   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:47:35,807 [INFO]   worker-w004   Ready    <none>          10d   v1.32.11   192.168.56.107   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:47:35,808 [INFO]   worker-w005   Ready    <none>          10d   v1.32.11   192.168.56.108   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:47:35,808 [INFO]   worker-w006   Ready    <none>          10d   v1.32.11   192.168.56.109   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:47:35,808 [INFO] 
Enhanced Node Status (with taint and cordon indicators):
2026-01-05 14:47:35,808 [INFO]   NAME                STATUS    ROLES           ZONE   CORDONED   TAINTS
2026-01-05 14:47:36,055 [INFO]   master-m001     Ready  ✓ worker         R1    No       node-role.kubernetes.io/control-plane 
2026-01-05 14:47:36,236 [INFO]   master-m002     Ready  ✓ worker         R2    No       None 
2026-01-05 14:47:36,417 [INFO]   master-m003     Ready  ✓ worker         R3    No       None 
2026-01-05 14:47:36,629 [INFO]   worker-w001     Ready  ✓ worker         R1    No       None 
2026-01-05 14:47:36,923 [INFO]   worker-w002     Ready  ✓ worker         R1    No       None 
2026-01-05 14:47:37,110 [INFO]   worker-w003     Ready  ✓ worker         R2    No       None 
2026-01-05 14:47:37,291 [INFO]   worker-w004     Ready  ✓ worker         R2    No       None 
2026-01-05 14:47:37,479 [INFO]   worker-w005     Ready  ✓ worker         R3    No       None 
2026-01-05 14:47:37,803 [INFO]   worker-w006     Ready  ✓ worker         R3    No       None 
2026-01-05 14:47:37,803 [INFO] 
Legend:
2026-01-05 14:47:37,803 [INFO]   ✓ = Node is Ready
2026-01-05 14:47:37,804 [INFO]   ⚠️ = Warning indicator (NotReady, Cordoned, or has simulated-failure taint)
2026-01-05 14:47:37,804 [INFO] ============ DETAILED POD INFORMATION ============
2026-01-05 14:47:37,804 [INFO] Running 'kubectl get pods -o wide' to show detailed pod placement:
2026-01-05 14:47:37,922 [INFO]   NAMESPACE     NAME                                                    READY   STATUS      RESTARTS        AGE     IP                NODE          NOMINATED NODE   READINESS GATES
2026-01-05 14:47:37,922 [INFO]   argo          argo-server-5c69cb69db-gdkl6                            1/1     Running     0               8d      192.168.221.65    master-m003   <none>           <none>
2026-01-05 14:47:37,922 [INFO]   argo          resilience-bench-ptngm-initialize-metrics-940927476     0/2     Completed   0               2d16h   192.168.221.125   master-m003   <none>           <none>
2026-01-05 14:47:37,922 [INFO]   argo          resilience-bench-ptngm-run-health-check-1168793163      0/2     Completed   0               2d16h   192.168.221.67    master-m003   <none>           <none>
2026-01-05 14:47:37,922 [INFO]   argo          resilience-bench-ptngm-run-health-check-1185570782      0/2     Completed   0               2d16h   192.168.221.66    master-m003   <none>           <none>
2026-01-05 14:47:37,922 [INFO]   argo          resilience-bench-ptngm-run-health-check-1202348401      0/2     Completed   0               2d16h   192.168.221.122   master-m003   <none>           <none>
2026-01-05 14:47:37,923 [INFO]   argo          resilience-bench-wf87f-initialize-metrics-3677018408    0/2     Completed   0               2d18h   192.168.221.100   master-m003   <none>           <none>
2026-01-05 14:47:37,923 [INFO]   argo          resilience-bench-wf87f-run-health-check-2914150102      0/2     Completed   0               2d18h   192.168.221.105   master-m003   <none>           <none>
2026-01-05 14:47:37,923 [INFO]   argo          resilience-bench-wf87f-run-health-check-3931060015      0/2     Completed   0               2d18h   192.168.221.104   master-m003   <none>           <none>
2026-01-05 14:47:37,923 [INFO]   argo          resilience-bench-wf87f-run-health-check-3947837634      0/2     Completed   0               2d18h   192.168.221.98    master-m003   <none>           <none>
2026-01-05 14:47:37,923 [INFO]   argo          resilience-bench-wf87f-run-health-check-3964615253      0/2     Completed   0               2d18h   192.168.221.103   master-m003   <none>           <none>
2026-01-05 14:47:37,923 [INFO]   argo          resilience-bench-wf87f-run-node-simulation-4173146970   0/2     Completed   0               2d18h   192.168.221.101   master-m003   <none>           <none>
2026-01-05 14:47:37,923 [INFO]   argo          resilience-bench-wf87f-run-rack-simulation-1356070019   0/2     Completed   0               2d18h   192.168.221.109   master-m003   <none>           <none>
2026-01-05 14:47:37,923 [INFO]   argo          resilience-heft-4cqbx-health-check-1555843180           0/2     Error       0               84m     192.168.132.166   worker-w001   <none>           <none>
2026-01-05 14:47:37,924 [INFO]   argo          resilience-heft-4cqbx-health-check-1589398418           0/2     Error       0               84m     192.168.15.231    worker-w005   <none>           <none>
2026-01-05 14:47:37,924 [INFO]   argo          resilience-heft-4cqbx-health-check-1606176037           0/2     Error       0               84m     192.168.240.157   worker-w003   <none>           <none>
2026-01-05 14:47:37,924 [INFO]   argo          resilience-heft-4cqbx-heft-initialize-2594215169        0/2     Completed   0               84m     192.168.221.79    master-m003   <none>           <none>
2026-01-05 14:47:37,924 [INFO]   argo          resilience-heft-76kbp-health-check-270747540            0/2     Error       0               75m     192.168.15.237    worker-w005   <none>           <none>
2026-01-05 14:47:37,924 [INFO]   argo          resilience-heft-76kbp-health-check-287525159            0/2     Error       0               75m     192.168.240.163   worker-w003   <none>           <none>
2026-01-05 14:47:37,924 [INFO]   argo          resilience-heft-76kbp-health-check-304302778            0/2     Error       0               75m     192.168.132.172   worker-w001   <none>           <none>
2026-01-05 14:47:37,924 [INFO]   argo          resilience-heft-76kbp-heft-initialize-3574960751        0/2     Completed   0               75m     192.168.221.86    master-m003   <none>           <none>
2026-01-05 14:47:37,924 [INFO]   argo          resilience-heft-b85xr-health-check-2807215411           0/2     Error       0               72m     192.168.132.175   worker-w001   <none>           <none>
2026-01-05 14:47:37,924 [INFO]   argo          resilience-heft-b85xr-health-check-2823993030           0/2     Error       0               72m     192.168.240.166   worker-w003   <none>           <none>
2026-01-05 14:47:37,925 [INFO]   argo          resilience-heft-b85xr-health-check-2840770649           0/2     Error       0               72m     192.168.15.240    worker-w005   <none>           <none>
2026-01-05 14:47:37,925 [INFO]   argo          resilience-heft-b85xr-heft-initialize-1754406316        0/2     Completed   0               72m     192.168.221.88    master-m003   <none>           <none>
2026-01-05 14:47:37,925 [INFO]   argo          resilience-heft-fdw9f-health-check-1896357264           0/2     Error       0               77m     192.168.15.235    worker-w005   <none>           <none>
2026-01-05 14:47:37,925 [INFO]   argo          resilience-heft-fdw9f-health-check-1913134883           0/2     Error       0               77m     192.168.240.161   worker-w003   <none>           <none>
2026-01-05 14:47:37,925 [INFO]   argo          resilience-heft-fdw9f-health-check-1929912502           0/2     Error       0               77m     192.168.132.170   worker-w001   <none>           <none>
2026-01-05 14:47:37,925 [INFO]   argo          resilience-heft-fdw9f-heft-initialize-2651619139        0/2     Completed   0               78m     192.168.221.77    master-m003   <none>           <none>
2026-01-05 14:47:37,925 [INFO]   argo          resilience-heft-gf256-health-check-946619120            0/2     Error       0               76m     192.168.132.171   worker-w001   <none>           <none>
2026-01-05 14:47:37,925 [INFO]   argo          resilience-heft-gf256-health-check-980174358            0/2     Error       0               76m     192.168.15.236    worker-w005   <none>           <none>
2026-01-05 14:47:37,925 [INFO]   argo          resilience-heft-gf256-health-check-996951977            0/2     Error       0               76m     192.168.240.162   worker-w003   <none>           <none>
2026-01-05 14:47:37,926 [INFO]   argo          resilience-heft-gf256-heft-initialize-1548293021        0/2     Completed   0               76m     192.168.221.84    master-m003   <none>           <none>
2026-01-05 14:47:37,926 [INFO]   argo          resilience-heft-gxwpg-health-check-3428526804           0/2     Error       0               79m     192.168.240.159   worker-w003   <none>           <none>
2026-01-05 14:47:37,926 [INFO]   argo          resilience-heft-gxwpg-health-check-3445304423           0/2     Error       0               79m     192.168.15.233    worker-w005   <none>           <none>
2026-01-05 14:47:37,926 [INFO]   argo          resilience-heft-gxwpg-health-check-3478859661           0/2     Error       0               79m     192.168.132.168   worker-w001   <none>           <none>
2026-01-05 14:47:37,926 [INFO]   argo          resilience-heft-gxwpg-heft-initialize-2377397022        0/2     Completed   0               80m     192.168.221.81    master-m003   <none>           <none>
2026-01-05 14:47:37,926 [INFO]   argo          resilience-heft-pc2kb-health-check-3258812212           0/2     Error       0               74m     192.168.15.238    worker-w005   <none>           <none>
2026-01-05 14:47:37,926 [INFO]   argo          resilience-heft-pc2kb-health-check-3275589831           0/2     Error       0               74m     192.168.240.164   worker-w003   <none>           <none>
2026-01-05 14:47:37,926 [INFO]   argo          resilience-heft-pc2kb-health-check-3292367450           0/2     Error       0               74m     192.168.132.173   worker-w001   <none>           <none>
2026-01-05 14:47:37,926 [INFO]   argo          resilience-heft-pc2kb-heft-initialize-321333135         0/2     Completed   0               74m     192.168.221.83    master-m003   <none>           <none>
2026-01-05 14:47:37,927 [INFO]   argo          resilience-heft-qsmch-health-check-283908068            0/2     Error       0               81m     192.168.15.232    worker-w005   <none>           <none>
2026-01-05 14:47:37,927 [INFO]   argo          resilience-heft-qsmch-health-check-300685687            0/2     Error       0               81m     192.168.240.158   worker-w003   <none>           <none>
2026-01-05 14:47:37,927 [INFO]   argo          resilience-heft-qsmch-health-check-317463306            0/2     Error       0               81m     192.168.132.167   worker-w001   <none>           <none>
2026-01-05 14:47:37,927 [INFO]   argo          resilience-heft-qsmch-heft-initialize-1908810015        0/2     Completed   0               81m     192.168.221.82    master-m003   <none>           <none>
2026-01-05 14:47:37,927 [INFO]   argo          resilience-heft-r2w8l-health-check-350149736            0/2     Error       0               73m     192.168.240.165   worker-w003   <none>           <none>
2026-01-05 14:47:37,927 [INFO]   argo          resilience-heft-r2w8l-health-check-366927355            0/2     Error       0               73m     192.168.15.239    worker-w005   <none>           <none>
2026-01-05 14:47:37,927 [INFO]   argo          resilience-heft-r2w8l-health-check-400482593            0/2     Error       0               73m     192.168.132.174   worker-w001   <none>           <none>
2026-01-05 14:47:37,927 [INFO]   argo          resilience-heft-r2w8l-heft-initialize-3054048250        0/2     Completed   0               73m     192.168.221.85    master-m003   <none>           <none>
2026-01-05 14:47:37,928 [INFO]   argo          resilience-heft-t988k-health-check-612195296            0/2     Error       0               78m     192.168.15.234    worker-w005   <none>           <none>
2026-01-05 14:47:37,928 [INFO]   argo          resilience-heft-t988k-health-check-628972915            0/2     Error       0               78m     192.168.240.160   worker-w003   <none>           <none>
2026-01-05 14:47:37,928 [INFO]   argo          resilience-heft-t988k-health-check-645750534            0/2     Error       0               78m     192.168.132.169   worker-w001   <none>           <none>
2026-01-05 14:47:37,928 [INFO]   argo          resilience-heft-t988k-heft-initialize-164752883         0/2     Completed   0               79m     192.168.221.80    master-m003   <none>           <none>
2026-01-05 14:47:37,928 [INFO]   argo          resilience-heft-vlt2m-health-check-2709447919           0/2     Error       0               35m     192.168.221.119   master-m003   <none>           <none>
2026-01-05 14:47:37,928 [INFO]   argo          resilience-heft-vlt2m-health-check-2726225538           0/2     Error       0               35m     192.168.221.120   master-m003   <none>           <none>
2026-01-05 14:47:37,928 [INFO]   argo          resilience-heft-vlt2m-health-check-2743003157           0/2     Error       0               35m     192.168.221.121   master-m003   <none>           <none>
2026-01-05 14:47:37,928 [INFO]   argo          resilience-heft-vlt2m-heft-initialize-840404112         0/2     Completed   0               35m     192.168.221.118   master-m003   <none>           <none>
2026-01-05 14:47:37,928 [INFO]   argo          resilience-heft-x86wd-health-check-2184182048           0/2     Error       0               33m     192.168.221.123   master-m003   <none>           <none>
2026-01-05 14:47:37,929 [INFO]   argo          resilience-heft-x86wd-health-check-2200959667           0/2     Error       0               33m     192.168.221.125   master-m003   <none>           <none>
2026-01-05 14:47:37,929 [INFO]   argo          resilience-heft-x86wd-health-check-2234514905           0/2     Error       0               33m     192.168.221.124   master-m003   <none>           <none>
2026-01-05 14:47:37,929 [INFO]   argo          resilience-heft-x86wd-heft-initialize-2152040770        0/2     Completed   0               33m     192.168.221.127   master-m003   <none>           <none>
2026-01-05 14:47:37,929 [INFO]   argo          workflow-controller-ccbd949dc-pzf68                     1/1     Running     0               2m19s   192.168.153.241   worker-w004   <none>           <none>
2026-01-05 14:47:37,929 [INFO]   default       resilience-heft-1-201507-qflcd                          1/1     Running     0               2m29s   192.168.221.71    master-m003   <none>           <none>
2026-01-05 14:47:37,929 [INFO]   kube-system   calico-kube-controllers-7498b9bb4c-xd7tb                1/1     Running     0               38h     192.168.191.108   worker-w006   <none>           <none>
2026-01-05 14:47:37,929 [INFO]   kube-system   calico-node-4zhd4                                       1/1     Running     0               10d     192.168.56.105    worker-w002   <none>           <none>
2026-01-05 14:47:37,929 [INFO]   kube-system   calico-node-75nx6                                       1/1     Running     0               10d     192.168.56.109    worker-w006   <none>           <none>
2026-01-05 14:47:37,929 [INFO]   kube-system   calico-node-7lkdq                                       1/1     Running     0               10d     192.168.56.104    worker-w001   <none>           <none>
2026-01-05 14:47:37,930 [INFO]   kube-system   calico-node-85f8c                                       1/1     Running     0               10d     192.168.56.102    master-m002   <none>           <none>
2026-01-05 14:47:37,930 [INFO]   kube-system   calico-node-j8nb9                                       1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:47:37,930 [INFO]   kube-system   calico-node-lbcb2                                       1/1     Running     0               10d     192.168.56.106    worker-w003   <none>           <none>
2026-01-05 14:47:37,930 [INFO]   kube-system   calico-node-wlr5v                                       1/1     Running     0               10d     192.168.56.107    worker-w004   <none>           <none>
2026-01-05 14:47:37,930 [INFO]   kube-system   calico-node-xnzjw                                       1/1     Running     0               10d     192.168.56.103    master-m003   <none>           <none>
2026-01-05 14:47:37,930 [INFO]   kube-system   calico-node-xsltn                                       1/1     Running     0               10d     192.168.56.108    worker-w005   <none>           <none>
2026-01-05 14:47:37,930 [INFO]   kube-system   coredns-668d6bf9bc-2f74f                                1/1     Running     0               39h     192.168.15.230    worker-w005   <none>           <none>
2026-01-05 14:47:37,930 [INFO]   kube-system   coredns-668d6bf9bc-ssl9h                                1/1     Running     0               39h     192.168.132.165   worker-w001   <none>           <none>
2026-01-05 14:47:37,930 [INFO]   kube-system   etcd-master-m001                                        1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:47:37,931 [INFO]   kube-system   kube-apiserver-master-m001                              1/1     Running     1 (2d11h ago)   10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:47:37,931 [INFO]   kube-system   kube-controller-manager-master-m001                     1/1     Running     12 (31m ago)    10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:47:37,931 [INFO]   kube-system   kube-proxy-5lzhj                                        1/1     Running     0               10d     192.168.56.102    master-m002   <none>           <none>
2026-01-05 14:47:37,931 [INFO]   kube-system   kube-proxy-7jqkv                                        1/1     Running     0               10d     192.168.56.109    worker-w006   <none>           <none>
2026-01-05 14:47:37,931 [INFO]   kube-system   kube-proxy-cv5dt                                        1/1     Running     0               10d     192.168.56.108    worker-w005   <none>           <none>
2026-01-05 14:47:37,931 [INFO]   kube-system   kube-proxy-fvpmr                                        1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:47:37,931 [INFO]   kube-system   kube-proxy-hgs5z                                        1/1     Running     0               10d     192.168.56.106    worker-w003   <none>           <none>
2026-01-05 14:47:37,931 [INFO]   kube-system   kube-proxy-kmgqr                                        1/1     Running     0               10d     192.168.56.105    worker-w002   <none>           <none>
2026-01-05 14:47:37,931 [INFO]   kube-system   kube-proxy-rdbz5                                        1/1     Running     0               10d     192.168.56.107    worker-w004   <none>           <none>
2026-01-05 14:47:37,932 [INFO]   kube-system   kube-proxy-w8mnb                                        1/1     Running     0               10d     192.168.56.103    master-m003   <none>           <none>
2026-01-05 14:47:37,932 [INFO]   kube-system   kube-proxy-x9jxr                                        1/1     Running     0               10d     192.168.56.104    worker-w001   <none>           <none>
2026-01-05 14:47:37,932 [INFO]   kube-system   kube-scheduler-master-m001                              1/1     Running     11 (31m ago)    10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:47:37,932 [INFO] 
Pod distribution by node:
2026-01-05 14:47:38,058 [INFO]   Node master-m003: 33 pods
2026-01-05 14:47:38,058 [INFO]   Node worker-w001: 13 pods
2026-01-05 14:47:38,058 [INFO]   Node worker-w005: 13 pods
2026-01-05 14:47:38,059 [INFO]   Node worker-w003: 12 pods
2026-01-05 14:47:38,059 [INFO]   Node worker-w004: 3 pods
2026-01-05 14:47:38,059 [INFO]   Node worker-w006: 3 pods
2026-01-05 14:47:38,059 [INFO]   Node worker-w002: 2 pods
2026-01-05 14:47:38,059 [INFO]   Node master-m002: 2 pods
2026-01-05 14:47:38,059 [INFO]   Node master-m001: 3 pods
2026-01-05 14:47:38,059 [INFO]   Node 10d: 3 pods
2026-01-05 14:47:38,059 [INFO] 
Filtering for simulation services:
2026-01-05 14:47:38,149 [INFO] Node master-m001 is Ready
2026-01-05 14:47:38,149 [INFO] Node master-m002 is Ready
2026-01-05 14:47:38,149 [INFO] Node master-m003 is Ready
2026-01-05 14:47:38,150 [INFO] Node worker-w001 is Ready
2026-01-05 14:47:38,150 [INFO] Node worker-w002 is Ready
2026-01-05 14:47:38,150 [INFO] Node worker-w003 is Ready
2026-01-05 14:47:38,150 [INFO] Node worker-w004 is Ready
2026-01-05 14:47:38,150 [INFO] Node worker-w005 is Ready
2026-01-05 14:47:38,150 [INFO] Node worker-w006 is Ready
2026-01-05 14:47:38,153 [WARNING] No pods found for etcd-sim
2026-01-05 14:47:38,155 [WARNING] No pods found for postgres-sim
2026-01-05 14:47:38,157 [WARNING] No pods found for redis-sim
2026-01-05 14:47:38,159 [WARNING] No pods found for nginx-sim
2026-01-05 14:47:38,161 [WARNING] No pods found for auth-sim
2026-01-05 14:47:38,161 [INFO] Completed full health check
[2026-01-05 14:47:38] TIMING: INTERIM_HEALTH_CHECK completed in 3 seconds

================================================================
 STEP 4: RACK FAILURE SIMULATION (HEFT-Aware)
 Excluding zone: R3
 Time: Mon Jan  5 14:47:38 UTC 2026
================================================================
Log directory created/verified: /app/logs
File logging configured successfully
2026-01-05 14:47:38,571 [INFO] Loaded in-cluster Kubernetes config
2026-01-05 14:47:38,572 [INFO] Running on host: resilience-heft-1-201507-qflcd
2026-01-05 14:47:38,572 [INFO] Detected current node: master-m003, zone: R3
2026-01-05 14:47:38,572 [INFO] Checking if we have permissions to modify nodes...
2026-01-05 14:47:38,592 [INFO] Testing permissions using node: master-m001
2026-01-05 14:47:38,598 [INFO] Permission check successful - we can modify nodes
2026-01-05 14:47:38,598 [INFO] Using real Kubernetes API for node control
2026-01-05 14:47:38,599 [INFO] Action received: simulate-rack
2026-01-05 14:47:38,599 [INFO] Stabilization time: 60 seconds
2026-01-05 14:47:38,599 [INFO] Current node: master-m003 in zone: R3
2026-01-05 14:47:38,599 [INFO] Safe zones for rack simulation: ['R1', 'R2']
2026-01-05 14:47:38,599 [INFO] Simulating full rack (zone) failure: R2 with nodes: ['master-m002', 'worker-w003', 'worker-w004']
2026-01-05 14:47:38,599 [INFO] Simulating node failure for master-m002 using Kubernetes API
2026-01-05 14:47:38,611 [INFO] Node master-m002 cordoned
2026-01-05 14:47:38,623 [INFO] Node master-m002 tainted with NoExecute
2026-01-05 14:47:38,623 [INFO] Node master-m002 powered off (delay 5s)
2026-01-05 14:47:38,623 [INFO] Node master-m002 powered off (delay 5s)
2026-01-05 14:47:43,624 [INFO] Simulating node failure for worker-w003 using Kubernetes API
2026-01-05 14:47:43,641 [INFO] Node worker-w003 cordoned
2026-01-05 14:47:44,025 [INFO] Node worker-w003 tainted with NoExecute
2026-01-05 14:47:44,026 [INFO] Node worker-w003 powered off (delay 5s)
2026-01-05 14:47:44,026 [INFO] Node worker-w003 powered off (delay 5s)
2026-01-05 14:47:49,030 [INFO] Simulating node failure for worker-w004 using Kubernetes API
2026-01-05 14:47:49,276 [INFO] Node worker-w004 cordoned
2026-01-05 14:47:49,299 [INFO] Node worker-w004 tainted with NoExecute
2026-01-05 14:47:49,299 [INFO] Node worker-w004 powered off (delay 5s)
2026-01-05 14:47:49,299 [INFO] Node worker-w004 powered off (delay 5s)
2026-01-05 14:47:54,315 [INFO] Waiting 60 seconds for the cluster to stabilize before health check...
2026-01-05 14:48:54,373 [INFO] Running health check after rack power off
2026-01-05 14:48:54,374 [INFO] Starting full health check
2026-01-05 14:48:54,374 [INFO] 
============ DETAILED NODE STATUS ============
2026-01-05 14:48:54,374 [INFO] Basic Node Information (kubectl get nodes -o wide):
2026-01-05 14:48:54,486 [INFO]   NAME          STATUS                     ROLES           AGE   VERSION    INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
2026-01-05 14:48:54,486 [INFO]   master-m001   Ready                      control-plane   10d   v1.32.11   192.168.56.101   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:48:54,486 [INFO]   master-m002   Ready,SchedulingDisabled   control-plane   10d   v1.32.11   192.168.56.102   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:48:54,486 [INFO]   master-m003   Ready                      control-plane   10d   v1.32.11   192.168.56.103   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:48:54,486 [INFO]   worker-w001   Ready                      <none>          10d   v1.32.11   192.168.56.104   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:48:54,487 [INFO]   worker-w002   Ready                      <none>          10d   v1.32.11   192.168.56.105   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:48:54,487 [INFO]   worker-w003   Ready,SchedulingDisabled   <none>          10d   v1.32.11   192.168.56.106   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:48:54,487 [INFO]   worker-w004   Ready,SchedulingDisabled   <none>          10d   v1.32.11   192.168.56.107   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:48:54,487 [INFO]   worker-w005   Ready                      <none>          10d   v1.32.11   192.168.56.108   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:48:54,487 [INFO]   worker-w006   Ready                      <none>          10d   v1.32.11   192.168.56.109   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:48:54,487 [INFO] 
Enhanced Node Status (with taint and cordon indicators):
2026-01-05 14:48:54,487 [INFO]   NAME                STATUS    ROLES           ZONE   CORDONED   TAINTS
2026-01-05 14:48:54,853 [INFO]   master-m001     Ready  ✓ worker         R1    No       node-role.kubernetes.io/control-plane 
2026-01-05 14:48:55,023 [INFO]   master-m002     Ready  ✓ worker         R2    YES     ⚠️ simulated-failure, node.kubernetes.io/unschedulable ⚠️
2026-01-05 14:48:55,203 [INFO]   master-m003     Ready  ✓ worker         R3    No       None 
2026-01-05 14:48:55,400 [INFO]   worker-w001     Ready  ✓ worker         R1    No       None 
2026-01-05 14:48:55,593 [INFO]   worker-w002     Ready  ✓ worker         R1    No       None 
2026-01-05 14:48:55,770 [INFO]   worker-w003     Ready  ✓ worker         R2    YES     ⚠️ simulated-failure, node.kubernetes.io/unschedulable ⚠️
2026-01-05 14:48:55,962 [INFO]   worker-w004     Ready  ✓ worker         R2    YES     ⚠️ simulated-failure, node.kubernetes.io/unschedulable ⚠️
2026-01-05 14:48:56,138 [INFO]   worker-w005     Ready  ✓ worker         R3    No       None 
2026-01-05 14:48:56,317 [INFO]   worker-w006     Ready  ✓ worker         R3    No       None 
2026-01-05 14:48:56,318 [INFO] 
Legend:
2026-01-05 14:48:56,318 [INFO]   ✓ = Node is Ready
2026-01-05 14:48:56,318 [INFO]   ⚠️ = Warning indicator (NotReady, Cordoned, or has simulated-failure taint)
2026-01-05 14:48:56,318 [INFO] ============ DETAILED POD INFORMATION ============
2026-01-05 14:48:56,318 [INFO] Running 'kubectl get pods -o wide' to show detailed pod placement:
2026-01-05 14:48:56,432 [INFO]   NAMESPACE     NAME                                                    READY   STATUS      RESTARTS        AGE     IP                NODE          NOMINATED NODE   READINESS GATES
2026-01-05 14:48:56,432 [INFO]   argo          argo-server-5c69cb69db-gdkl6                            1/1     Running     0               8d      192.168.221.65    master-m003   <none>           <none>
2026-01-05 14:48:56,432 [INFO]   argo          resilience-bench-ptngm-initialize-metrics-940927476     0/2     Completed   0               2d16h   192.168.221.125   master-m003   <none>           <none>
2026-01-05 14:48:56,432 [INFO]   argo          resilience-bench-ptngm-run-health-check-1168793163      0/2     Completed   0               2d16h   192.168.221.67    master-m003   <none>           <none>
2026-01-05 14:48:56,432 [INFO]   argo          resilience-bench-ptngm-run-health-check-1185570782      0/2     Completed   0               2d16h   192.168.221.66    master-m003   <none>           <none>
2026-01-05 14:48:56,432 [INFO]   argo          resilience-bench-ptngm-run-health-check-1202348401      0/2     Completed   0               2d16h   192.168.221.122   master-m003   <none>           <none>
2026-01-05 14:48:56,432 [INFO]   argo          resilience-bench-wf87f-initialize-metrics-3677018408    0/2     Completed   0               2d18h   192.168.221.100   master-m003   <none>           <none>
2026-01-05 14:48:56,433 [INFO]   argo          resilience-bench-wf87f-run-health-check-2914150102      0/2     Completed   0               2d18h   192.168.221.105   master-m003   <none>           <none>
2026-01-05 14:48:56,433 [INFO]   argo          resilience-bench-wf87f-run-health-check-3931060015      0/2     Completed   0               2d18h   192.168.221.104   master-m003   <none>           <none>
2026-01-05 14:48:56,433 [INFO]   argo          resilience-bench-wf87f-run-health-check-3947837634      0/2     Completed   0               2d18h   192.168.221.98    master-m003   <none>           <none>
2026-01-05 14:48:56,433 [INFO]   argo          resilience-bench-wf87f-run-health-check-3964615253      0/2     Completed   0               2d18h   192.168.221.103   master-m003   <none>           <none>
2026-01-05 14:48:56,433 [INFO]   argo          resilience-bench-wf87f-run-node-simulation-4173146970   0/2     Completed   0               2d18h   192.168.221.101   master-m003   <none>           <none>
2026-01-05 14:48:56,433 [INFO]   argo          resilience-bench-wf87f-run-rack-simulation-1356070019   0/2     Completed   0               2d18h   192.168.221.109   master-m003   <none>           <none>
2026-01-05 14:48:56,433 [INFO]   argo          resilience-heft-4cqbx-health-check-1555843180           0/2     Error       0               85m     192.168.132.166   worker-w001   <none>           <none>
2026-01-05 14:48:56,433 [INFO]   argo          resilience-heft-4cqbx-health-check-1589398418           0/2     Error       0               85m     192.168.15.231    worker-w005   <none>           <none>
2026-01-05 14:48:56,433 [INFO]   argo          resilience-heft-4cqbx-heft-initialize-2594215169        0/2     Completed   0               86m     192.168.221.79    master-m003   <none>           <none>
2026-01-05 14:48:56,433 [INFO]   argo          resilience-heft-76kbp-health-check-270747540            0/2     Error       0               76m     192.168.15.237    worker-w005   <none>           <none>
2026-01-05 14:48:56,433 [INFO]   argo          resilience-heft-76kbp-health-check-304302778            0/2     Error       0               76m     192.168.132.172   worker-w001   <none>           <none>
2026-01-05 14:48:56,434 [INFO]   argo          resilience-heft-76kbp-heft-initialize-3574960751        0/2     Completed   0               77m     192.168.221.86    master-m003   <none>           <none>
2026-01-05 14:48:56,434 [INFO]   argo          resilience-heft-b85xr-health-check-2807215411           0/2     Error       0               73m     192.168.132.175   worker-w001   <none>           <none>
2026-01-05 14:48:56,434 [INFO]   argo          resilience-heft-b85xr-health-check-2840770649           0/2     Error       0               73m     192.168.15.240    worker-w005   <none>           <none>
2026-01-05 14:48:56,434 [INFO]   argo          resilience-heft-b85xr-heft-initialize-1754406316        0/2     Completed   0               73m     192.168.221.88    master-m003   <none>           <none>
2026-01-05 14:48:56,434 [INFO]   argo          resilience-heft-fdw9f-health-check-1896357264           0/2     Error       0               79m     192.168.15.235    worker-w005   <none>           <none>
2026-01-05 14:48:56,434 [INFO]   argo          resilience-heft-fdw9f-health-check-1929912502           0/2     Error       0               79m     192.168.132.170   worker-w001   <none>           <none>
2026-01-05 14:48:56,434 [INFO]   argo          resilience-heft-fdw9f-heft-initialize-2651619139        0/2     Completed   0               79m     192.168.221.77    master-m003   <none>           <none>
2026-01-05 14:48:56,434 [INFO]   argo          resilience-heft-gf256-health-check-946619120            0/2     Error       0               77m     192.168.132.171   worker-w001   <none>           <none>
2026-01-05 14:48:56,434 [INFO]   argo          resilience-heft-gf256-health-check-980174358            0/2     Error       0               77m     192.168.15.236    worker-w005   <none>           <none>
2026-01-05 14:48:56,434 [INFO]   argo          resilience-heft-gf256-heft-initialize-1548293021        0/2     Completed   0               78m     192.168.221.84    master-m003   <none>           <none>
2026-01-05 14:48:56,434 [INFO]   argo          resilience-heft-gxwpg-health-check-3445304423           0/2     Error       0               81m     192.168.15.233    worker-w005   <none>           <none>
2026-01-05 14:48:56,435 [INFO]   argo          resilience-heft-gxwpg-health-check-3478859661           0/2     Error       0               81m     192.168.132.168   worker-w001   <none>           <none>
2026-01-05 14:48:56,435 [INFO]   argo          resilience-heft-gxwpg-heft-initialize-2377397022        0/2     Completed   0               81m     192.168.221.81    master-m003   <none>           <none>
2026-01-05 14:48:56,435 [INFO]   argo          resilience-heft-pc2kb-health-check-3258812212           0/2     Error       0               75m     192.168.15.238    worker-w005   <none>           <none>
2026-01-05 14:48:56,435 [INFO]   argo          resilience-heft-pc2kb-health-check-3292367450           0/2     Error       0               75m     192.168.132.173   worker-w001   <none>           <none>
2026-01-05 14:48:56,435 [INFO]   argo          resilience-heft-pc2kb-heft-initialize-321333135         0/2     Completed   0               76m     192.168.221.83    master-m003   <none>           <none>
2026-01-05 14:48:56,435 [INFO]   argo          resilience-heft-qsmch-health-check-283908068            0/2     Error       0               82m     192.168.15.232    worker-w005   <none>           <none>
2026-01-05 14:48:56,435 [INFO]   argo          resilience-heft-qsmch-health-check-317463306            0/2     Error       0               82m     192.168.132.167   worker-w001   <none>           <none>
2026-01-05 14:48:56,435 [INFO]   argo          resilience-heft-qsmch-heft-initialize-1908810015        0/2     Completed   0               83m     192.168.221.82    master-m003   <none>           <none>
2026-01-05 14:48:56,435 [INFO]   argo          resilience-heft-r2w8l-health-check-366927355            0/2     Error       0               74m     192.168.15.239    worker-w005   <none>           <none>
2026-01-05 14:48:56,435 [INFO]   argo          resilience-heft-r2w8l-health-check-400482593            0/2     Error       0               74m     192.168.132.174   worker-w001   <none>           <none>
2026-01-05 14:48:56,435 [INFO]   argo          resilience-heft-r2w8l-heft-initialize-3054048250        0/2     Completed   0               75m     192.168.221.85    master-m003   <none>           <none>
2026-01-05 14:48:56,435 [INFO]   argo          resilience-heft-t988k-health-check-612195296            0/2     Error       0               79m     192.168.15.234    worker-w005   <none>           <none>
2026-01-05 14:48:56,436 [INFO]   argo          resilience-heft-t988k-health-check-645750534            0/2     Error       0               79m     192.168.132.169   worker-w001   <none>           <none>
2026-01-05 14:48:56,436 [INFO]   argo          resilience-heft-t988k-heft-initialize-164752883         0/2     Completed   0               80m     192.168.221.80    master-m003   <none>           <none>
2026-01-05 14:48:56,436 [INFO]   argo          resilience-heft-vlt2m-health-check-2709447919           0/2     Error       0               36m     192.168.221.119   master-m003   <none>           <none>
2026-01-05 14:48:56,436 [INFO]   argo          resilience-heft-vlt2m-health-check-2726225538           0/2     Error       0               36m     192.168.221.120   master-m003   <none>           <none>
2026-01-05 14:48:56,436 [INFO]   argo          resilience-heft-vlt2m-health-check-2743003157           0/2     Error       0               36m     192.168.221.121   master-m003   <none>           <none>
2026-01-05 14:48:56,436 [INFO]   argo          resilience-heft-vlt2m-heft-initialize-840404112         0/2     Completed   0               36m     192.168.221.118   master-m003   <none>           <none>
2026-01-05 14:48:56,436 [INFO]   argo          resilience-heft-x86wd-health-check-2184182048           0/2     Error       0               34m     192.168.221.123   master-m003   <none>           <none>
2026-01-05 14:48:56,436 [INFO]   argo          resilience-heft-x86wd-health-check-2200959667           0/2     Error       0               34m     192.168.221.125   master-m003   <none>           <none>
2026-01-05 14:48:56,436 [INFO]   argo          resilience-heft-x86wd-health-check-2234514905           0/2     Error       0               34m     192.168.221.124   master-m003   <none>           <none>
2026-01-05 14:48:56,436 [INFO]   argo          resilience-heft-x86wd-heft-initialize-2152040770        0/2     Completed   0               35m     192.168.221.127   master-m003   <none>           <none>
2026-01-05 14:48:56,436 [INFO]   argo          workflow-controller-ccbd949dc-t4rx9                     1/1     Running     0               67s     192.168.195.243   worker-w002   <none>           <none>
2026-01-05 14:48:56,437 [INFO]   default       resilience-heft-1-201507-qflcd                          1/1     Running     0               3m48s   192.168.221.71    master-m003   <none>           <none>
2026-01-05 14:48:56,437 [INFO]   kube-system   calico-kube-controllers-7498b9bb4c-xd7tb                1/1     Running     0               38h     192.168.191.108   worker-w006   <none>           <none>
2026-01-05 14:48:56,437 [INFO]   kube-system   calico-node-4zhd4                                       1/1     Running     0               10d     192.168.56.105    worker-w002   <none>           <none>
2026-01-05 14:48:56,437 [INFO]   kube-system   calico-node-75nx6                                       1/1     Running     0               10d     192.168.56.109    worker-w006   <none>           <none>
2026-01-05 14:48:56,437 [INFO]   kube-system   calico-node-7lkdq                                       1/1     Running     0               10d     192.168.56.104    worker-w001   <none>           <none>
2026-01-05 14:48:56,437 [INFO]   kube-system   calico-node-85f8c                                       1/1     Running     0               10d     192.168.56.102    master-m002   <none>           <none>
2026-01-05 14:48:56,437 [INFO]   kube-system   calico-node-j8nb9                                       1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:48:56,437 [INFO]   kube-system   calico-node-lbcb2                                       1/1     Running     0               10d     192.168.56.106    worker-w003   <none>           <none>
2026-01-05 14:48:56,437 [INFO]   kube-system   calico-node-wlr5v                                       1/1     Running     0               10d     192.168.56.107    worker-w004   <none>           <none>
2026-01-05 14:48:56,438 [INFO]   kube-system   calico-node-xnzjw                                       1/1     Running     0               10d     192.168.56.103    master-m003   <none>           <none>
2026-01-05 14:48:56,438 [INFO]   kube-system   calico-node-xsltn                                       1/1     Running     0               10d     192.168.56.108    worker-w005   <none>           <none>
2026-01-05 14:48:56,438 [INFO]   kube-system   coredns-668d6bf9bc-2f74f                                1/1     Running     0               39h     192.168.15.230    worker-w005   <none>           <none>
2026-01-05 14:48:56,438 [INFO]   kube-system   coredns-668d6bf9bc-ssl9h                                1/1     Running     0               39h     192.168.132.165   worker-w001   <none>           <none>
2026-01-05 14:48:56,438 [INFO]   kube-system   etcd-master-m001                                        1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:48:56,438 [INFO]   kube-system   kube-apiserver-master-m001                              1/1     Running     1 (2d11h ago)   10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:48:56,438 [INFO]   kube-system   kube-controller-manager-master-m001                     1/1     Running     12 (33m ago)    10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:48:56,438 [INFO]   kube-system   kube-proxy-5lzhj                                        1/1     Running     0               10d     192.168.56.102    master-m002   <none>           <none>
2026-01-05 14:48:56,438 [INFO]   kube-system   kube-proxy-7jqkv                                        1/1     Running     0               10d     192.168.56.109    worker-w006   <none>           <none>
2026-01-05 14:48:56,438 [INFO]   kube-system   kube-proxy-cv5dt                                        1/1     Running     0               10d     192.168.56.108    worker-w005   <none>           <none>
2026-01-05 14:48:56,438 [INFO]   kube-system   kube-proxy-fvpmr                                        1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:48:56,438 [INFO]   kube-system   kube-proxy-hgs5z                                        1/1     Running     0               10d     192.168.56.106    worker-w003   <none>           <none>
2026-01-05 14:48:56,439 [INFO]   kube-system   kube-proxy-kmgqr                                        1/1     Running     0               10d     192.168.56.105    worker-w002   <none>           <none>
2026-01-05 14:48:56,439 [INFO]   kube-system   kube-proxy-rdbz5                                        1/1     Running     0               10d     192.168.56.107    worker-w004   <none>           <none>
2026-01-05 14:48:56,439 [INFO]   kube-system   kube-proxy-w8mnb                                        1/1     Running     0               10d     192.168.56.103    master-m003   <none>           <none>
2026-01-05 14:48:56,439 [INFO]   kube-system   kube-proxy-x9jxr                                        1/1     Running     0               10d     192.168.56.104    worker-w001   <none>           <none>
2026-01-05 14:48:56,439 [INFO]   kube-system   kube-scheduler-master-m001                              1/1     Running     11 (33m ago)    10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:48:56,439 [INFO] 
Pod distribution by node:
2026-01-05 14:48:56,552 [INFO]   Node master-m003: 33 pods
2026-01-05 14:48:56,552 [INFO]   Node worker-w001: 13 pods
2026-01-05 14:48:56,552 [INFO]   Node worker-w005: 13 pods
2026-01-05 14:48:56,552 [INFO]   Node worker-w002: 3 pods
2026-01-05 14:48:56,552 [INFO]   Node worker-w006: 3 pods
2026-01-05 14:48:56,552 [INFO]   Node master-m002: 2 pods
2026-01-05 14:48:56,552 [INFO]   Node master-m001: 3 pods
2026-01-05 14:48:56,552 [INFO]   Node worker-w003: 2 pods
2026-01-05 14:48:56,553 [INFO]   Node worker-w004: 2 pods
2026-01-05 14:48:56,553 [INFO]   Node 10d: 3 pods
2026-01-05 14:48:56,553 [INFO] 
Filtering for simulation services:
2026-01-05 14:48:56,675 [INFO] Node master-m001 is Ready
2026-01-05 14:48:56,676 [INFO] Node master-m002 is Ready
2026-01-05 14:48:56,676 [INFO] Node master-m003 is Ready
2026-01-05 14:48:56,676 [INFO] Node worker-w001 is Ready
2026-01-05 14:48:56,676 [INFO] Node worker-w002 is Ready
2026-01-05 14:48:56,676 [INFO] Node worker-w003 is Ready
2026-01-05 14:48:56,676 [INFO] Node worker-w004 is Ready
2026-01-05 14:48:56,676 [INFO] Node worker-w005 is Ready
2026-01-05 14:48:56,677 [INFO] Node worker-w006 is Ready
2026-01-05 14:48:56,739 [WARNING] No pods found for etcd-sim
2026-01-05 14:48:56,743 [WARNING] No pods found for postgres-sim
2026-01-05 14:48:56,746 [WARNING] No pods found for redis-sim
2026-01-05 14:48:56,748 [WARNING] No pods found for nginx-sim
2026-01-05 14:48:56,751 [WARNING] No pods found for auth-sim
2026-01-05 14:48:56,758 [INFO] Completed full health check
2026-01-05 14:48:56,758 [INFO] Zone R2 remains down for 10 seconds
2026-01-05 14:49:06,765 [INFO] Running health check before rack power on
2026-01-05 14:49:06,766 [INFO] Starting full health check
2026-01-05 14:49:06,766 [INFO] 
============ DETAILED NODE STATUS ============
2026-01-05 14:49:06,766 [INFO] Basic Node Information (kubectl get nodes -o wide):
2026-01-05 14:49:07,029 [INFO]   NAME          STATUS                     ROLES           AGE   VERSION    INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
2026-01-05 14:49:07,030 [INFO]   master-m001   Ready                      control-plane   10d   v1.32.11   192.168.56.101   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:49:07,030 [INFO]   master-m002   Ready,SchedulingDisabled   control-plane   10d   v1.32.11   192.168.56.102   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:49:07,030 [INFO]   master-m003   Ready                      control-plane   10d   v1.32.11   192.168.56.103   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:49:07,030 [INFO]   worker-w001   Ready                      <none>          10d   v1.32.11   192.168.56.104   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:49:07,030 [INFO]   worker-w002   Ready                      <none>          10d   v1.32.11   192.168.56.105   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:49:07,030 [INFO]   worker-w003   Ready,SchedulingDisabled   <none>          10d   v1.32.11   192.168.56.106   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:49:07,030 [INFO]   worker-w004   Ready,SchedulingDisabled   <none>          10d   v1.32.11   192.168.56.107   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:49:07,030 [INFO]   worker-w005   Ready                      <none>          10d   v1.32.11   192.168.56.108   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:49:07,031 [INFO]   worker-w006   Ready                      <none>          10d   v1.32.11   192.168.56.109   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:49:07,031 [INFO] 
Enhanced Node Status (with taint and cordon indicators):
2026-01-05 14:49:07,031 [INFO]   NAME                STATUS    ROLES           ZONE   CORDONED   TAINTS
2026-01-05 14:49:07,334 [INFO]   master-m001     Ready  ✓ worker         R1    No       node-role.kubernetes.io/control-plane 
2026-01-05 14:49:07,516 [INFO]   master-m002     Ready  ✓ worker         R2    YES     ⚠️ simulated-failure, node.kubernetes.io/unschedulable ⚠️
2026-01-05 14:49:07,692 [INFO]   master-m003     Ready  ✓ worker         R3    No       None 
2026-01-05 14:49:07,864 [INFO]   worker-w001     Ready  ✓ worker         R1    No       None 
2026-01-05 14:49:08,049 [INFO]   worker-w002     Ready  ✓ worker         R1    No       None 
2026-01-05 14:49:08,197 [INFO]   worker-w003     Ready  ✓ worker         R2    YES     ⚠️ simulated-failure, node.kubernetes.io/unschedulable ⚠️
2026-01-05 14:49:08,337 [INFO]   worker-w004     Ready  ✓ worker         R2    YES     ⚠️ simulated-failure, node.kubernetes.io/unschedulable ⚠️
2026-01-05 14:49:08,524 [INFO]   worker-w005     Ready  ✓ worker         R3    No       None 
2026-01-05 14:49:08,684 [INFO]   worker-w006     Ready  ✓ worker         R3    No       None 
2026-01-05 14:49:08,684 [INFO] 
Legend:
2026-01-05 14:49:08,684 [INFO]   ✓ = Node is Ready
2026-01-05 14:49:08,684 [INFO]   ⚠️ = Warning indicator (NotReady, Cordoned, or has simulated-failure taint)
2026-01-05 14:49:08,684 [INFO] ============ DETAILED POD INFORMATION ============
2026-01-05 14:49:08,684 [INFO] Running 'kubectl get pods -o wide' to show detailed pod placement:
2026-01-05 14:49:08,803 [INFO]   NAMESPACE     NAME                                                    READY   STATUS      RESTARTS        AGE     IP                NODE          NOMINATED NODE   READINESS GATES
2026-01-05 14:49:08,803 [INFO]   argo          argo-server-5c69cb69db-gdkl6                            1/1     Running     0               8d      192.168.221.65    master-m003   <none>           <none>
2026-01-05 14:49:08,803 [INFO]   argo          resilience-bench-ptngm-initialize-metrics-940927476     0/2     Completed   0               2d16h   192.168.221.125   master-m003   <none>           <none>
2026-01-05 14:49:08,803 [INFO]   argo          resilience-bench-ptngm-run-health-check-1168793163      0/2     Completed   0               2d16h   192.168.221.67    master-m003   <none>           <none>
2026-01-05 14:49:08,803 [INFO]   argo          resilience-bench-ptngm-run-health-check-1185570782      0/2     Completed   0               2d16h   192.168.221.66    master-m003   <none>           <none>
2026-01-05 14:49:08,803 [INFO]   argo          resilience-bench-ptngm-run-health-check-1202348401      0/2     Completed   0               2d16h   192.168.221.122   master-m003   <none>           <none>
2026-01-05 14:49:08,803 [INFO]   argo          resilience-bench-wf87f-initialize-metrics-3677018408    0/2     Completed   0               2d18h   192.168.221.100   master-m003   <none>           <none>
2026-01-05 14:49:08,803 [INFO]   argo          resilience-bench-wf87f-run-health-check-2914150102      0/2     Completed   0               2d18h   192.168.221.105   master-m003   <none>           <none>
2026-01-05 14:49:08,803 [INFO]   argo          resilience-bench-wf87f-run-health-check-3931060015      0/2     Completed   0               2d18h   192.168.221.104   master-m003   <none>           <none>
2026-01-05 14:49:08,803 [INFO]   argo          resilience-bench-wf87f-run-health-check-3947837634      0/2     Completed   0               2d18h   192.168.221.98    master-m003   <none>           <none>
2026-01-05 14:49:08,803 [INFO]   argo          resilience-bench-wf87f-run-health-check-3964615253      0/2     Completed   0               2d18h   192.168.221.103   master-m003   <none>           <none>
2026-01-05 14:49:08,803 [INFO]   argo          resilience-bench-wf87f-run-node-simulation-4173146970   0/2     Completed   0               2d18h   192.168.221.101   master-m003   <none>           <none>
2026-01-05 14:49:08,803 [INFO]   argo          resilience-bench-wf87f-run-rack-simulation-1356070019   0/2     Completed   0               2d18h   192.168.221.109   master-m003   <none>           <none>
2026-01-05 14:49:08,803 [INFO]   argo          resilience-heft-4cqbx-health-check-1555843180           0/2     Error       0               85m     192.168.132.166   worker-w001   <none>           <none>
2026-01-05 14:49:08,803 [INFO]   argo          resilience-heft-4cqbx-health-check-1589398418           0/2     Error       0               85m     192.168.15.231    worker-w005   <none>           <none>
2026-01-05 14:49:08,803 [INFO]   argo          resilience-heft-4cqbx-heft-initialize-2594215169        0/2     Completed   0               86m     192.168.221.79    master-m003   <none>           <none>
2026-01-05 14:49:08,804 [INFO]   argo          resilience-heft-76kbp-health-check-270747540            0/2     Error       0               77m     192.168.15.237    worker-w005   <none>           <none>
2026-01-05 14:49:08,804 [INFO]   argo          resilience-heft-76kbp-health-check-304302778            0/2     Error       0               77m     192.168.132.172   worker-w001   <none>           <none>
2026-01-05 14:49:08,804 [INFO]   argo          resilience-heft-76kbp-heft-initialize-3574960751        0/2     Completed   0               77m     192.168.221.86    master-m003   <none>           <none>
2026-01-05 14:49:08,804 [INFO]   argo          resilience-heft-b85xr-health-check-2807215411           0/2     Error       0               73m     192.168.132.175   worker-w001   <none>           <none>
2026-01-05 14:49:08,804 [INFO]   argo          resilience-heft-b85xr-health-check-2840770649           0/2     Error       0               73m     192.168.15.240    worker-w005   <none>           <none>
2026-01-05 14:49:08,804 [INFO]   argo          resilience-heft-b85xr-heft-initialize-1754406316        0/2     Completed   0               74m     192.168.221.88    master-m003   <none>           <none>
2026-01-05 14:49:08,804 [INFO]   argo          resilience-heft-fdw9f-health-check-1896357264           0/2     Error       0               79m     192.168.15.235    worker-w005   <none>           <none>
2026-01-05 14:49:08,804 [INFO]   argo          resilience-heft-fdw9f-health-check-1929912502           0/2     Error       0               79m     192.168.132.170   worker-w001   <none>           <none>
2026-01-05 14:49:08,804 [INFO]   argo          resilience-heft-fdw9f-heft-initialize-2651619139        0/2     Completed   0               79m     192.168.221.77    master-m003   <none>           <none>
2026-01-05 14:49:08,804 [INFO]   argo          resilience-heft-gf256-health-check-946619120            0/2     Error       0               78m     192.168.132.171   worker-w001   <none>           <none>
2026-01-05 14:49:08,804 [INFO]   argo          resilience-heft-gf256-health-check-980174358            0/2     Error       0               78m     192.168.15.236    worker-w005   <none>           <none>
2026-01-05 14:49:08,804 [INFO]   argo          resilience-heft-gf256-heft-initialize-1548293021        0/2     Completed   0               78m     192.168.221.84    master-m003   <none>           <none>
2026-01-05 14:49:08,804 [INFO]   argo          resilience-heft-gxwpg-health-check-3445304423           0/2     Error       0               81m     192.168.15.233    worker-w005   <none>           <none>
2026-01-05 14:49:08,804 [INFO]   argo          resilience-heft-gxwpg-health-check-3478859661           0/2     Error       0               81m     192.168.132.168   worker-w001   <none>           <none>
2026-01-05 14:49:08,804 [INFO]   argo          resilience-heft-gxwpg-heft-initialize-2377397022        0/2     Completed   0               81m     192.168.221.81    master-m003   <none>           <none>
2026-01-05 14:49:08,804 [INFO]   argo          resilience-heft-pc2kb-health-check-3258812212           0/2     Error       0               75m     192.168.15.238    worker-w005   <none>           <none>
2026-01-05 14:49:08,804 [INFO]   argo          resilience-heft-pc2kb-health-check-3292367450           0/2     Error       0               75m     192.168.132.173   worker-w001   <none>           <none>
2026-01-05 14:49:08,804 [INFO]   argo          resilience-heft-pc2kb-heft-initialize-321333135         0/2     Completed   0               76m     192.168.221.83    master-m003   <none>           <none>
2026-01-05 14:49:08,804 [INFO]   argo          resilience-heft-qsmch-health-check-283908068            0/2     Error       0               82m     192.168.15.232    worker-w005   <none>           <none>
2026-01-05 14:49:08,804 [INFO]   argo          resilience-heft-qsmch-health-check-317463306            0/2     Error       0               82m     192.168.132.167   worker-w001   <none>           <none>
2026-01-05 14:49:08,804 [INFO]   argo          resilience-heft-qsmch-heft-initialize-1908810015        0/2     Completed   0               83m     192.168.221.82    master-m003   <none>           <none>
2026-01-05 14:49:08,804 [INFO]   argo          resilience-heft-r2w8l-health-check-366927355            0/2     Error       0               74m     192.168.15.239    worker-w005   <none>           <none>
2026-01-05 14:49:08,804 [INFO]   argo          resilience-heft-r2w8l-health-check-400482593            0/2     Error       0               74m     192.168.132.174   worker-w001   <none>           <none>
2026-01-05 14:49:08,804 [INFO]   argo          resilience-heft-r2w8l-heft-initialize-3054048250        0/2     Completed   0               75m     192.168.221.85    master-m003   <none>           <none>
2026-01-05 14:49:08,804 [INFO]   argo          resilience-heft-t988k-health-check-612195296            0/2     Error       0               80m     192.168.15.234    worker-w005   <none>           <none>
2026-01-05 14:49:08,805 [INFO]   argo          resilience-heft-t988k-health-check-645750534            0/2     Error       0               80m     192.168.132.169   worker-w001   <none>           <none>
2026-01-05 14:49:08,805 [INFO]   argo          resilience-heft-t988k-heft-initialize-164752883         0/2     Completed   0               80m     192.168.221.80    master-m003   <none>           <none>
2026-01-05 14:49:08,805 [INFO]   argo          resilience-heft-vlt2m-health-check-2709447919           0/2     Error       0               36m     192.168.221.119   master-m003   <none>           <none>
2026-01-05 14:49:08,805 [INFO]   argo          resilience-heft-vlt2m-health-check-2726225538           0/2     Error       0               36m     192.168.221.120   master-m003   <none>           <none>
2026-01-05 14:49:08,805 [INFO]   argo          resilience-heft-vlt2m-health-check-2743003157           0/2     Error       0               36m     192.168.221.121   master-m003   <none>           <none>
2026-01-05 14:49:08,805 [INFO]   argo          resilience-heft-vlt2m-heft-initialize-840404112         0/2     Completed   0               37m     192.168.221.118   master-m003   <none>           <none>
2026-01-05 14:49:08,805 [INFO]   argo          resilience-heft-x86wd-health-check-2184182048           0/2     Error       0               35m     192.168.221.123   master-m003   <none>           <none>
2026-01-05 14:49:08,805 [INFO]   argo          resilience-heft-x86wd-health-check-2200959667           0/2     Error       0               35m     192.168.221.125   master-m003   <none>           <none>
2026-01-05 14:49:08,805 [INFO]   argo          resilience-heft-x86wd-health-check-2234514905           0/2     Error       0               35m     192.168.221.124   master-m003   <none>           <none>
2026-01-05 14:49:08,805 [INFO]   argo          resilience-heft-x86wd-heft-initialize-2152040770        0/2     Completed   0               35m     192.168.221.127   master-m003   <none>           <none>
2026-01-05 14:49:08,805 [INFO]   argo          workflow-controller-ccbd949dc-t4rx9                     1/1     Running     0               79s     192.168.195.243   worker-w002   <none>           <none>
2026-01-05 14:49:08,805 [INFO]   default       resilience-heft-1-201507-qflcd                          1/1     Running     0               4m      192.168.221.71    master-m003   <none>           <none>
2026-01-05 14:49:08,805 [INFO]   kube-system   calico-kube-controllers-7498b9bb4c-xd7tb                1/1     Running     0               38h     192.168.191.108   worker-w006   <none>           <none>
2026-01-05 14:49:08,805 [INFO]   kube-system   calico-node-4zhd4                                       1/1     Running     0               10d     192.168.56.105    worker-w002   <none>           <none>
2026-01-05 14:49:08,805 [INFO]   kube-system   calico-node-75nx6                                       1/1     Running     0               10d     192.168.56.109    worker-w006   <none>           <none>
2026-01-05 14:49:08,805 [INFO]   kube-system   calico-node-7lkdq                                       1/1     Running     0               10d     192.168.56.104    worker-w001   <none>           <none>
2026-01-05 14:49:08,805 [INFO]   kube-system   calico-node-85f8c                                       1/1     Running     0               10d     192.168.56.102    master-m002   <none>           <none>
2026-01-05 14:49:08,805 [INFO]   kube-system   calico-node-j8nb9                                       1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:49:08,805 [INFO]   kube-system   calico-node-lbcb2                                       1/1     Running     0               10d     192.168.56.106    worker-w003   <none>           <none>
2026-01-05 14:49:08,805 [INFO]   kube-system   calico-node-wlr5v                                       1/1     Running     0               10d     192.168.56.107    worker-w004   <none>           <none>
2026-01-05 14:49:08,805 [INFO]   kube-system   calico-node-xnzjw                                       1/1     Running     0               10d     192.168.56.103    master-m003   <none>           <none>
2026-01-05 14:49:08,805 [INFO]   kube-system   calico-node-xsltn                                       1/1     Running     0               10d     192.168.56.108    worker-w005   <none>           <none>
2026-01-05 14:49:08,805 [INFO]   kube-system   coredns-668d6bf9bc-2f74f                                1/1     Running     0               39h     192.168.15.230    worker-w005   <none>           <none>
2026-01-05 14:49:08,805 [INFO]   kube-system   coredns-668d6bf9bc-ssl9h                                1/1     Running     0               39h     192.168.132.165   worker-w001   <none>           <none>
2026-01-05 14:49:08,805 [INFO]   kube-system   etcd-master-m001                                        1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:49:08,805 [INFO]   kube-system   kube-apiserver-master-m001                              1/1     Running     1 (2d11h ago)   10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:49:08,805 [INFO]   kube-system   kube-controller-manager-master-m001                     1/1     Running     12 (33m ago)    10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:49:08,805 [INFO]   kube-system   kube-proxy-5lzhj                                        1/1     Running     0               10d     192.168.56.102    master-m002   <none>           <none>
2026-01-05 14:49:08,806 [INFO]   kube-system   kube-proxy-7jqkv                                        1/1     Running     0               10d     192.168.56.109    worker-w006   <none>           <none>
2026-01-05 14:49:08,806 [INFO]   kube-system   kube-proxy-cv5dt                                        1/1     Running     0               10d     192.168.56.108    worker-w005   <none>           <none>
2026-01-05 14:49:08,806 [INFO]   kube-system   kube-proxy-fvpmr                                        1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:49:08,806 [INFO]   kube-system   kube-proxy-hgs5z                                        1/1     Running     0               10d     192.168.56.106    worker-w003   <none>           <none>
2026-01-05 14:49:08,806 [INFO]   kube-system   kube-proxy-kmgqr                                        1/1     Running     0               10d     192.168.56.105    worker-w002   <none>           <none>
2026-01-05 14:49:08,806 [INFO]   kube-system   kube-proxy-rdbz5                                        1/1     Running     0               10d     192.168.56.107    worker-w004   <none>           <none>
2026-01-05 14:49:08,806 [INFO]   kube-system   kube-proxy-w8mnb                                        1/1     Running     0               10d     192.168.56.103    master-m003   <none>           <none>
2026-01-05 14:49:08,806 [INFO]   kube-system   kube-proxy-x9jxr                                        1/1     Running     0               10d     192.168.56.104    worker-w001   <none>           <none>
2026-01-05 14:49:08,806 [INFO]   kube-system   kube-scheduler-master-m001                              1/1     Running     11 (33m ago)    10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:49:08,806 [INFO] 
Pod distribution by node:
2026-01-05 14:49:08,920 [INFO]   Node master-m003: 33 pods
2026-01-05 14:49:08,921 [INFO]   Node worker-w001: 13 pods
2026-01-05 14:49:08,921 [INFO]   Node worker-w005: 13 pods
2026-01-05 14:49:08,921 [INFO]   Node worker-w002: 3 pods
2026-01-05 14:49:08,921 [INFO]   Node worker-w006: 3 pods
2026-01-05 14:49:08,921 [INFO]   Node master-m002: 2 pods
2026-01-05 14:49:08,921 [INFO]   Node master-m001: 3 pods
2026-01-05 14:49:08,921 [INFO]   Node worker-w003: 2 pods
2026-01-05 14:49:08,921 [INFO]   Node worker-w004: 2 pods
2026-01-05 14:49:08,921 [INFO]   Node 10d: 3 pods
2026-01-05 14:49:08,921 [INFO] 
Filtering for simulation services:
2026-01-05 14:49:09,007 [INFO] Node master-m001 is Ready
2026-01-05 14:49:09,008 [INFO] Node master-m002 is Ready
2026-01-05 14:49:09,008 [INFO] Node master-m003 is Ready
2026-01-05 14:49:09,008 [INFO] Node worker-w001 is Ready
2026-01-05 14:49:09,008 [INFO] Node worker-w002 is Ready
2026-01-05 14:49:09,008 [INFO] Node worker-w003 is Ready
2026-01-05 14:49:09,008 [INFO] Node worker-w004 is Ready
2026-01-05 14:49:09,008 [INFO] Node worker-w005 is Ready
2026-01-05 14:49:09,008 [INFO] Node worker-w006 is Ready
2026-01-05 14:49:09,011 [WARNING] No pods found for etcd-sim
2026-01-05 14:49:09,013 [WARNING] No pods found for postgres-sim
2026-01-05 14:49:09,016 [WARNING] No pods found for redis-sim
2026-01-05 14:49:09,018 [WARNING] No pods found for nginx-sim
2026-01-05 14:49:09,020 [WARNING] No pods found for auth-sim
2026-01-05 14:49:09,020 [INFO] Completed full health check
2026-01-05 14:49:09,020 [INFO] Simulating node recovery for master-m002 using Kubernetes API
2026-01-05 14:49:09,036 [INFO] Removed simulated-failure taint from node master-m002
2026-01-05 14:49:09,047 [INFO] Node master-m002 uncordoned and ready
2026-01-05 14:49:09,047 [INFO] Node master-m002 powered on (delay 5s)
2026-01-05 14:49:14,053 [INFO] Simulating node recovery for worker-w003 using Kubernetes API
2026-01-05 14:49:14,070 [INFO] Removed simulated-failure taint from node worker-w003
2026-01-05 14:49:14,080 [INFO] Node worker-w003 uncordoned and ready
2026-01-05 14:49:14,080 [INFO] Node worker-w003 powered on (delay 5s)
2026-01-05 14:49:19,098 [INFO] Simulating node recovery for worker-w004 using Kubernetes API
2026-01-05 14:49:19,116 [INFO] Removed simulated-failure taint from node worker-w004
2026-01-05 14:49:19,129 [INFO] Node worker-w004 uncordoned and ready
2026-01-05 14:49:19,130 [INFO] Node worker-w004 powered on (delay 5s)
2026-01-05 14:49:24,130 [INFO] Waiting 60 seconds for the cluster to stabilize after recovery...
2026-01-05 14:50:24,190 [INFO] Running final health check
2026-01-05 14:50:24,190 [INFO] Starting full health check
2026-01-05 14:50:24,190 [INFO] 
============ DETAILED NODE STATUS ============
2026-01-05 14:50:24,191 [INFO] Basic Node Information (kubectl get nodes -o wide):
2026-01-05 14:50:24,326 [INFO]   NAME          STATUS   ROLES           AGE   VERSION    INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
2026-01-05 14:50:24,326 [INFO]   master-m001   Ready    control-plane   10d   v1.32.11   192.168.56.101   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:50:24,326 [INFO]   master-m002   Ready    control-plane   10d   v1.32.11   192.168.56.102   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:50:24,326 [INFO]   master-m003   Ready    control-plane   10d   v1.32.11   192.168.56.103   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:50:24,326 [INFO]   worker-w001   Ready    <none>          10d   v1.32.11   192.168.56.104   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:50:24,326 [INFO]   worker-w002   Ready    <none>          10d   v1.32.11   192.168.56.105   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:50:24,326 [INFO]   worker-w003   Ready    <none>          10d   v1.32.11   192.168.56.106   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:50:24,327 [INFO]   worker-w004   Ready    <none>          10d   v1.32.11   192.168.56.107   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:50:24,327 [INFO]   worker-w005   Ready    <none>          10d   v1.32.11   192.168.56.108   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:50:24,327 [INFO]   worker-w006   Ready    <none>          10d   v1.32.11   192.168.56.109   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:50:24,327 [INFO] 
Enhanced Node Status (with taint and cordon indicators):
2026-01-05 14:50:24,327 [INFO]   NAME                STATUS    ROLES           ZONE   CORDONED   TAINTS
2026-01-05 14:50:24,580 [INFO]   master-m001     Ready  ✓ worker         R1    No       node-role.kubernetes.io/control-plane 
2026-01-05 14:50:24,719 [INFO]   master-m002     Ready  ✓ worker         R2    No       None 
2026-01-05 14:50:24,852 [INFO]   master-m003     Ready  ✓ worker         R3    No       None 
2026-01-05 14:50:25,002 [INFO]   worker-w001     Ready  ✓ worker         R1    No       None 
2026-01-05 14:50:25,128 [INFO]   worker-w002     Ready  ✓ worker         R1    No       None 
2026-01-05 14:50:25,265 [INFO]   worker-w003     Ready  ✓ worker         R2    No       None 
2026-01-05 14:50:25,437 [INFO]   worker-w004     Ready  ✓ worker         R2    No       None 
2026-01-05 14:50:25,637 [INFO]   worker-w005     Ready  ✓ worker         R3    No       None 
2026-01-05 14:50:25,794 [INFO]   worker-w006     Ready  ✓ worker         R3    No       None 
2026-01-05 14:50:25,794 [INFO] 
Legend:
2026-01-05 14:50:25,794 [INFO]   ✓ = Node is Ready
2026-01-05 14:50:25,794 [INFO]   ⚠️ = Warning indicator (NotReady, Cordoned, or has simulated-failure taint)
2026-01-05 14:50:25,794 [INFO] ============ DETAILED POD INFORMATION ============
2026-01-05 14:50:25,794 [INFO] Running 'kubectl get pods -o wide' to show detailed pod placement:
2026-01-05 14:50:25,900 [INFO]   NAMESPACE     NAME                                                    READY   STATUS      RESTARTS        AGE     IP                NODE          NOMINATED NODE   READINESS GATES
2026-01-05 14:50:25,901 [INFO]   argo          argo-server-5c69cb69db-gdkl6                            1/1     Running     0               8d      192.168.221.65    master-m003   <none>           <none>
2026-01-05 14:50:25,901 [INFO]   argo          resilience-bench-ptngm-initialize-metrics-940927476     0/2     Completed   0               2d16h   192.168.221.125   master-m003   <none>           <none>
2026-01-05 14:50:25,901 [INFO]   argo          resilience-bench-ptngm-run-health-check-1168793163      0/2     Completed   0               2d16h   192.168.221.67    master-m003   <none>           <none>
2026-01-05 14:50:25,901 [INFO]   argo          resilience-bench-ptngm-run-health-check-1185570782      0/2     Completed   0               2d16h   192.168.221.66    master-m003   <none>           <none>
2026-01-05 14:50:25,901 [INFO]   argo          resilience-bench-ptngm-run-health-check-1202348401      0/2     Completed   0               2d16h   192.168.221.122   master-m003   <none>           <none>
2026-01-05 14:50:25,901 [INFO]   argo          resilience-bench-wf87f-initialize-metrics-3677018408    0/2     Completed   0               2d18h   192.168.221.100   master-m003   <none>           <none>
2026-01-05 14:50:25,901 [INFO]   argo          resilience-bench-wf87f-run-health-check-2914150102      0/2     Completed   0               2d18h   192.168.221.105   master-m003   <none>           <none>
2026-01-05 14:50:25,901 [INFO]   argo          resilience-bench-wf87f-run-health-check-3931060015      0/2     Completed   0               2d18h   192.168.221.104   master-m003   <none>           <none>
2026-01-05 14:50:25,901 [INFO]   argo          resilience-bench-wf87f-run-health-check-3947837634      0/2     Completed   0               2d18h   192.168.221.98    master-m003   <none>           <none>
2026-01-05 14:50:25,901 [INFO]   argo          resilience-bench-wf87f-run-health-check-3964615253      0/2     Completed   0               2d18h   192.168.221.103   master-m003   <none>           <none>
2026-01-05 14:50:25,902 [INFO]   argo          resilience-bench-wf87f-run-node-simulation-4173146970   0/2     Completed   0               2d18h   192.168.221.101   master-m003   <none>           <none>
2026-01-05 14:50:25,902 [INFO]   argo          resilience-bench-wf87f-run-rack-simulation-1356070019   0/2     Completed   0               2d18h   192.168.221.109   master-m003   <none>           <none>
2026-01-05 14:50:25,902 [INFO]   argo          resilience-heft-4cqbx-health-check-1555843180           0/2     Error       0               86m     192.168.132.166   worker-w001   <none>           <none>
2026-01-05 14:50:25,902 [INFO]   argo          resilience-heft-4cqbx-health-check-1589398418           0/2     Error       0               86m     192.168.15.231    worker-w005   <none>           <none>
2026-01-05 14:50:25,902 [INFO]   argo          resilience-heft-4cqbx-heft-initialize-2594215169        0/2     Completed   0               87m     192.168.221.79    master-m003   <none>           <none>
2026-01-05 14:50:25,902 [INFO]   argo          resilience-heft-76kbp-health-check-270747540            0/2     Error       0               78m     192.168.15.237    worker-w005   <none>           <none>
2026-01-05 14:50:25,902 [INFO]   argo          resilience-heft-76kbp-health-check-304302778            0/2     Error       0               78m     192.168.132.172   worker-w001   <none>           <none>
2026-01-05 14:50:25,902 [INFO]   argo          resilience-heft-76kbp-heft-initialize-3574960751        0/2     Completed   0               78m     192.168.221.86    master-m003   <none>           <none>
2026-01-05 14:50:25,902 [INFO]   argo          resilience-heft-b85xr-health-check-2807215411           0/2     Error       0               74m     192.168.132.175   worker-w001   <none>           <none>
2026-01-05 14:50:25,902 [INFO]   argo          resilience-heft-b85xr-health-check-2840770649           0/2     Error       0               74m     192.168.15.240    worker-w005   <none>           <none>
2026-01-05 14:50:25,902 [INFO]   argo          resilience-heft-b85xr-heft-initialize-1754406316        0/2     Completed   0               75m     192.168.221.88    master-m003   <none>           <none>
2026-01-05 14:50:25,902 [INFO]   argo          resilience-heft-fdw9f-health-check-1896357264           0/2     Error       0               80m     192.168.15.235    worker-w005   <none>           <none>
2026-01-05 14:50:25,902 [INFO]   argo          resilience-heft-fdw9f-health-check-1929912502           0/2     Error       0               80m     192.168.132.170   worker-w001   <none>           <none>
2026-01-05 14:50:25,902 [INFO]   argo          resilience-heft-fdw9f-heft-initialize-2651619139        0/2     Completed   0               80m     192.168.221.77    master-m003   <none>           <none>
2026-01-05 14:50:25,902 [INFO]   argo          resilience-heft-gf256-health-check-946619120            0/2     Error       0               79m     192.168.132.171   worker-w001   <none>           <none>
2026-01-05 14:50:25,902 [INFO]   argo          resilience-heft-gf256-health-check-980174358            0/2     Error       0               79m     192.168.15.236    worker-w005   <none>           <none>
2026-01-05 14:50:25,902 [INFO]   argo          resilience-heft-gf256-heft-initialize-1548293021        0/2     Completed   0               79m     192.168.221.84    master-m003   <none>           <none>
2026-01-05 14:50:25,902 [INFO]   argo          resilience-heft-gxwpg-health-check-3445304423           0/2     Error       0               82m     192.168.15.233    worker-w005   <none>           <none>
2026-01-05 14:50:25,903 [INFO]   argo          resilience-heft-gxwpg-health-check-3478859661           0/2     Error       0               82m     192.168.132.168   worker-w001   <none>           <none>
2026-01-05 14:50:25,903 [INFO]   argo          resilience-heft-gxwpg-heft-initialize-2377397022        0/2     Completed   0               83m     192.168.221.81    master-m003   <none>           <none>
2026-01-05 14:50:25,903 [INFO]   argo          resilience-heft-pc2kb-health-check-3258812212           0/2     Error       0               77m     192.168.15.238    worker-w005   <none>           <none>
2026-01-05 14:50:25,903 [INFO]   argo          resilience-heft-pc2kb-health-check-3292367450           0/2     Error       0               77m     192.168.132.173   worker-w001   <none>           <none>
2026-01-05 14:50:25,903 [INFO]   argo          resilience-heft-pc2kb-heft-initialize-321333135         0/2     Completed   0               77m     192.168.221.83    master-m003   <none>           <none>
2026-01-05 14:50:25,903 [INFO]   argo          resilience-heft-qsmch-health-check-283908068            0/2     Error       0               84m     192.168.15.232    worker-w005   <none>           <none>
2026-01-05 14:50:25,903 [INFO]   argo          resilience-heft-qsmch-health-check-317463306            0/2     Error       0               84m     192.168.132.167   worker-w001   <none>           <none>
2026-01-05 14:50:25,903 [INFO]   argo          resilience-heft-qsmch-heft-initialize-1908810015        0/2     Completed   0               84m     192.168.221.82    master-m003   <none>           <none>
2026-01-05 14:50:25,903 [INFO]   argo          resilience-heft-r2w8l-health-check-366927355            0/2     Error       0               76m     192.168.15.239    worker-w005   <none>           <none>
2026-01-05 14:50:25,903 [INFO]   argo          resilience-heft-r2w8l-health-check-400482593            0/2     Error       0               76m     192.168.132.174   worker-w001   <none>           <none>
2026-01-05 14:50:25,903 [INFO]   argo          resilience-heft-r2w8l-heft-initialize-3054048250        0/2     Completed   0               76m     192.168.221.85    master-m003   <none>           <none>
2026-01-05 14:50:25,903 [INFO]   argo          resilience-heft-t988k-health-check-612195296            0/2     Error       0               81m     192.168.15.234    worker-w005   <none>           <none>
2026-01-05 14:50:25,903 [INFO]   argo          resilience-heft-t988k-health-check-645750534            0/2     Error       0               81m     192.168.132.169   worker-w001   <none>           <none>
2026-01-05 14:50:25,903 [INFO]   argo          resilience-heft-t988k-heft-initialize-164752883         0/2     Completed   0               81m     192.168.221.80    master-m003   <none>           <none>
2026-01-05 14:50:25,903 [INFO]   argo          resilience-heft-vlt2m-health-check-2709447919           0/2     Error       0               38m     192.168.221.119   master-m003   <none>           <none>
2026-01-05 14:50:25,903 [INFO]   argo          resilience-heft-vlt2m-health-check-2726225538           0/2     Error       0               38m     192.168.221.120   master-m003   <none>           <none>
2026-01-05 14:50:25,903 [INFO]   argo          resilience-heft-vlt2m-health-check-2743003157           0/2     Error       0               38m     192.168.221.121   master-m003   <none>           <none>
2026-01-05 14:50:25,903 [INFO]   argo          resilience-heft-vlt2m-heft-initialize-840404112         0/2     Completed   0               38m     192.168.221.118   master-m003   <none>           <none>
2026-01-05 14:50:25,903 [INFO]   argo          resilience-heft-x86wd-health-check-2184182048           0/2     Error       0               36m     192.168.221.123   master-m003   <none>           <none>
2026-01-05 14:50:25,903 [INFO]   argo          resilience-heft-x86wd-health-check-2200959667           0/2     Error       0               36m     192.168.221.125   master-m003   <none>           <none>
2026-01-05 14:50:25,903 [INFO]   argo          resilience-heft-x86wd-health-check-2234514905           0/2     Error       0               36m     192.168.221.124   master-m003   <none>           <none>
2026-01-05 14:50:25,904 [INFO]   argo          resilience-heft-x86wd-heft-initialize-2152040770        0/2     Completed   0               36m     192.168.221.127   master-m003   <none>           <none>
2026-01-05 14:50:25,904 [INFO]   argo          workflow-controller-ccbd949dc-t4rx9                     1/1     Running     0               2m36s   192.168.195.243   worker-w002   <none>           <none>
2026-01-05 14:50:25,904 [INFO]   default       resilience-heft-1-201507-qflcd                          1/1     Running     0               5m17s   192.168.221.71    master-m003   <none>           <none>
2026-01-05 14:50:25,904 [INFO]   kube-system   calico-kube-controllers-7498b9bb4c-xd7tb                1/1     Running     0               38h     192.168.191.108   worker-w006   <none>           <none>
2026-01-05 14:50:25,904 [INFO]   kube-system   calico-node-4zhd4                                       1/1     Running     0               10d     192.168.56.105    worker-w002   <none>           <none>
2026-01-05 14:50:25,904 [INFO]   kube-system   calico-node-75nx6                                       1/1     Running     0               10d     192.168.56.109    worker-w006   <none>           <none>
2026-01-05 14:50:25,904 [INFO]   kube-system   calico-node-7lkdq                                       1/1     Running     0               10d     192.168.56.104    worker-w001   <none>           <none>
2026-01-05 14:50:25,904 [INFO]   kube-system   calico-node-85f8c                                       1/1     Running     0               10d     192.168.56.102    master-m002   <none>           <none>
2026-01-05 14:50:25,904 [INFO]   kube-system   calico-node-j8nb9                                       1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:50:25,904 [INFO]   kube-system   calico-node-lbcb2                                       1/1     Running     0               10d     192.168.56.106    worker-w003   <none>           <none>
2026-01-05 14:50:25,904 [INFO]   kube-system   calico-node-wlr5v                                       1/1     Running     0               10d     192.168.56.107    worker-w004   <none>           <none>
2026-01-05 14:50:25,904 [INFO]   kube-system   calico-node-xnzjw                                       1/1     Running     0               10d     192.168.56.103    master-m003   <none>           <none>
2026-01-05 14:50:25,904 [INFO]   kube-system   calico-node-xsltn                                       1/1     Running     0               10d     192.168.56.108    worker-w005   <none>           <none>
2026-01-05 14:50:25,904 [INFO]   kube-system   coredns-668d6bf9bc-2f74f                                1/1     Running     0               39h     192.168.15.230    worker-w005   <none>           <none>
2026-01-05 14:50:25,904 [INFO]   kube-system   coredns-668d6bf9bc-ssl9h                                1/1     Running     0               39h     192.168.132.165   worker-w001   <none>           <none>
2026-01-05 14:50:25,904 [INFO]   kube-system   etcd-master-m001                                        1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:50:25,904 [INFO]   kube-system   kube-apiserver-master-m001                              1/1     Running     1 (2d11h ago)   10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:50:25,905 [INFO]   kube-system   kube-controller-manager-master-m001                     1/1     Running     12 (34m ago)    10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:50:25,905 [INFO]   kube-system   kube-proxy-5lzhj                                        1/1     Running     0               10d     192.168.56.102    master-m002   <none>           <none>
2026-01-05 14:50:25,905 [INFO]   kube-system   kube-proxy-7jqkv                                        1/1     Running     0               10d     192.168.56.109    worker-w006   <none>           <none>
2026-01-05 14:50:25,905 [INFO]   kube-system   kube-proxy-cv5dt                                        1/1     Running     0               10d     192.168.56.108    worker-w005   <none>           <none>
2026-01-05 14:50:25,905 [INFO]   kube-system   kube-proxy-fvpmr                                        1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:50:25,905 [INFO]   kube-system   kube-proxy-hgs5z                                        1/1     Running     0               10d     192.168.56.106    worker-w003   <none>           <none>
2026-01-05 14:50:25,905 [INFO]   kube-system   kube-proxy-kmgqr                                        1/1     Running     0               10d     192.168.56.105    worker-w002   <none>           <none>
2026-01-05 14:50:25,905 [INFO]   kube-system   kube-proxy-rdbz5                                        1/1     Running     0               10d     192.168.56.107    worker-w004   <none>           <none>
2026-01-05 14:50:25,905 [INFO]   kube-system   kube-proxy-w8mnb                                        1/1     Running     0               10d     192.168.56.103    master-m003   <none>           <none>
2026-01-05 14:50:25,905 [INFO]   kube-system   kube-proxy-x9jxr                                        1/1     Running     0               10d     192.168.56.104    worker-w001   <none>           <none>
2026-01-05 14:50:25,905 [INFO]   kube-system   kube-scheduler-master-m001                              1/1     Running     11 (34m ago)    10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:50:25,905 [INFO] 
Pod distribution by node:
2026-01-05 14:50:26,018 [INFO]   Node master-m003: 33 pods
2026-01-05 14:50:26,018 [INFO]   Node worker-w001: 13 pods
2026-01-05 14:50:26,018 [INFO]   Node worker-w005: 13 pods
2026-01-05 14:50:26,019 [INFO]   Node worker-w002: 3 pods
2026-01-05 14:50:26,019 [INFO]   Node worker-w006: 3 pods
2026-01-05 14:50:26,019 [INFO]   Node master-m002: 2 pods
2026-01-05 14:50:26,019 [INFO]   Node master-m001: 3 pods
2026-01-05 14:50:26,019 [INFO]   Node worker-w003: 2 pods
2026-01-05 14:50:26,019 [INFO]   Node worker-w004: 2 pods
2026-01-05 14:50:26,019 [INFO]   Node 10d: 3 pods
2026-01-05 14:50:26,019 [INFO] 
Filtering for simulation services:
2026-01-05 14:50:26,107 [INFO] Node master-m001 is Ready
2026-01-05 14:50:26,107 [INFO] Node master-m002 is Ready
2026-01-05 14:50:26,108 [INFO] Node master-m003 is Ready
2026-01-05 14:50:26,108 [INFO] Node worker-w001 is Ready
2026-01-05 14:50:26,108 [INFO] Node worker-w002 is Ready
2026-01-05 14:50:26,108 [INFO] Node worker-w003 is Ready
2026-01-05 14:50:26,108 [INFO] Node worker-w004 is Ready
2026-01-05 14:50:26,108 [INFO] Node worker-w005 is Ready
2026-01-05 14:50:26,109 [INFO] Node worker-w006 is Ready
2026-01-05 14:50:26,112 [WARNING] No pods found for etcd-sim
2026-01-05 14:50:26,114 [WARNING] No pods found for postgres-sim
2026-01-05 14:50:26,116 [WARNING] No pods found for redis-sim
2026-01-05 14:50:26,118 [WARNING] No pods found for nginx-sim
2026-01-05 14:50:26,120 [WARNING] No pods found for auth-sim
2026-01-05 14:50:26,120 [INFO] Completed full health check
2026-01-05 14:50:26,120 [INFO] Rack R2 has been fully restored
[2026-01-05 14:50:26] TIMING: RACK_SIMULATION completed in 168 seconds

================================================================
 STEP 5: FINAL HEALTH CHECK
 Time: Mon Jan  5 14:50:26 UTC 2026
================================================================
Log directory created/verified: /app/logs
File logging configured successfully
2026-01-05 14:50:26,563 [INFO] Loaded in-cluster Kubernetes config
2026-01-05 14:50:26,564 [INFO] Running on host: resilience-heft-1-201507-qflcd
2026-01-05 14:50:26,564 [INFO] Detected current node: master-m003, zone: R3
2026-01-05 14:50:26,564 [INFO] Checking if we have permissions to modify nodes...
2026-01-05 14:50:26,603 [INFO] Testing permissions using node: master-m001
2026-01-05 14:50:26,611 [INFO] Permission check successful - we can modify nodes
2026-01-05 14:50:26,611 [INFO] Using real Kubernetes API for node control
2026-01-05 14:50:26,612 [INFO] Action received: health-check
2026-01-05 14:50:26,612 [INFO] Stabilization time: 10 seconds
2026-01-05 14:50:26,612 [INFO] Starting full health check
2026-01-05 14:50:26,612 [INFO] 
============ DETAILED NODE STATUS ============
2026-01-05 14:50:26,612 [INFO] Basic Node Information (kubectl get nodes -o wide):
2026-01-05 14:50:26,724 [INFO]   NAME          STATUS   ROLES           AGE   VERSION    INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
2026-01-05 14:50:26,724 [INFO]   master-m001   Ready    control-plane   10d   v1.32.11   192.168.56.101   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:50:26,725 [INFO]   master-m002   Ready    control-plane   10d   v1.32.11   192.168.56.102   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:50:26,725 [INFO]   master-m003   Ready    control-plane   10d   v1.32.11   192.168.56.103   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:50:26,725 [INFO]   worker-w001   Ready    <none>          10d   v1.32.11   192.168.56.104   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:50:26,725 [INFO]   worker-w002   Ready    <none>          10d   v1.32.11   192.168.56.105   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:50:26,725 [INFO]   worker-w003   Ready    <none>          10d   v1.32.11   192.168.56.106   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:50:26,725 [INFO]   worker-w004   Ready    <none>          10d   v1.32.11   192.168.56.107   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:50:26,725 [INFO]   worker-w005   Ready    <none>          10d   v1.32.11   192.168.56.108   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:50:26,725 [INFO]   worker-w006   Ready    <none>          10d   v1.32.11   192.168.56.109   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:50:26,725 [INFO] 
Enhanced Node Status (with taint and cordon indicators):
2026-01-05 14:50:26,725 [INFO]   NAME                STATUS    ROLES           ZONE   CORDONED   TAINTS
2026-01-05 14:50:27,196 [INFO]   master-m001     Ready  ✓ worker         R1    No       node-role.kubernetes.io/control-plane 
2026-01-05 14:50:27,371 [INFO]   master-m002     Ready  ✓ worker         R2    No       None 
2026-01-05 14:50:27,547 [INFO]   master-m003     Ready  ✓ worker         R3    No       None 
2026-01-05 14:50:27,722 [INFO]   worker-w001     Ready  ✓ worker         R1    No       None 
2026-01-05 14:50:27,911 [INFO]   worker-w002     Ready  ✓ worker         R1    No       None 
2026-01-05 14:50:28,083 [INFO]   worker-w003     Ready  ✓ worker         R2    No       None 
2026-01-05 14:50:28,264 [INFO]   worker-w004     Ready  ✓ worker         R2    No       None 
2026-01-05 14:50:28,450 [INFO]   worker-w005     Ready  ✓ worker         R3    No       None 
2026-01-05 14:50:28,588 [INFO]   worker-w006     Ready  ✓ worker         R3    No       None 
2026-01-05 14:50:28,589 [INFO] 
Legend:
2026-01-05 14:50:28,589 [INFO]   ✓ = Node is Ready
2026-01-05 14:50:28,589 [INFO]   ⚠️ = Warning indicator (NotReady, Cordoned, or has simulated-failure taint)
2026-01-05 14:50:28,589 [INFO] ============ DETAILED POD INFORMATION ============
2026-01-05 14:50:28,590 [INFO] Running 'kubectl get pods -o wide' to show detailed pod placement:
2026-01-05 14:50:28,717 [INFO]   NAMESPACE     NAME                                                    READY   STATUS      RESTARTS        AGE     IP                NODE          NOMINATED NODE   READINESS GATES
2026-01-05 14:50:28,718 [INFO]   argo          argo-server-5c69cb69db-gdkl6                            1/1     Running     0               8d      192.168.221.65    master-m003   <none>           <none>
2026-01-05 14:50:28,718 [INFO]   argo          resilience-bench-ptngm-initialize-metrics-940927476     0/2     Completed   0               2d16h   192.168.221.125   master-m003   <none>           <none>
2026-01-05 14:50:28,718 [INFO]   argo          resilience-bench-ptngm-run-health-check-1168793163      0/2     Completed   0               2d16h   192.168.221.67    master-m003   <none>           <none>
2026-01-05 14:50:28,718 [INFO]   argo          resilience-bench-ptngm-run-health-check-1185570782      0/2     Completed   0               2d16h   192.168.221.66    master-m003   <none>           <none>
2026-01-05 14:50:28,718 [INFO]   argo          resilience-bench-ptngm-run-health-check-1202348401      0/2     Completed   0               2d16h   192.168.221.122   master-m003   <none>           <none>
2026-01-05 14:50:28,718 [INFO]   argo          resilience-bench-wf87f-initialize-metrics-3677018408    0/2     Completed   0               2d18h   192.168.221.100   master-m003   <none>           <none>
2026-01-05 14:50:28,718 [INFO]   argo          resilience-bench-wf87f-run-health-check-2914150102      0/2     Completed   0               2d18h   192.168.221.105   master-m003   <none>           <none>
2026-01-05 14:50:28,718 [INFO]   argo          resilience-bench-wf87f-run-health-check-3931060015      0/2     Completed   0               2d18h   192.168.221.104   master-m003   <none>           <none>
2026-01-05 14:50:28,719 [INFO]   argo          resilience-bench-wf87f-run-health-check-3947837634      0/2     Completed   0               2d18h   192.168.221.98    master-m003   <none>           <none>
2026-01-05 14:50:28,719 [INFO]   argo          resilience-bench-wf87f-run-health-check-3964615253      0/2     Completed   0               2d18h   192.168.221.103   master-m003   <none>           <none>
2026-01-05 14:50:28,719 [INFO]   argo          resilience-bench-wf87f-run-node-simulation-4173146970   0/2     Completed   0               2d18h   192.168.221.101   master-m003   <none>           <none>
2026-01-05 14:50:28,719 [INFO]   argo          resilience-bench-wf87f-run-rack-simulation-1356070019   0/2     Completed   0               2d18h   192.168.221.109   master-m003   <none>           <none>
2026-01-05 14:50:28,719 [INFO]   argo          resilience-heft-4cqbx-health-check-1555843180           0/2     Error       0               86m     192.168.132.166   worker-w001   <none>           <none>
2026-01-05 14:50:28,719 [INFO]   argo          resilience-heft-4cqbx-health-check-1589398418           0/2     Error       0               86m     192.168.15.231    worker-w005   <none>           <none>
2026-01-05 14:50:28,719 [INFO]   argo          resilience-heft-4cqbx-heft-initialize-2594215169        0/2     Completed   0               87m     192.168.221.79    master-m003   <none>           <none>
2026-01-05 14:50:28,719 [INFO]   argo          resilience-heft-76kbp-health-check-270747540            0/2     Error       0               78m     192.168.15.237    worker-w005   <none>           <none>
2026-01-05 14:50:28,719 [INFO]   argo          resilience-heft-76kbp-health-check-304302778            0/2     Error       0               78m     192.168.132.172   worker-w001   <none>           <none>
2026-01-05 14:50:28,719 [INFO]   argo          resilience-heft-76kbp-heft-initialize-3574960751        0/2     Completed   0               78m     192.168.221.86    master-m003   <none>           <none>
2026-01-05 14:50:28,719 [INFO]   argo          resilience-heft-b85xr-health-check-2807215411           0/2     Error       0               74m     192.168.132.175   worker-w001   <none>           <none>
2026-01-05 14:50:28,719 [INFO]   argo          resilience-heft-b85xr-health-check-2840770649           0/2     Error       0               74m     192.168.15.240    worker-w005   <none>           <none>
2026-01-05 14:50:28,719 [INFO]   argo          resilience-heft-b85xr-heft-initialize-1754406316        0/2     Completed   0               75m     192.168.221.88    master-m003   <none>           <none>
2026-01-05 14:50:28,719 [INFO]   argo          resilience-heft-fdw9f-health-check-1896357264           0/2     Error       0               80m     192.168.15.235    worker-w005   <none>           <none>
2026-01-05 14:50:28,719 [INFO]   argo          resilience-heft-fdw9f-health-check-1929912502           0/2     Error       0               80m     192.168.132.170   worker-w001   <none>           <none>
2026-01-05 14:50:28,720 [INFO]   argo          resilience-heft-fdw9f-heft-initialize-2651619139        0/2     Completed   0               80m     192.168.221.77    master-m003   <none>           <none>
2026-01-05 14:50:28,720 [INFO]   argo          resilience-heft-gf256-health-check-946619120            0/2     Error       0               79m     192.168.132.171   worker-w001   <none>           <none>
2026-01-05 14:50:28,720 [INFO]   argo          resilience-heft-gf256-health-check-980174358            0/2     Error       0               79m     192.168.15.236    worker-w005   <none>           <none>
2026-01-05 14:50:28,720 [INFO]   argo          resilience-heft-gf256-heft-initialize-1548293021        0/2     Completed   0               79m     192.168.221.84    master-m003   <none>           <none>
2026-01-05 14:50:28,720 [INFO]   argo          resilience-heft-gxwpg-health-check-3445304423           0/2     Error       0               82m     192.168.15.233    worker-w005   <none>           <none>
2026-01-05 14:50:28,720 [INFO]   argo          resilience-heft-gxwpg-health-check-3478859661           0/2     Error       0               82m     192.168.132.168   worker-w001   <none>           <none>
2026-01-05 14:50:28,720 [INFO]   argo          resilience-heft-gxwpg-heft-initialize-2377397022        0/2     Completed   0               83m     192.168.221.81    master-m003   <none>           <none>
2026-01-05 14:50:28,720 [INFO]   argo          resilience-heft-pc2kb-health-check-3258812212           0/2     Error       0               77m     192.168.15.238    worker-w005   <none>           <none>
2026-01-05 14:50:28,720 [INFO]   argo          resilience-heft-pc2kb-health-check-3292367450           0/2     Error       0               77m     192.168.132.173   worker-w001   <none>           <none>
2026-01-05 14:50:28,720 [INFO]   argo          resilience-heft-pc2kb-heft-initialize-321333135         0/2     Completed   0               77m     192.168.221.83    master-m003   <none>           <none>
2026-01-05 14:50:28,720 [INFO]   argo          resilience-heft-qsmch-health-check-283908068            0/2     Error       0               84m     192.168.15.232    worker-w005   <none>           <none>
2026-01-05 14:50:28,720 [INFO]   argo          resilience-heft-qsmch-health-check-317463306            0/2     Error       0               84m     192.168.132.167   worker-w001   <none>           <none>
2026-01-05 14:50:28,720 [INFO]   argo          resilience-heft-qsmch-heft-initialize-1908810015        0/2     Completed   0               84m     192.168.221.82    master-m003   <none>           <none>
2026-01-05 14:50:28,720 [INFO]   argo          resilience-heft-r2w8l-health-check-366927355            0/2     Error       0               76m     192.168.15.239    worker-w005   <none>           <none>
2026-01-05 14:50:28,721 [INFO]   argo          resilience-heft-r2w8l-health-check-400482593            0/2     Error       0               76m     192.168.132.174   worker-w001   <none>           <none>
2026-01-05 14:50:28,721 [INFO]   argo          resilience-heft-r2w8l-heft-initialize-3054048250        0/2     Completed   0               76m     192.168.221.85    master-m003   <none>           <none>
2026-01-05 14:50:28,721 [INFO]   argo          resilience-heft-t988k-health-check-612195296            0/2     Error       0               81m     192.168.15.234    worker-w005   <none>           <none>
2026-01-05 14:50:28,721 [INFO]   argo          resilience-heft-t988k-health-check-645750534            0/2     Error       0               81m     192.168.132.169   worker-w001   <none>           <none>
2026-01-05 14:50:28,721 [INFO]   argo          resilience-heft-t988k-heft-initialize-164752883         0/2     Completed   0               82m     192.168.221.80    master-m003   <none>           <none>
2026-01-05 14:50:28,721 [INFO]   argo          resilience-heft-vlt2m-health-check-2709447919           0/2     Error       0               38m     192.168.221.119   master-m003   <none>           <none>
2026-01-05 14:50:28,721 [INFO]   argo          resilience-heft-vlt2m-health-check-2726225538           0/2     Error       0               38m     192.168.221.120   master-m003   <none>           <none>
2026-01-05 14:50:28,721 [INFO]   argo          resilience-heft-vlt2m-health-check-2743003157           0/2     Error       0               38m     192.168.221.121   master-m003   <none>           <none>
2026-01-05 14:50:28,721 [INFO]   argo          resilience-heft-vlt2m-heft-initialize-840404112         0/2     Completed   0               38m     192.168.221.118   master-m003   <none>           <none>
2026-01-05 14:50:28,721 [INFO]   argo          resilience-heft-x86wd-health-check-2184182048           0/2     Error       0               36m     192.168.221.123   master-m003   <none>           <none>
2026-01-05 14:50:28,722 [INFO]   argo          resilience-heft-x86wd-health-check-2200959667           0/2     Error       0               36m     192.168.221.125   master-m003   <none>           <none>
2026-01-05 14:50:28,722 [INFO]   argo          resilience-heft-x86wd-health-check-2234514905           0/2     Error       0               36m     192.168.221.124   master-m003   <none>           <none>
2026-01-05 14:50:28,722 [INFO]   argo          resilience-heft-x86wd-heft-initialize-2152040770        0/2     Completed   0               36m     192.168.221.127   master-m003   <none>           <none>
2026-01-05 14:50:28,722 [INFO]   argo          workflow-controller-ccbd949dc-t4rx9                     1/1     Running     0               2m39s   192.168.195.243   worker-w002   <none>           <none>
2026-01-05 14:50:28,722 [INFO]   default       resilience-heft-1-201507-qflcd                          1/1     Running     0               5m20s   192.168.221.71    master-m003   <none>           <none>
2026-01-05 14:50:28,722 [INFO]   kube-system   calico-kube-controllers-7498b9bb4c-xd7tb                1/1     Running     0               38h     192.168.191.108   worker-w006   <none>           <none>
2026-01-05 14:50:28,722 [INFO]   kube-system   calico-node-4zhd4                                       1/1     Running     0               10d     192.168.56.105    worker-w002   <none>           <none>
2026-01-05 14:50:28,722 [INFO]   kube-system   calico-node-75nx6                                       1/1     Running     0               10d     192.168.56.109    worker-w006   <none>           <none>
2026-01-05 14:50:28,722 [INFO]   kube-system   calico-node-7lkdq                                       1/1     Running     0               10d     192.168.56.104    worker-w001   <none>           <none>
2026-01-05 14:50:28,723 [INFO]   kube-system   calico-node-85f8c                                       1/1     Running     0               10d     192.168.56.102    master-m002   <none>           <none>
2026-01-05 14:50:28,723 [INFO]   kube-system   calico-node-j8nb9                                       1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:50:28,723 [INFO]   kube-system   calico-node-lbcb2                                       1/1     Running     0               10d     192.168.56.106    worker-w003   <none>           <none>
2026-01-05 14:50:28,723 [INFO]   kube-system   calico-node-wlr5v                                       1/1     Running     0               10d     192.168.56.107    worker-w004   <none>           <none>
2026-01-05 14:50:28,723 [INFO]   kube-system   calico-node-xnzjw                                       1/1     Running     0               10d     192.168.56.103    master-m003   <none>           <none>
2026-01-05 14:50:28,723 [INFO]   kube-system   calico-node-xsltn                                       1/1     Running     0               10d     192.168.56.108    worker-w005   <none>           <none>
2026-01-05 14:50:28,723 [INFO]   kube-system   coredns-668d6bf9bc-2f74f                                1/1     Running     0               39h     192.168.15.230    worker-w005   <none>           <none>
2026-01-05 14:50:28,723 [INFO]   kube-system   coredns-668d6bf9bc-ssl9h                                1/1     Running     0               39h     192.168.132.165   worker-w001   <none>           <none>
2026-01-05 14:50:28,723 [INFO]   kube-system   etcd-master-m001                                        1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:50:28,723 [INFO]   kube-system   kube-apiserver-master-m001                              1/1     Running     1 (2d11h ago)   10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:50:28,724 [INFO]   kube-system   kube-controller-manager-master-m001                     1/1     Running     12 (34m ago)    10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:50:28,724 [INFO]   kube-system   kube-proxy-5lzhj                                        1/1     Running     0               10d     192.168.56.102    master-m002   <none>           <none>
2026-01-05 14:50:28,724 [INFO]   kube-system   kube-proxy-7jqkv                                        1/1     Running     0               10d     192.168.56.109    worker-w006   <none>           <none>
2026-01-05 14:50:28,724 [INFO]   kube-system   kube-proxy-cv5dt                                        1/1     Running     0               10d     192.168.56.108    worker-w005   <none>           <none>
2026-01-05 14:50:28,724 [INFO]   kube-system   kube-proxy-fvpmr                                        1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:50:28,724 [INFO]   kube-system   kube-proxy-hgs5z                                        1/1     Running     0               10d     192.168.56.106    worker-w003   <none>           <none>
2026-01-05 14:50:28,724 [INFO]   kube-system   kube-proxy-kmgqr                                        1/1     Running     0               10d     192.168.56.105    worker-w002   <none>           <none>
2026-01-05 14:50:28,724 [INFO]   kube-system   kube-proxy-rdbz5                                        1/1     Running     0               10d     192.168.56.107    worker-w004   <none>           <none>
2026-01-05 14:50:28,724 [INFO]   kube-system   kube-proxy-w8mnb                                        1/1     Running     0               10d     192.168.56.103    master-m003   <none>           <none>
2026-01-05 14:50:28,724 [INFO]   kube-system   kube-proxy-x9jxr                                        1/1     Running     0               10d     192.168.56.104    worker-w001   <none>           <none>
2026-01-05 14:50:28,725 [INFO]   kube-system   kube-scheduler-master-m001                              1/1     Running     11 (34m ago)    10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:50:28,725 [INFO] 
Pod distribution by node:
2026-01-05 14:50:28,873 [INFO]   Node master-m003: 33 pods
2026-01-05 14:50:28,873 [INFO]   Node worker-w001: 13 pods
2026-01-05 14:50:28,873 [INFO]   Node worker-w005: 13 pods
2026-01-05 14:50:28,874 [INFO]   Node worker-w002: 3 pods
2026-01-05 14:50:28,874 [INFO]   Node worker-w006: 3 pods
2026-01-05 14:50:28,874 [INFO]   Node master-m002: 2 pods
2026-01-05 14:50:28,874 [INFO]   Node master-m001: 3 pods
2026-01-05 14:50:28,874 [INFO]   Node worker-w003: 2 pods
2026-01-05 14:50:28,874 [INFO]   Node worker-w004: 2 pods
2026-01-05 14:50:28,874 [INFO]   Node 10d: 3 pods
2026-01-05 14:50:28,874 [INFO] 
Filtering for simulation services:
2026-01-05 14:50:28,962 [INFO] Node master-m001 is Ready
2026-01-05 14:50:28,962 [INFO] Node master-m002 is Ready
2026-01-05 14:50:28,962 [INFO] Node master-m003 is Ready
2026-01-05 14:50:28,963 [INFO] Node worker-w001 is Ready
2026-01-05 14:50:28,963 [INFO] Node worker-w002 is Ready
2026-01-05 14:50:28,963 [INFO] Node worker-w003 is Ready
2026-01-05 14:50:28,963 [INFO] Node worker-w004 is Ready
2026-01-05 14:50:28,963 [INFO] Node worker-w005 is Ready
2026-01-05 14:50:28,963 [INFO] Node worker-w006 is Ready
2026-01-05 14:50:28,966 [WARNING] No pods found for etcd-sim
2026-01-05 14:50:28,968 [WARNING] No pods found for postgres-sim
2026-01-05 14:50:28,970 [WARNING] No pods found for redis-sim
2026-01-05 14:50:28,972 [WARNING] No pods found for nginx-sim
2026-01-05 14:50:28,974 [WARNING] No pods found for auth-sim
2026-01-05 14:50:28,974 [INFO] Completed full health check
[2026-01-05 14:50:29] TIMING: FINAL_HEALTH_CHECK completed in 3 seconds

================================================================
 HEFT-OPTIMIZED SIMULATION COMPLETE
================================================================
Run ID: heft-native-20260105-144512
Total Duration: 317 seconds
HEFT Exclusions Applied:
  - Node Exclusion: worker-w005
  - Zone Exclusion: R3
Metrics saved to: /app/logs/heft-native-20260105-144512/metrics.txt
Timing saved to: /app/logs/heft-native-20260105-144512/timing.csv
