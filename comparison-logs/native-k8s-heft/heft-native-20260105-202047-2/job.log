================================================================
 HEFT-OPTIMIZED NATIVE KUBERNETES RESILIENCE SIMULATION
 Time: Mon Jan  5 14:50:51 UTC 2026
================================================================
[2026-01-05 14:50:51] Run ID: heft-native-20260105-145051
[2026-01-05 14:50:51] Platform: Native Kubernetes (HEFT-Optimized)
[2026-01-05 14:50:51] HEFT Exclusion Node: worker-w005
[2026-01-05 14:50:51] HEFT Exclusion Zone: R3

================================================================
 STEP 1: PARALLEL HEALTH CHECKS (3x)
 Time: Mon Jan  5 14:50:51 UTC 2026
================================================================
Log directory created/verified: /app/logs
File logging configured successfully
2026-01-05 14:50:51,403 [INFO] Loaded in-cluster Kubernetes config
2026-01-05 14:50:51,403 [INFO] Running on host: resilience-heft-2-202047-tbk8r
2026-01-05 14:50:51,403 [INFO] Detected current node: master-m003, zone: R3
2026-01-05 14:50:51,404 [INFO] Checking if we have permissions to modify nodes...
2026-01-05 14:50:51,438 [INFO] Testing permissions using node: master-m001
2026-01-05 14:50:51,445 [INFO] Permission check successful - we can modify nodes
2026-01-05 14:50:51,446 [INFO] Using real Kubernetes API for node control
2026-01-05 14:50:51,447 [INFO] Action received: health-check
2026-01-05 14:50:51,447 [INFO] Stabilization time: 10 seconds
2026-01-05 14:50:51,447 [INFO] Starting full health check
2026-01-05 14:50:51,447 [INFO] 
============ DETAILED NODE STATUS ============
2026-01-05 14:50:51,447 [INFO] Basic Node Information (kubectl get nodes -o wide):
Log directory created/verified: /app/logs
File logging configured successfully
2026-01-05 14:50:51,619 [INFO] Loaded in-cluster Kubernetes config
2026-01-05 14:50:51,619 [INFO] Running on host: resilience-heft-2-202047-tbk8r
2026-01-05 14:50:51,620 [INFO]   NAME          STATUS   ROLES           AGE   VERSION    INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
2026-01-05 14:50:51,620 [INFO]   master-m001   Ready    control-plane   10d   v1.32.11   192.168.56.101   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:50:51,621 [INFO]   master-m002   Ready    control-plane   10d   v1.32.11   192.168.56.102   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:50:51,621 [INFO]   master-m003   Ready    control-plane   10d   v1.32.11   192.168.56.103   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:50:51,621 [INFO]   worker-w001   Ready    <none>          10d   v1.32.11   192.168.56.104   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:50:51,621 [INFO]   worker-w002   Ready    <none>          10d   v1.32.11   192.168.56.105   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:50:51,621 [INFO]   worker-w003   Ready    <none>          10d   v1.32.11   192.168.56.106   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:50:51,621 [INFO]   worker-w004   Ready    <none>          10d   v1.32.11   192.168.56.107   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:50:51,621 [INFO]   worker-w005   Ready    <none>          10d   v1.32.11   192.168.56.108   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:50:51,621 [INFO]   worker-w006   Ready    <none>          10d   v1.32.11   192.168.56.109   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:50:51,622 [INFO] 
Enhanced Node Status (with taint and cordon indicators):
2026-01-05 14:50:51,622 [INFO]   NAME                STATUS    ROLES           ZONE   CORDONED   TAINTS
2026-01-05 14:50:51,623 [INFO] Detected current node: master-m003, zone: R3
2026-01-05 14:50:51,624 [INFO] Checking if we have permissions to modify nodes...
2026-01-05 14:50:51,671 [INFO] Testing permissions using node: master-m001
Log directory created/verified: /app/logs
File logging configured successfully
2026-01-05 14:50:51,686 [INFO] Loaded in-cluster Kubernetes config
2026-01-05 14:50:51,687 [INFO] Running on host: resilience-heft-2-202047-tbk8r
2026-01-05 14:50:51,687 [INFO] Detected current node: master-m003, zone: R3
2026-01-05 14:50:51,687 [INFO] Checking if we have permissions to modify nodes...
2026-01-05 14:50:51,689 [INFO] Permission check successful - we can modify nodes
2026-01-05 14:50:51,689 [INFO] Using real Kubernetes API for node control
2026-01-05 14:50:51,690 [INFO] Action received: health-check
2026-01-05 14:50:51,690 [INFO] Stabilization time: 10 seconds
2026-01-05 14:50:51,690 [INFO] Starting full health check
2026-01-05 14:50:51,690 [INFO] 
============ DETAILED NODE STATUS ============
2026-01-05 14:50:51,690 [INFO] Basic Node Information (kubectl get nodes -o wide):
2026-01-05 14:50:51,736 [INFO] Testing permissions using node: master-m001
2026-01-05 14:50:51,755 [INFO] Permission check successful - we can modify nodes
2026-01-05 14:50:51,756 [INFO] Using real Kubernetes API for node control
2026-01-05 14:50:51,757 [INFO] Action received: health-check
2026-01-05 14:50:51,757 [INFO] Stabilization time: 10 seconds
2026-01-05 14:50:51,757 [INFO] Starting full health check
2026-01-05 14:50:51,757 [INFO] 
============ DETAILED NODE STATUS ============
2026-01-05 14:50:51,757 [INFO] Basic Node Information (kubectl get nodes -o wide):
2026-01-05 14:50:51,796 [INFO]   NAME          STATUS   ROLES           AGE   VERSION    INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
2026-01-05 14:50:51,797 [INFO]   master-m001   Ready    control-plane   10d   v1.32.11   192.168.56.101   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:50:51,797 [INFO]   master-m002   Ready    control-plane   10d   v1.32.11   192.168.56.102   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:50:51,797 [INFO]   master-m003   Ready    control-plane   10d   v1.32.11   192.168.56.103   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:50:51,797 [INFO]   worker-w001   Ready    <none>          10d   v1.32.11   192.168.56.104   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:50:51,797 [INFO]   worker-w002   Ready    <none>          10d   v1.32.11   192.168.56.105   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:50:51,797 [INFO]   worker-w003   Ready    <none>          10d   v1.32.11   192.168.56.106   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:50:51,798 [INFO]   worker-w004   Ready    <none>          10d   v1.32.11   192.168.56.107   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:50:51,798 [INFO]   worker-w005   Ready    <none>          10d   v1.32.11   192.168.56.108   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:50:51,798 [INFO]   worker-w006   Ready    <none>          10d   v1.32.11   192.168.56.109   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:50:51,798 [INFO] 
Enhanced Node Status (with taint and cordon indicators):
2026-01-05 14:50:51,798 [INFO]   NAME                STATUS    ROLES           ZONE   CORDONED   TAINTS
2026-01-05 14:50:51,858 [INFO]   NAME          STATUS   ROLES           AGE   VERSION    INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
2026-01-05 14:50:51,858 [INFO]   master-m001   Ready    control-plane   10d   v1.32.11   192.168.56.101   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:50:51,858 [INFO]   master-m002   Ready    control-plane   10d   v1.32.11   192.168.56.102   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:50:51,859 [INFO]   master-m003   Ready    control-plane   10d   v1.32.11   192.168.56.103   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:50:51,859 [INFO]   worker-w001   Ready    <none>          10d   v1.32.11   192.168.56.104   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:50:51,859 [INFO]   worker-w002   Ready    <none>          10d   v1.32.11   192.168.56.105   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:50:51,859 [INFO]   worker-w003   Ready    <none>          10d   v1.32.11   192.168.56.106   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:50:51,859 [INFO]   worker-w004   Ready    <none>          10d   v1.32.11   192.168.56.107   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:50:51,859 [INFO]   worker-w005   Ready    <none>          10d   v1.32.11   192.168.56.108   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:50:51,859 [INFO]   worker-w006   Ready    <none>          10d   v1.32.11   192.168.56.109   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:50:51,859 [INFO] 
Enhanced Node Status (with taint and cordon indicators):
2026-01-05 14:50:51,860 [INFO]   NAME                STATUS    ROLES           ZONE   CORDONED   TAINTS
2026-01-05 14:50:52,153 [INFO]   master-m001     Ready  ✓ worker         R1    No       node-role.kubernetes.io/control-plane 
2026-01-05 14:50:52,208 [INFO]   master-m001     Ready  ✓ worker         R1    No       node-role.kubernetes.io/control-plane 
2026-01-05 14:50:52,282 [INFO]   master-m001     Ready  ✓ worker         R1    No       node-role.kubernetes.io/control-plane 
2026-01-05 14:50:52,414 [INFO]   master-m002     Ready  ✓ worker         R2    No       None 
2026-01-05 14:50:52,503 [INFO]   master-m002     Ready  ✓ worker         R2    No       None 
2026-01-05 14:50:52,570 [INFO]   master-m002     Ready  ✓ worker         R2    No       None 
2026-01-05 14:50:52,735 [INFO]   master-m003     Ready  ✓ worker         R3    No       None 
2026-01-05 14:50:52,750 [INFO]   master-m003     Ready  ✓ worker         R3    No       None 
2026-01-05 14:50:52,839 [INFO]   master-m003     Ready  ✓ worker         R3    No       None 
2026-01-05 14:50:53,096 [INFO]   worker-w001     Ready  ✓ worker         R1    No       None 
2026-01-05 14:50:53,124 [INFO]   worker-w001     Ready  ✓ worker         R1    No       None 
2026-01-05 14:50:53,146 [INFO]   worker-w001     Ready  ✓ worker         R1    No       None 
2026-01-05 14:50:53,411 [INFO]   worker-w002     Ready  ✓ worker         R1    No       None 
2026-01-05 14:50:53,451 [INFO]   worker-w002     Ready  ✓ worker         R1    No       None 
2026-01-05 14:50:53,484 [INFO]   worker-w002     Ready  ✓ worker         R1    No       None 
2026-01-05 14:50:53,754 [INFO]   worker-w003     Ready  ✓ worker         R2    No       None 
2026-01-05 14:50:53,825 [INFO]   worker-w003     Ready  ✓ worker         R2    No       None 
2026-01-05 14:50:53,841 [INFO]   worker-w003     Ready  ✓ worker         R2    No       None 
2026-01-05 14:50:54,121 [INFO]   worker-w004     Ready  ✓ worker         R2    No       None 
2026-01-05 14:50:54,123 [INFO]   worker-w004     Ready  ✓ worker         R2    No       None 
2026-01-05 14:50:54,153 [INFO]   worker-w004     Ready  ✓ worker         R2    No       None 
2026-01-05 14:50:54,938 [INFO]   worker-w005     Ready  ✓ worker         R3    No       None 
2026-01-05 14:50:54,960 [INFO]   worker-w005     Ready  ✓ worker         R3    No       None 
2026-01-05 14:50:54,974 [INFO]   worker-w005     Ready  ✓ worker         R3    No       None 
2026-01-05 14:50:55,401 [INFO]   worker-w006     Ready  ✓ worker         R3    No       None 
2026-01-05 14:50:55,401 [INFO] 
Legend:
2026-01-05 14:50:55,401 [INFO]   ✓ = Node is Ready
2026-01-05 14:50:55,402 [INFO]   ⚠️ = Warning indicator (NotReady, Cordoned, or has simulated-failure taint)
2026-01-05 14:50:55,402 [INFO] ============ DETAILED POD INFORMATION ============
2026-01-05 14:50:55,402 [INFO] Running 'kubectl get pods -o wide' to show detailed pod placement:
2026-01-05 14:50:55,408 [INFO]   worker-w006     Ready  ✓ worker         R3    No       None 
2026-01-05 14:50:55,408 [INFO] 
Legend:
2026-01-05 14:50:55,408 [INFO]   ✓ = Node is Ready
2026-01-05 14:50:55,409 [INFO]   ⚠️ = Warning indicator (NotReady, Cordoned, or has simulated-failure taint)
2026-01-05 14:50:55,409 [INFO] ============ DETAILED POD INFORMATION ============
2026-01-05 14:50:55,409 [INFO] Running 'kubectl get pods -o wide' to show detailed pod placement:
2026-01-05 14:50:55,414 [INFO]   worker-w006     Ready  ✓ worker         R3    No       None 
2026-01-05 14:50:55,415 [INFO] 
Legend:
2026-01-05 14:50:55,415 [INFO]   ✓ = Node is Ready
2026-01-05 14:50:55,415 [INFO]   ⚠️ = Warning indicator (NotReady, Cordoned, or has simulated-failure taint)
2026-01-05 14:50:55,415 [INFO] ============ DETAILED POD INFORMATION ============
2026-01-05 14:50:55,415 [INFO] Running 'kubectl get pods -o wide' to show detailed pod placement:
2026-01-05 14:50:55,565 [INFO]   NAMESPACE     NAME                                                    READY   STATUS      RESTARTS        AGE     IP                NODE          NOMINATED NODE   READINESS GATES
2026-01-05 14:50:55,565 [INFO]   argo          argo-server-5c69cb69db-gdkl6                            1/1     Running     0               8d      192.168.221.65    master-m003   <none>           <none>
2026-01-05 14:50:55,565 [INFO]   argo          resilience-bench-ptngm-initialize-metrics-940927476     0/2     Completed   0               2d16h   192.168.221.125   master-m003   <none>           <none>
2026-01-05 14:50:55,565 [INFO]   argo          resilience-bench-ptngm-run-health-check-1168793163      0/2     Completed   0               2d16h   192.168.221.67    master-m003   <none>           <none>
2026-01-05 14:50:55,565 [INFO]   argo          resilience-bench-ptngm-run-health-check-1185570782      0/2     Completed   0               2d16h   192.168.221.66    master-m003   <none>           <none>
2026-01-05 14:50:55,566 [INFO]   argo          resilience-bench-ptngm-run-health-check-1202348401      0/2     Completed   0               2d16h   192.168.221.122   master-m003   <none>           <none>
2026-01-05 14:50:55,566 [INFO]   argo          resilience-bench-wf87f-initialize-metrics-3677018408    0/2     Completed   0               2d18h   192.168.221.100   master-m003   <none>           <none>
2026-01-05 14:50:55,566 [INFO]   argo          resilience-bench-wf87f-run-health-check-2914150102      0/2     Completed   0               2d18h   192.168.221.105   master-m003   <none>           <none>
2026-01-05 14:50:55,566 [INFO]   argo          resilience-bench-wf87f-run-health-check-3931060015      0/2     Completed   0               2d18h   192.168.221.104   master-m003   <none>           <none>
2026-01-05 14:50:55,566 [INFO]   argo          resilience-bench-wf87f-run-health-check-3947837634      0/2     Completed   0               2d18h   192.168.221.98    master-m003   <none>           <none>
2026-01-05 14:50:55,566 [INFO]   argo          resilience-bench-wf87f-run-health-check-3964615253      0/2     Completed   0               2d18h   192.168.221.103   master-m003   <none>           <none>
2026-01-05 14:50:55,566 [INFO]   argo          resilience-bench-wf87f-run-node-simulation-4173146970   0/2     Completed   0               2d18h   192.168.221.101   master-m003   <none>           <none>
2026-01-05 14:50:55,566 [INFO]   argo          resilience-bench-wf87f-run-rack-simulation-1356070019   0/2     Completed   0               2d18h   192.168.221.109   master-m003   <none>           <none>
2026-01-05 14:50:55,567 [INFO]   argo          resilience-heft-4cqbx-health-check-1555843180           0/2     Error       0               87m     192.168.132.166   worker-w001   <none>           <none>
2026-01-05 14:50:55,567 [INFO]   argo          resilience-heft-4cqbx-health-check-1589398418           0/2     Error       0               87m     192.168.15.231    worker-w005   <none>           <none>
2026-01-05 14:50:55,567 [INFO]   argo          resilience-heft-4cqbx-heft-initialize-2594215169        0/2     Completed   0               88m     192.168.221.79    master-m003   <none>           <none>
2026-01-05 14:50:55,567 [INFO]   argo          resilience-heft-76kbp-health-check-270747540            0/2     Error       0               78m     192.168.15.237    worker-w005   <none>           <none>
2026-01-05 14:50:55,567 [INFO]   argo          resilience-heft-76kbp-health-check-304302778            0/2     Error       0               78m     192.168.132.172   worker-w001   <none>           <none>
2026-01-05 14:50:55,567 [INFO]   argo          resilience-heft-76kbp-heft-initialize-3574960751        0/2     Completed   0               79m     192.168.221.86    master-m003   <none>           <none>
2026-01-05 14:50:55,567 [INFO]   argo          resilience-heft-b85xr-health-check-2807215411           0/2     Error       0               75m     192.168.132.175   worker-w001   <none>           <none>
2026-01-05 14:50:55,567 [INFO]   argo          resilience-heft-b85xr-health-check-2840770649           0/2     Error       0               75m     192.168.15.240    worker-w005   <none>           <none>
2026-01-05 14:50:55,567 [INFO]   argo          resilience-heft-b85xr-heft-initialize-1754406316        0/2     Completed   0               75m     192.168.221.88    master-m003   <none>           <none>
2026-01-05 14:50:55,568 [INFO]   argo          resilience-heft-fdw9f-health-check-1896357264           0/2     Error       0               81m     192.168.15.235    worker-w005   <none>           <none>
2026-01-05 14:50:55,568 [INFO]   argo          resilience-heft-fdw9f-health-check-1929912502           0/2     Error       0               81m     192.168.132.170   worker-w001   <none>           <none>
2026-01-05 14:50:55,568 [INFO]   argo          resilience-heft-fdw9f-heft-initialize-2651619139        0/2     Completed   0               81m     192.168.221.77    master-m003   <none>           <none>
2026-01-05 14:50:55,568 [INFO]   argo          resilience-heft-gf256-health-check-946619120            0/2     Error       0               79m     192.168.132.171   worker-w001   <none>           <none>
2026-01-05 14:50:55,568 [INFO]   argo          resilience-heft-gf256-health-check-980174358            0/2     Error       0               79m     192.168.15.236    worker-w005   <none>           <none>
2026-01-05 14:50:55,568 [INFO]   argo          resilience-heft-gf256-heft-initialize-1548293021        0/2     Completed   0               80m     192.168.221.84    master-m003   <none>           <none>
2026-01-05 14:50:55,568 [INFO]   argo          resilience-heft-gxwpg-health-check-3445304423           0/2     Error       0               83m     192.168.15.233    worker-w005   <none>           <none>
2026-01-05 14:50:55,568 [INFO]   argo          resilience-heft-gxwpg-health-check-3478859661           0/2     Error       0               83m     192.168.132.168   worker-w001   <none>           <none>
2026-01-05 14:50:55,568 [INFO]   argo          resilience-heft-gxwpg-heft-initialize-2377397022        0/2     Completed   0               83m     192.168.221.81    master-m003   <none>           <none>
2026-01-05 14:50:55,569 [INFO]   argo          resilience-heft-pc2kb-health-check-3258812212           0/2     Error       0               77m     192.168.15.238    worker-w005   <none>           <none>
2026-01-05 14:50:55,569 [INFO]   argo          resilience-heft-pc2kb-health-check-3292367450           0/2     Error       0               77m     192.168.132.173   worker-w001   <none>           <none>
2026-01-05 14:50:55,569 [INFO]   argo          resilience-heft-pc2kb-heft-initialize-321333135         0/2     Completed   0               78m     192.168.221.83    master-m003   <none>           <none>
2026-01-05 14:50:55,569 [INFO]   argo          resilience-heft-qsmch-health-check-283908068            0/2     Error       0               84m     192.168.15.232    worker-w005   <none>           <none>
2026-01-05 14:50:55,569 [INFO]   argo          resilience-heft-qsmch-health-check-317463306            0/2     Error       0               84m     192.168.132.167   worker-w001   <none>           <none>
2026-01-05 14:50:55,569 [INFO]   argo          resilience-heft-qsmch-heft-initialize-1908810015        0/2     Completed   0               85m     192.168.221.82    master-m003   <none>           <none>
2026-01-05 14:50:55,569 [INFO]   argo          resilience-heft-r2w8l-health-check-366927355            0/2     Error       0               76m     192.168.15.239    worker-w005   <none>           <none>
2026-01-05 14:50:55,569 [INFO]   argo          resilience-heft-r2w8l-health-check-400482593            0/2     Error       0               76m     192.168.132.174   worker-w001   <none>           <none>
2026-01-05 14:50:55,570 [INFO]   argo          resilience-heft-r2w8l-heft-initialize-3054048250        0/2     Completed   0               77m     192.168.221.85    master-m003   <none>           <none>
2026-01-05 14:50:55,570 [INFO]   argo          resilience-heft-t988k-health-check-612195296            0/2     Error       0               81m     192.168.15.234    worker-w005   <none>           <none>
2026-01-05 14:50:55,570 [INFO]   argo          resilience-heft-t988k-health-check-645750534            0/2     Error       0               81m     192.168.132.169   worker-w001   <none>           <none>
2026-01-05 14:50:55,570 [INFO]   argo          resilience-heft-t988k-heft-initialize-164752883         0/2     Completed   0               82m     192.168.221.80    master-m003   <none>           <none>
2026-01-05 14:50:55,570 [INFO]   argo          resilience-heft-vlt2m-health-check-2709447919           0/2     Error       0               38m     192.168.221.119   master-m003   <none>           <none>
2026-01-05 14:50:55,570 [INFO]   argo          resilience-heft-vlt2m-health-check-2726225538           0/2     Error       0               38m     192.168.221.120   master-m003   <none>           <none>
2026-01-05 14:50:55,570 [INFO]   argo          resilience-heft-vlt2m-health-check-2743003157           0/2     Error       0               38m     192.168.221.121   master-m003   <none>           <none>
2026-01-05 14:50:55,570 [INFO]   argo          resilience-heft-vlt2m-heft-initialize-840404112         0/2     Completed   0               38m     192.168.221.118   master-m003   <none>           <none>
2026-01-05 14:50:55,570 [INFO]   argo          resilience-heft-x86wd-health-check-2184182048           0/2     Error       0               36m     192.168.221.123   master-m003   <none>           <none>
2026-01-05 14:50:55,571 [INFO]   argo          resilience-heft-x86wd-health-check-2200959667           0/2     Error       0               36m     192.168.221.125   master-m003   <none>           <none>
2026-01-05 14:50:55,571 [INFO]   argo          resilience-heft-x86wd-health-check-2234514905           0/2     Error       0               36m     192.168.221.124   master-m003   <none>           <none>
2026-01-05 14:50:55,571 [INFO]   argo          resilience-heft-x86wd-heft-initialize-2152040770        0/2     Completed   0               37m     192.168.221.127   master-m003   <none>           <none>
2026-01-05 14:50:55,571 [INFO]   argo          workflow-controller-ccbd949dc-t4rx9                     1/1     Running     0               3m6s    192.168.195.243   worker-w002   <none>           <none>
2026-01-05 14:50:55,571 [INFO]   default       resilience-heft-2-202047-tbk8r                          1/1     Running     0               8s      192.168.221.126   master-m003   <none>           <none>
2026-01-05 14:50:55,571 [INFO]   kube-system   calico-kube-controllers-7498b9bb4c-xd7tb                1/1     Running     0               38h     192.168.191.108   worker-w006   <none>           <none>
2026-01-05 14:50:55,571 [INFO]   kube-system   calico-node-4zhd4                                       1/1     Running     0               10d     192.168.56.105    worker-w002   <none>           <none>
2026-01-05 14:50:55,572 [INFO]   kube-system   calico-node-75nx6                                       1/1     Running     0               10d     192.168.56.109    worker-w006   <none>           <none>
2026-01-05 14:50:55,572 [INFO]   kube-system   calico-node-7lkdq                                       1/1     Running     0               10d     192.168.56.104    worker-w001   <none>           <none>
2026-01-05 14:50:55,572 [INFO]   kube-system   calico-node-85f8c                                       1/1     Running     0               10d     192.168.56.102    master-m002   <none>           <none>
2026-01-05 14:50:55,572 [INFO]   kube-system   calico-node-j8nb9                                       1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:50:55,572 [INFO]   kube-system   calico-node-lbcb2                                       1/1     Running     0               10d     192.168.56.106    worker-w003   <none>           <none>
2026-01-05 14:50:55,572 [INFO]   kube-system   calico-node-wlr5v                                       1/1     Running     0               10d     192.168.56.107    worker-w004   <none>           <none>
2026-01-05 14:50:55,572 [INFO]   kube-system   calico-node-xnzjw                                       1/1     Running     0               10d     192.168.56.103    master-m003   <none>           <none>
2026-01-05 14:50:55,572 [INFO]   kube-system   calico-node-xsltn                                       1/1     Running     0               10d     192.168.56.108    worker-w005   <none>           <none>
2026-01-05 14:50:55,573 [INFO]   kube-system   coredns-668d6bf9bc-2f74f                                1/1     Running     0               39h     192.168.15.230    worker-w005   <none>           <none>
2026-01-05 14:50:55,573 [INFO]   kube-system   coredns-668d6bf9bc-ssl9h                                1/1     Running     0               39h     192.168.132.165   worker-w001   <none>           <none>
2026-01-05 14:50:55,573 [INFO]   kube-system   etcd-master-m001                                        1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:50:55,573 [INFO]   kube-system   kube-apiserver-master-m001                              1/1     Running     1 (2d11h ago)   10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:50:55,573 [INFO]   kube-system   kube-controller-manager-master-m001                     1/1     Running     12 (35m ago)    10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:50:55,573 [INFO]   kube-system   kube-proxy-5lzhj                                        1/1     Running     0               10d     192.168.56.102    master-m002   <none>           <none>
2026-01-05 14:50:55,573 [INFO]   kube-system   kube-proxy-7jqkv                                        1/1     Running     0               10d     192.168.56.109    worker-w006   <none>           <none>
2026-01-05 14:50:55,573 [INFO]   kube-system   kube-proxy-cv5dt                                        1/1     Running     0               10d     192.168.56.108    worker-w005   <none>           <none>
2026-01-05 14:50:55,574 [INFO]   kube-system   kube-proxy-fvpmr                                        1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:50:55,574 [INFO]   kube-system   kube-proxy-hgs5z                                        1/1     Running     0               10d     192.168.56.106    worker-w003   <none>           <none>
2026-01-05 14:50:55,574 [INFO]   kube-system   kube-proxy-kmgqr                                        1/1     Running     0               10d     192.168.56.105    worker-w002   <none>           <none>
2026-01-05 14:50:55,574 [INFO]   kube-system   kube-proxy-rdbz5                                        1/1     Running     0               10d     192.168.56.107    worker-w004   <none>           <none>
2026-01-05 14:50:55,574 [INFO]   kube-system   kube-proxy-w8mnb                                        1/1     Running     0               10d     192.168.56.103    master-m003   <none>           <none>
2026-01-05 14:50:55,574 [INFO]   kube-system   kube-proxy-x9jxr                                        1/1     Running     0               10d     192.168.56.104    worker-w001   <none>           <none>
2026-01-05 14:50:55,574 [INFO]   kube-system   kube-scheduler-master-m001                              1/1     Running     11 (35m ago)    10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:50:55,574 [INFO] 
Pod distribution by node:
2026-01-05 14:50:55,704 [INFO]   NAMESPACE     NAME                                                    READY   STATUS      RESTARTS        AGE     IP                NODE          NOMINATED NODE   READINESS GATES
2026-01-05 14:50:55,704 [INFO]   argo          argo-server-5c69cb69db-gdkl6                            1/1     Running     0               8d      192.168.221.65    master-m003   <none>           <none>
2026-01-05 14:50:55,704 [INFO]   argo          resilience-bench-ptngm-initialize-metrics-940927476     0/2     Completed   0               2d16h   192.168.221.125   master-m003   <none>           <none>
2026-01-05 14:50:55,704 [INFO]   argo          resilience-bench-ptngm-run-health-check-1168793163      0/2     Completed   0               2d16h   192.168.221.67    master-m003   <none>           <none>
2026-01-05 14:50:55,704 [INFO]   argo          resilience-bench-ptngm-run-health-check-1185570782      0/2     Completed   0               2d16h   192.168.221.66    master-m003   <none>           <none>
2026-01-05 14:50:55,705 [INFO]   argo          resilience-bench-ptngm-run-health-check-1202348401      0/2     Completed   0               2d16h   192.168.221.122   master-m003   <none>           <none>
2026-01-05 14:50:55,705 [INFO]   argo          resilience-bench-wf87f-initialize-metrics-3677018408    0/2     Completed   0               2d18h   192.168.221.100   master-m003   <none>           <none>
2026-01-05 14:50:55,705 [INFO]   argo          resilience-bench-wf87f-run-health-check-2914150102      0/2     Completed   0               2d18h   192.168.221.105   master-m003   <none>           <none>
2026-01-05 14:50:55,705 [INFO]   argo          resilience-bench-wf87f-run-health-check-3931060015      0/2     Completed   0               2d18h   192.168.221.104   master-m003   <none>           <none>
2026-01-05 14:50:55,705 [INFO]   argo          resilience-bench-wf87f-run-health-check-3947837634      0/2     Completed   0               2d18h   192.168.221.98    master-m003   <none>           <none>
2026-01-05 14:50:55,705 [INFO]   argo          resilience-bench-wf87f-run-health-check-3964615253      0/2     Completed   0               2d18h   192.168.221.103   master-m003   <none>           <none>
2026-01-05 14:50:55,705 [INFO]   argo          resilience-bench-wf87f-run-node-simulation-4173146970   0/2     Completed   0               2d18h   192.168.221.101   master-m003   <none>           <none>
2026-01-05 14:50:55,705 [INFO]   argo          resilience-bench-wf87f-run-rack-simulation-1356070019   0/2     Completed   0               2d18h   192.168.221.109   master-m003   <none>           <none>
2026-01-05 14:50:55,706 [INFO]   argo          resilience-heft-4cqbx-health-check-1555843180           0/2     Error       0               87m     192.168.132.166   worker-w001   <none>           <none>
2026-01-05 14:50:55,706 [INFO]   argo          resilience-heft-4cqbx-health-check-1589398418           0/2     Error       0               87m     192.168.15.231    worker-w005   <none>           <none>
2026-01-05 14:50:55,706 [INFO]   argo          resilience-heft-4cqbx-heft-initialize-2594215169        0/2     Completed   0               88m     192.168.221.79    master-m003   <none>           <none>
2026-01-05 14:50:55,706 [INFO]   argo          resilience-heft-76kbp-health-check-270747540            0/2     Error       0               78m     192.168.15.237    worker-w005   <none>           <none>
2026-01-05 14:50:55,706 [INFO]   argo          resilience-heft-76kbp-health-check-304302778            0/2     Error       0               78m     192.168.132.172   worker-w001   <none>           <none>
2026-01-05 14:50:55,706 [INFO]   argo          resilience-heft-76kbp-heft-initialize-3574960751        0/2     Completed   0               79m     192.168.221.86    master-m003   <none>           <none>
2026-01-05 14:50:55,706 [INFO]   argo          resilience-heft-b85xr-health-check-2807215411           0/2     Error       0               75m     192.168.132.175   worker-w001   <none>           <none>
2026-01-05 14:50:55,706 [INFO]   argo          resilience-heft-b85xr-health-check-2840770649           0/2     Error       0               75m     192.168.15.240    worker-w005   <none>           <none>
2026-01-05 14:50:55,706 [INFO]   argo          resilience-heft-b85xr-heft-initialize-1754406316        0/2     Completed   0               75m     192.168.221.88    master-m003   <none>           <none>
2026-01-05 14:50:55,706 [INFO]   argo          resilience-heft-fdw9f-health-check-1896357264           0/2     Error       0               81m     192.168.15.235    worker-w005   <none>           <none>
2026-01-05 14:50:55,706 [INFO]   argo          resilience-heft-fdw9f-health-check-1929912502           0/2     Error       0               81m     192.168.132.170   worker-w001   <none>           <none>
2026-01-05 14:50:55,706 [INFO]   argo          resilience-heft-fdw9f-heft-initialize-2651619139        0/2     Completed   0               81m     192.168.221.77    master-m003   <none>           <none>
2026-01-05 14:50:55,706 [INFO]   argo          resilience-heft-gf256-health-check-946619120            0/2     Error       0               79m     192.168.132.171   worker-w001   <none>           <none>
2026-01-05 14:50:55,706 [INFO]   argo          resilience-heft-gf256-health-check-980174358            0/2     Error       0               79m     192.168.15.236    worker-w005   <none>           <none>
2026-01-05 14:50:55,706 [INFO]   argo          resilience-heft-gf256-heft-initialize-1548293021        0/2     Completed   0               80m     192.168.221.84    master-m003   <none>           <none>
2026-01-05 14:50:55,706 [INFO]   argo          resilience-heft-gxwpg-health-check-3445304423           0/2     Error       0               83m     192.168.15.233    worker-w005   <none>           <none>
2026-01-05 14:50:55,706 [INFO]   argo          resilience-heft-gxwpg-health-check-3478859661           0/2     Error       0               83m     192.168.132.168   worker-w001   <none>           <none>
2026-01-05 14:50:55,706 [INFO]   argo          resilience-heft-gxwpg-heft-initialize-2377397022        0/2     Completed   0               83m     192.168.221.81    master-m003   <none>           <none>
2026-01-05 14:50:55,706 [INFO]   argo          resilience-heft-pc2kb-health-check-3258812212           0/2     Error       0               77m     192.168.15.238    worker-w005   <none>           <none>
2026-01-05 14:50:55,706 [INFO]   argo          resilience-heft-pc2kb-health-check-3292367450           0/2     Error       0               77m     192.168.132.173   worker-w001   <none>           <none>
2026-01-05 14:50:55,706 [INFO]   argo          resilience-heft-pc2kb-heft-initialize-321333135         0/2     Completed   0               78m     192.168.221.83    master-m003   <none>           <none>
2026-01-05 14:50:55,707 [INFO]   argo          resilience-heft-qsmch-health-check-283908068            0/2     Error       0               84m     192.168.15.232    worker-w005   <none>           <none>
2026-01-05 14:50:55,707 [INFO]   argo          resilience-heft-qsmch-health-check-317463306            0/2     Error       0               84m     192.168.132.167   worker-w001   <none>           <none>
2026-01-05 14:50:55,707 [INFO]   argo          resilience-heft-qsmch-heft-initialize-1908810015        0/2     Completed   0               85m     192.168.221.82    master-m003   <none>           <none>
2026-01-05 14:50:55,707 [INFO]   argo          resilience-heft-r2w8l-health-check-366927355            0/2     Error       0               76m     192.168.15.239    worker-w005   <none>           <none>
2026-01-05 14:50:55,707 [INFO]   argo          resilience-heft-r2w8l-health-check-400482593            0/2     Error       0               76m     192.168.132.174   worker-w001   <none>           <none>
2026-01-05 14:50:55,707 [INFO]   argo          resilience-heft-r2w8l-heft-initialize-3054048250        0/2     Completed   0               77m     192.168.221.85    master-m003   <none>           <none>
2026-01-05 14:50:55,707 [INFO]   argo          resilience-heft-t988k-health-check-612195296            0/2     Error       0               81m     192.168.15.234    worker-w005   <none>           <none>
2026-01-05 14:50:55,707 [INFO]   argo          resilience-heft-t988k-health-check-645750534            0/2     Error       0               81m     192.168.132.169   worker-w001   <none>           <none>
2026-01-05 14:50:55,707 [INFO]   argo          resilience-heft-t988k-heft-initialize-164752883         0/2     Completed   0               82m     192.168.221.80    master-m003   <none>           <none>
2026-01-05 14:50:55,707 [INFO]   argo          resilience-heft-vlt2m-health-check-2709447919           0/2     Error       0               38m     192.168.221.119   master-m003   <none>           <none>
2026-01-05 14:50:55,707 [INFO]   argo          resilience-heft-vlt2m-health-check-2726225538           0/2     Error       0               38m     192.168.221.120   master-m003   <none>           <none>
2026-01-05 14:50:55,707 [INFO]   argo          resilience-heft-vlt2m-health-check-2743003157           0/2     Error       0               38m     192.168.221.121   master-m003   <none>           <none>
2026-01-05 14:50:55,707 [INFO]   argo          resilience-heft-vlt2m-heft-initialize-840404112         0/2     Completed   0               38m     192.168.221.118   master-m003   <none>           <none>
2026-01-05 14:50:55,707 [INFO]   argo          resilience-heft-x86wd-health-check-2184182048           0/2     Error       0               36m     192.168.221.123   master-m003   <none>           <none>
2026-01-05 14:50:55,707 [INFO]   argo          resilience-heft-x86wd-health-check-2200959667           0/2     Error       0               36m     192.168.221.125   master-m003   <none>           <none>
2026-01-05 14:50:55,707 [INFO]   argo          resilience-heft-x86wd-health-check-2234514905           0/2     Error       0               36m     192.168.221.124   master-m003   <none>           <none>
2026-01-05 14:50:55,707 [INFO]   argo          resilience-heft-x86wd-heft-initialize-2152040770        0/2     Completed   0               37m     192.168.221.127   master-m003   <none>           <none>
2026-01-05 14:50:55,707 [INFO]   argo          workflow-controller-ccbd949dc-t4rx9                     1/1     Running     0               3m6s    192.168.195.243   worker-w002   <none>           <none>
2026-01-05 14:50:55,707 [INFO]   default       resilience-heft-2-202047-tbk8r                          1/1     Running     0               8s      192.168.221.126   master-m003   <none>           <none>
2026-01-05 14:50:55,707 [INFO]   kube-system   calico-kube-controllers-7498b9bb4c-xd7tb                1/1     Running     0               38h     192.168.191.108   worker-w006   <none>           <none>
2026-01-05 14:50:55,708 [INFO]   kube-system   calico-node-4zhd4                                       1/1     Running     0               10d     192.168.56.105    worker-w002   <none>           <none>
2026-01-05 14:50:55,708 [INFO]   kube-system   calico-node-75nx6                                       1/1     Running     0               10d     192.168.56.109    worker-w006   <none>           <none>
2026-01-05 14:50:55,708 [INFO]   kube-system   calico-node-7lkdq                                       1/1     Running     0               10d     192.168.56.104    worker-w001   <none>           <none>
2026-01-05 14:50:55,708 [INFO]   kube-system   calico-node-85f8c                                       1/1     Running     0               10d     192.168.56.102    master-m002   <none>           <none>
2026-01-05 14:50:55,708 [INFO]   kube-system   calico-node-j8nb9                                       1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:50:55,708 [INFO]   kube-system   calico-node-lbcb2                                       1/1     Running     0               10d     192.168.56.106    worker-w003   <none>           <none>
2026-01-05 14:50:55,708 [INFO]   kube-system   calico-node-wlr5v                                       1/1     Running     0               10d     192.168.56.107    worker-w004   <none>           <none>
2026-01-05 14:50:55,708 [INFO]   kube-system   calico-node-xnzjw                                       1/1     Running     0               10d     192.168.56.103    master-m003   <none>           <none>
2026-01-05 14:50:55,708 [INFO]   kube-system   calico-node-xsltn                                       1/1     Running     0               10d     192.168.56.108    worker-w005   <none>           <none>
2026-01-05 14:50:55,708 [INFO]   kube-system   coredns-668d6bf9bc-2f74f                                1/1     Running     0               39h     192.168.15.230    worker-w005   <none>           <none>
2026-01-05 14:50:55,708 [INFO]   kube-system   coredns-668d6bf9bc-ssl9h                                1/1     Running     0               39h     192.168.132.165   worker-w001   <none>           <none>
2026-01-05 14:50:55,708 [INFO]   kube-system   etcd-master-m001                                        1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:50:55,708 [INFO]   kube-system   kube-apiserver-master-m001                              1/1     Running     1 (2d11h ago)   10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:50:55,708 [INFO]   kube-system   kube-controller-manager-master-m001                     1/1     Running     12 (35m ago)    10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:50:55,708 [INFO]   kube-system   kube-proxy-5lzhj                                        1/1     Running     0               10d     192.168.56.102    master-m002   <none>           <none>
2026-01-05 14:50:55,708 [INFO]   kube-system   kube-proxy-7jqkv                                        1/1     Running     0               10d     192.168.56.109    worker-w006   <none>           <none>
2026-01-05 14:50:55,708 [INFO]   kube-system   kube-proxy-cv5dt                                        1/1     Running     0               10d     192.168.56.108    worker-w005   <none>           <none>
2026-01-05 14:50:55,708 [INFO]   kube-system   kube-proxy-fvpmr                                        1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:50:55,708 [INFO]   kube-system   kube-proxy-hgs5z                                        1/1     Running     0               10d     192.168.56.106    worker-w003   <none>           <none>
2026-01-05 14:50:55,708 [INFO]   kube-system   kube-proxy-kmgqr                                        1/1     Running     0               10d     192.168.56.105    worker-w002   <none>           <none>
2026-01-05 14:50:55,709 [INFO]   kube-system   kube-proxy-rdbz5                                        1/1     Running     0               10d     192.168.56.107    worker-w004   <none>           <none>
2026-01-05 14:50:55,709 [INFO]   kube-system   kube-proxy-w8mnb                                        1/1     Running     0               10d     192.168.56.103    master-m003   <none>           <none>
2026-01-05 14:50:55,709 [INFO]   kube-system   kube-proxy-x9jxr                                        1/1     Running     0               10d     192.168.56.104    worker-w001   <none>           <none>
2026-01-05 14:50:55,709 [INFO]   kube-system   kube-scheduler-master-m001                              1/1     Running     11 (35m ago)    10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:50:55,709 [INFO] 
Pod distribution by node:
2026-01-05 14:50:55,714 [INFO]   NAMESPACE     NAME                                                    READY   STATUS      RESTARTS        AGE     IP                NODE          NOMINATED NODE   READINESS GATES
2026-01-05 14:50:55,714 [INFO]   argo          argo-server-5c69cb69db-gdkl6                            1/1     Running     0               8d      192.168.221.65    master-m003   <none>           <none>
2026-01-05 14:50:55,714 [INFO]   argo          resilience-bench-ptngm-initialize-metrics-940927476     0/2     Completed   0               2d16h   192.168.221.125   master-m003   <none>           <none>
2026-01-05 14:50:55,714 [INFO]   argo          resilience-bench-ptngm-run-health-check-1168793163      0/2     Completed   0               2d16h   192.168.221.67    master-m003   <none>           <none>
2026-01-05 14:50:55,714 [INFO]   argo          resilience-bench-ptngm-run-health-check-1185570782      0/2     Completed   0               2d16h   192.168.221.66    master-m003   <none>           <none>
2026-01-05 14:50:55,714 [INFO]   argo          resilience-bench-ptngm-run-health-check-1202348401      0/2     Completed   0               2d16h   192.168.221.122   master-m003   <none>           <none>
2026-01-05 14:50:55,714 [INFO]   argo          resilience-bench-wf87f-initialize-metrics-3677018408    0/2     Completed   0               2d18h   192.168.221.100   master-m003   <none>           <none>
2026-01-05 14:50:55,715 [INFO]   argo          resilience-bench-wf87f-run-health-check-2914150102      0/2     Completed   0               2d18h   192.168.221.105   master-m003   <none>           <none>
2026-01-05 14:50:55,715 [INFO]   argo          resilience-bench-wf87f-run-health-check-3931060015      0/2     Completed   0               2d18h   192.168.221.104   master-m003   <none>           <none>
2026-01-05 14:50:55,715 [INFO]   argo          resilience-bench-wf87f-run-health-check-3947837634      0/2     Completed   0               2d18h   192.168.221.98    master-m003   <none>           <none>
2026-01-05 14:50:55,715 [INFO]   argo          resilience-bench-wf87f-run-health-check-3964615253      0/2     Completed   0               2d18h   192.168.221.103   master-m003   <none>           <none>
2026-01-05 14:50:55,715 [INFO]   argo          resilience-bench-wf87f-run-node-simulation-4173146970   0/2     Completed   0               2d18h   192.168.221.101   master-m003   <none>           <none>
2026-01-05 14:50:55,715 [INFO]   argo          resilience-bench-wf87f-run-rack-simulation-1356070019   0/2     Completed   0               2d18h   192.168.221.109   master-m003   <none>           <none>
2026-01-05 14:50:55,715 [INFO]   argo          resilience-heft-4cqbx-health-check-1555843180           0/2     Error       0               87m     192.168.132.166   worker-w001   <none>           <none>
2026-01-05 14:50:55,715 [INFO]   argo          resilience-heft-4cqbx-health-check-1589398418           0/2     Error       0               87m     192.168.15.231    worker-w005   <none>           <none>
2026-01-05 14:50:55,716 [INFO]   argo          resilience-heft-4cqbx-heft-initialize-2594215169        0/2     Completed   0               88m     192.168.221.79    master-m003   <none>           <none>
2026-01-05 14:50:55,716 [INFO]   argo          resilience-heft-76kbp-health-check-270747540            0/2     Error       0               78m     192.168.15.237    worker-w005   <none>           <none>
2026-01-05 14:50:55,716 [INFO]   argo          resilience-heft-76kbp-health-check-304302778            0/2     Error       0               78m     192.168.132.172   worker-w001   <none>           <none>
2026-01-05 14:50:55,716 [INFO]   argo          resilience-heft-76kbp-heft-initialize-3574960751        0/2     Completed   0               79m     192.168.221.86    master-m003   <none>           <none>
2026-01-05 14:50:55,716 [INFO]   argo          resilience-heft-b85xr-health-check-2807215411           0/2     Error       0               75m     192.168.132.175   worker-w001   <none>           <none>
2026-01-05 14:50:55,716 [INFO]   argo          resilience-heft-b85xr-health-check-2840770649           0/2     Error       0               75m     192.168.15.240    worker-w005   <none>           <none>
2026-01-05 14:50:55,716 [INFO]   argo          resilience-heft-b85xr-heft-initialize-1754406316        0/2     Completed   0               75m     192.168.221.88    master-m003   <none>           <none>
2026-01-05 14:50:55,716 [INFO]   argo          resilience-heft-fdw9f-health-check-1896357264           0/2     Error       0               81m     192.168.15.235    worker-w005   <none>           <none>
2026-01-05 14:50:55,716 [INFO]   argo          resilience-heft-fdw9f-health-check-1929912502           0/2     Error       0               81m     192.168.132.170   worker-w001   <none>           <none>
2026-01-05 14:50:55,716 [INFO]   argo          resilience-heft-fdw9f-heft-initialize-2651619139        0/2     Completed   0               81m     192.168.221.77    master-m003   <none>           <none>
2026-01-05 14:50:55,716 [INFO]   argo          resilience-heft-gf256-health-check-946619120            0/2     Error       0               79m     192.168.132.171   worker-w001   <none>           <none>
2026-01-05 14:50:55,717 [INFO]   argo          resilience-heft-gf256-health-check-980174358            0/2     Error       0               79m     192.168.15.236    worker-w005   <none>           <none>
2026-01-05 14:50:55,717 [INFO]   argo          resilience-heft-gf256-heft-initialize-1548293021        0/2     Completed   0               80m     192.168.221.84    master-m003   <none>           <none>
2026-01-05 14:50:55,717 [INFO]   argo          resilience-heft-gxwpg-health-check-3445304423           0/2     Error       0               83m     192.168.15.233    worker-w005   <none>           <none>
2026-01-05 14:50:55,717 [INFO]   argo          resilience-heft-gxwpg-health-check-3478859661           0/2     Error       0               83m     192.168.132.168   worker-w001   <none>           <none>
2026-01-05 14:50:55,717 [INFO]   argo          resilience-heft-gxwpg-heft-initialize-2377397022        0/2     Completed   0               83m     192.168.221.81    master-m003   <none>           <none>
2026-01-05 14:50:55,717 [INFO]   argo          resilience-heft-pc2kb-health-check-3258812212           0/2     Error       0               77m     192.168.15.238    worker-w005   <none>           <none>
2026-01-05 14:50:55,717 [INFO]   argo          resilience-heft-pc2kb-health-check-3292367450           0/2     Error       0               77m     192.168.132.173   worker-w001   <none>           <none>
2026-01-05 14:50:55,717 [INFO]   argo          resilience-heft-pc2kb-heft-initialize-321333135         0/2     Completed   0               78m     192.168.221.83    master-m003   <none>           <none>
2026-01-05 14:50:55,717 [INFO]   argo          resilience-heft-qsmch-health-check-283908068            0/2     Error       0               84m     192.168.15.232    worker-w005   <none>           <none>
2026-01-05 14:50:55,717 [INFO]   argo          resilience-heft-qsmch-health-check-317463306            0/2     Error       0               84m     192.168.132.167   worker-w001   <none>           <none>
2026-01-05 14:50:55,717 [INFO]   argo          resilience-heft-qsmch-heft-initialize-1908810015        0/2     Completed   0               85m     192.168.221.82    master-m003   <none>           <none>
2026-01-05 14:50:55,717 [INFO]   argo          resilience-heft-r2w8l-health-check-366927355            0/2     Error       0               76m     192.168.15.239    worker-w005   <none>           <none>
2026-01-05 14:50:55,717 [INFO]   argo          resilience-heft-r2w8l-health-check-400482593            0/2     Error       0               76m     192.168.132.174   worker-w001   <none>           <none>
2026-01-05 14:50:55,717 [INFO]   argo          resilience-heft-r2w8l-heft-initialize-3054048250        0/2     Completed   0               77m     192.168.221.85    master-m003   <none>           <none>
2026-01-05 14:50:55,718 [INFO]   argo          resilience-heft-t988k-health-check-612195296            0/2     Error       0               81m     192.168.15.234    worker-w005   <none>           <none>
2026-01-05 14:50:55,718 [INFO]   argo          resilience-heft-t988k-health-check-645750534            0/2     Error       0               81m     192.168.132.169   worker-w001   <none>           <none>
2026-01-05 14:50:55,718 [INFO]   argo          resilience-heft-t988k-heft-initialize-164752883         0/2     Completed   0               82m     192.168.221.80    master-m003   <none>           <none>
2026-01-05 14:50:55,718 [INFO]   argo          resilience-heft-vlt2m-health-check-2709447919           0/2     Error       0               38m     192.168.221.119   master-m003   <none>           <none>
2026-01-05 14:50:55,718 [INFO]   argo          resilience-heft-vlt2m-health-check-2726225538           0/2     Error       0               38m     192.168.221.120   master-m003   <none>           <none>
2026-01-05 14:50:55,718 [INFO]   argo          resilience-heft-vlt2m-health-check-2743003157           0/2     Error       0               38m     192.168.221.121   master-m003   <none>           <none>
2026-01-05 14:50:55,718 [INFO]   argo          resilience-heft-vlt2m-heft-initialize-840404112         0/2     Completed   0               38m     192.168.221.118   master-m003   <none>           <none>
2026-01-05 14:50:55,718 [INFO]   argo          resilience-heft-x86wd-health-check-2184182048           0/2     Error       0               36m     192.168.221.123   master-m003   <none>           <none>
2026-01-05 14:50:55,718 [INFO]   argo          resilience-heft-x86wd-health-check-2200959667           0/2     Error       0               36m     192.168.221.125   master-m003   <none>           <none>
2026-01-05 14:50:55,718 [INFO]   argo          resilience-heft-x86wd-health-check-2234514905           0/2     Error       0               36m     192.168.221.124   master-m003   <none>           <none>
2026-01-05 14:50:55,718 [INFO]   argo          resilience-heft-x86wd-heft-initialize-2152040770        0/2     Completed   0               37m     192.168.221.127   master-m003   <none>           <none>
2026-01-05 14:50:55,718 [INFO]   argo          workflow-controller-ccbd949dc-t4rx9                     1/1     Running     0               3m6s    192.168.195.243   worker-w002   <none>           <none>
2026-01-05 14:50:55,718 [INFO]   default       resilience-heft-2-202047-tbk8r                          1/1     Running     0               8s      192.168.221.126   master-m003   <none>           <none>
2026-01-05 14:50:55,718 [INFO]   kube-system   calico-kube-controllers-7498b9bb4c-xd7tb                1/1     Running     0               38h     192.168.191.108   worker-w006   <none>           <none>
2026-01-05 14:50:55,719 [INFO]   kube-system   calico-node-4zhd4                                       1/1     Running     0               10d     192.168.56.105    worker-w002   <none>           <none>
2026-01-05 14:50:55,719 [INFO]   kube-system   calico-node-75nx6                                       1/1     Running     0               10d     192.168.56.109    worker-w006   <none>           <none>
2026-01-05 14:50:55,719 [INFO]   kube-system   calico-node-7lkdq                                       1/1     Running     0               10d     192.168.56.104    worker-w001   <none>           <none>
2026-01-05 14:50:55,719 [INFO]   kube-system   calico-node-85f8c                                       1/1     Running     0               10d     192.168.56.102    master-m002   <none>           <none>
2026-01-05 14:50:55,719 [INFO]   kube-system   calico-node-j8nb9                                       1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:50:55,719 [INFO]   kube-system   calico-node-lbcb2                                       1/1     Running     0               10d     192.168.56.106    worker-w003   <none>           <none>
2026-01-05 14:50:55,719 [INFO]   kube-system   calico-node-wlr5v                                       1/1     Running     0               10d     192.168.56.107    worker-w004   <none>           <none>
2026-01-05 14:50:55,719 [INFO]   kube-system   calico-node-xnzjw                                       1/1     Running     0               10d     192.168.56.103    master-m003   <none>           <none>
2026-01-05 14:50:55,719 [INFO]   kube-system   calico-node-xsltn                                       1/1     Running     0               10d     192.168.56.108    worker-w005   <none>           <none>
2026-01-05 14:50:55,719 [INFO]   kube-system   coredns-668d6bf9bc-2f74f                                1/1     Running     0               39h     192.168.15.230    worker-w005   <none>           <none>
2026-01-05 14:50:55,719 [INFO]   kube-system   coredns-668d6bf9bc-ssl9h                                1/1     Running     0               39h     192.168.132.165   worker-w001   <none>           <none>
2026-01-05 14:50:55,719 [INFO]   kube-system   etcd-master-m001                                        1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:50:55,719 [INFO]   kube-system   kube-apiserver-master-m001                              1/1     Running     1 (2d11h ago)   10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:50:55,719 [INFO]   kube-system   kube-controller-manager-master-m001                     1/1     Running     12 (35m ago)    10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:50:55,719 [INFO]   kube-system   kube-proxy-5lzhj                                        1/1     Running     0               10d     192.168.56.102    master-m002   <none>           <none>
2026-01-05 14:50:55,720 [INFO]   kube-system   kube-proxy-7jqkv                                        1/1     Running     0               10d     192.168.56.109    worker-w006   <none>           <none>
2026-01-05 14:50:55,720 [INFO]   kube-system   kube-proxy-cv5dt                                        1/1     Running     0               10d     192.168.56.108    worker-w005   <none>           <none>
2026-01-05 14:50:55,720 [INFO]   kube-system   kube-proxy-fvpmr                                        1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:50:55,720 [INFO]   kube-system   kube-proxy-hgs5z                                        1/1     Running     0               10d     192.168.56.106    worker-w003   <none>           <none>
2026-01-05 14:50:55,720 [INFO]   kube-system   kube-proxy-kmgqr                                        1/1     Running     0               10d     192.168.56.105    worker-w002   <none>           <none>
2026-01-05 14:50:55,720 [INFO]   kube-system   kube-proxy-rdbz5                                        1/1     Running     0               10d     192.168.56.107    worker-w004   <none>           <none>
2026-01-05 14:50:55,720 [INFO]   kube-system   kube-proxy-w8mnb                                        1/1     Running     0               10d     192.168.56.103    master-m003   <none>           <none>
2026-01-05 14:50:55,720 [INFO]   kube-system   kube-proxy-x9jxr                                        1/1     Running     0               10d     192.168.56.104    worker-w001   <none>           <none>
2026-01-05 14:50:55,720 [INFO]   kube-system   kube-scheduler-master-m001                              1/1     Running     11 (35m ago)    10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:50:55,720 [INFO] 
Pod distribution by node:
2026-01-05 14:50:55,732 [INFO]   Node master-m003: 33 pods
2026-01-05 14:50:55,733 [INFO]   Node worker-w001: 13 pods
2026-01-05 14:50:55,733 [INFO]   Node worker-w005: 13 pods
2026-01-05 14:50:55,733 [INFO]   Node worker-w002: 3 pods
2026-01-05 14:50:55,733 [INFO]   Node worker-w006: 3 pods
2026-01-05 14:50:55,733 [INFO]   Node master-m002: 2 pods
2026-01-05 14:50:55,733 [INFO]   Node master-m001: 3 pods
2026-01-05 14:50:55,733 [INFO]   Node worker-w003: 2 pods
2026-01-05 14:50:55,733 [INFO]   Node worker-w004: 2 pods
2026-01-05 14:50:55,734 [INFO]   Node 10d: 3 pods
2026-01-05 14:50:55,734 [INFO] 
Filtering for simulation services:
2026-01-05 14:50:55,901 [INFO]   Node master-m003: 33 pods
2026-01-05 14:50:55,902 [INFO]   Node worker-w001: 13 pods
2026-01-05 14:50:55,902 [INFO]   Node worker-w005: 13 pods
2026-01-05 14:50:55,902 [INFO]   Node worker-w002: 3 pods
2026-01-05 14:50:55,902 [INFO]   Node worker-w006: 3 pods
2026-01-05 14:50:55,902 [INFO]   Node master-m002: 2 pods
2026-01-05 14:50:55,902 [INFO]   Node master-m001: 3 pods
2026-01-05 14:50:55,902 [INFO]   Node worker-w003: 2 pods
2026-01-05 14:50:55,902 [INFO]   Node worker-w004: 2 pods
2026-01-05 14:50:55,903 [INFO]   Node 10d: 3 pods
2026-01-05 14:50:55,903 [INFO] 
Filtering for simulation services:
2026-01-05 14:50:55,918 [INFO]   Node master-m003: 33 pods
2026-01-05 14:50:55,918 [INFO]   Node worker-w001: 13 pods
2026-01-05 14:50:55,918 [INFO]   Node worker-w005: 13 pods
2026-01-05 14:50:55,918 [INFO]   Node worker-w002: 3 pods
2026-01-05 14:50:55,918 [INFO]   Node worker-w006: 3 pods
2026-01-05 14:50:55,919 [INFO]   Node master-m002: 2 pods
2026-01-05 14:50:55,919 [INFO]   Node master-m001: 3 pods
2026-01-05 14:50:55,919 [INFO]   Node worker-w003: 2 pods
2026-01-05 14:50:55,919 [INFO]   Node worker-w004: 2 pods
2026-01-05 14:50:55,919 [INFO]   Node 10d: 3 pods
2026-01-05 14:50:55,919 [INFO] 
Filtering for simulation services:
2026-01-05 14:50:55,937 [INFO] Node master-m001 is Ready
2026-01-05 14:50:55,938 [INFO] Node master-m002 is Ready
2026-01-05 14:50:55,938 [INFO] Node master-m003 is Ready
2026-01-05 14:50:55,938 [INFO] Node worker-w001 is Ready
2026-01-05 14:50:55,938 [INFO] Node worker-w002 is Ready
2026-01-05 14:50:55,938 [INFO] Node worker-w003 is Ready
2026-01-05 14:50:55,938 [INFO] Node worker-w004 is Ready
2026-01-05 14:50:55,938 [INFO] Node worker-w005 is Ready
2026-01-05 14:50:55,938 [INFO] Node worker-w006 is Ready
2026-01-05 14:50:55,946 [WARNING] No pods found for etcd-sim
2026-01-05 14:50:55,949 [WARNING] No pods found for postgres-sim
2026-01-05 14:50:55,951 [WARNING] No pods found for redis-sim
2026-01-05 14:50:55,953 [WARNING] No pods found for nginx-sim
2026-01-05 14:50:55,956 [WARNING] No pods found for auth-sim
2026-01-05 14:50:55,956 [INFO] Completed full health check
2026-01-05 14:50:56,069 [INFO] Node master-m001 is Ready
2026-01-05 14:50:56,069 [INFO] Node master-m002 is Ready
2026-01-05 14:50:56,069 [INFO] Node master-m003 is Ready
2026-01-05 14:50:56,069 [INFO] Node worker-w001 is Ready
2026-01-05 14:50:56,069 [INFO] Node worker-w002 is Ready
2026-01-05 14:50:56,070 [INFO] Node worker-w003 is Ready
2026-01-05 14:50:56,070 [INFO] Node worker-w004 is Ready
2026-01-05 14:50:56,070 [INFO] Node worker-w005 is Ready
2026-01-05 14:50:56,070 [INFO] Node worker-w006 is Ready
2026-01-05 14:50:56,073 [WARNING] No pods found for etcd-sim
2026-01-05 14:50:56,075 [WARNING] No pods found for postgres-sim
2026-01-05 14:50:56,076 [INFO] Node master-m001 is Ready
2026-01-05 14:50:56,077 [INFO] Node master-m002 is Ready
2026-01-05 14:50:56,077 [INFO] Node master-m003 is Ready
2026-01-05 14:50:56,077 [INFO] Node worker-w001 is Ready
2026-01-05 14:50:56,077 [INFO] Node worker-w002 is Ready
2026-01-05 14:50:56,077 [INFO] Node worker-w003 is Ready
2026-01-05 14:50:56,077 [INFO] Node worker-w004 is Ready
2026-01-05 14:50:56,077 [INFO] Node worker-w005 is Ready
2026-01-05 14:50:56,077 [INFO] Node worker-w006 is Ready
2026-01-05 14:50:56,078 [WARNING] No pods found for redis-sim
2026-01-05 14:50:56,080 [WARNING] No pods found for nginx-sim
2026-01-05 14:50:56,080 [WARNING] No pods found for etcd-sim
2026-01-05 14:50:56,082 [WARNING] No pods found for auth-sim
2026-01-05 14:50:56,082 [INFO] Completed full health check
2026-01-05 14:50:56,082 [WARNING] No pods found for postgres-sim
2026-01-05 14:50:56,085 [WARNING] No pods found for redis-sim
2026-01-05 14:50:56,087 [WARNING] No pods found for nginx-sim
2026-01-05 14:50:56,088 [WARNING] No pods found for auth-sim
2026-01-05 14:50:56,089 [INFO] Completed full health check
[2026-01-05 14:50:56] TIMING: HEALTH_CHECKS_PARALLEL completed in 5 seconds

================================================================
 STEP 2: NODE FAILURE SIMULATION (HEFT-Aware)
 Excluding node: worker-w005, zone: R3
 Time: Mon Jan  5 14:50:56 UTC 2026
================================================================
Log directory created/verified: /app/logs
File logging configured successfully
2026-01-05 14:50:56,535 [INFO] Loaded in-cluster Kubernetes config
2026-01-05 14:50:56,535 [INFO] Running on host: resilience-heft-2-202047-tbk8r
2026-01-05 14:50:56,535 [INFO] Detected current node: master-m003, zone: R3
2026-01-05 14:50:56,535 [INFO] Checking if we have permissions to modify nodes...
2026-01-05 14:50:56,581 [INFO] Testing permissions using node: master-m001
2026-01-05 14:50:56,589 [INFO] Permission check successful - we can modify nodes
2026-01-05 14:50:56,589 [INFO] Using real Kubernetes API for node control
2026-01-05 14:50:56,590 [INFO] Action received: simulate-node
2026-01-05 14:50:56,590 [INFO] Stabilization time: 60 seconds
2026-01-05 14:50:56,590 [INFO] Simulating node failure: worker-w001
2026-01-05 14:50:56,591 [INFO] Simulating node failure for worker-w001 using Kubernetes API
2026-01-05 14:50:56,605 [INFO] Node worker-w001 cordoned
2026-01-05 14:50:56,620 [INFO] Node worker-w001 tainted with NoExecute
2026-01-05 14:50:56,620 [INFO] Node worker-w001 powered off (delay 5s)
2026-01-05 14:50:56,620 [INFO] Node worker-w001 down for 10 seconds
2026-01-05 14:50:56,621 [INFO] Waiting 60 seconds for the cluster to stabilize before health check...
2026-01-05 14:51:56,693 [INFO] Running health check after node power off
2026-01-05 14:51:56,693 [INFO] Starting full health check
2026-01-05 14:51:56,693 [INFO] 
============ DETAILED NODE STATUS ============
2026-01-05 14:51:56,694 [INFO] Basic Node Information (kubectl get nodes -o wide):
2026-01-05 14:51:56,791 [INFO]   NAME          STATUS                     ROLES           AGE   VERSION    INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
2026-01-05 14:51:56,791 [INFO]   master-m001   Ready                      control-plane   10d   v1.32.11   192.168.56.101   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:51:56,792 [INFO]   master-m002   Ready                      control-plane   10d   v1.32.11   192.168.56.102   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:51:56,792 [INFO]   master-m003   Ready                      control-plane   10d   v1.32.11   192.168.56.103   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:51:56,792 [INFO]   worker-w001   Ready,SchedulingDisabled   <none>          10d   v1.32.11   192.168.56.104   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:51:56,792 [INFO]   worker-w002   Ready                      <none>          10d   v1.32.11   192.168.56.105   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:51:56,792 [INFO]   worker-w003   Ready                      <none>          10d   v1.32.11   192.168.56.106   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:51:56,792 [INFO]   worker-w004   Ready                      <none>          10d   v1.32.11   192.168.56.107   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:51:56,792 [INFO]   worker-w005   Ready                      <none>          10d   v1.32.11   192.168.56.108   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:51:56,792 [INFO]   worker-w006   Ready                      <none>          10d   v1.32.11   192.168.56.109   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:51:56,792 [INFO] 
Enhanced Node Status (with taint and cordon indicators):
2026-01-05 14:51:56,793 [INFO]   NAME                STATUS    ROLES           ZONE   CORDONED   TAINTS
2026-01-05 14:51:57,055 [INFO]   master-m001     Ready  ✓ worker         R1    No       node-role.kubernetes.io/control-plane 
2026-01-05 14:51:57,225 [INFO]   master-m002     Ready  ✓ worker         R2    No       None 
2026-01-05 14:51:57,403 [INFO]   master-m003     Ready  ✓ worker         R3    No       None 
2026-01-05 14:51:57,580 [INFO]   worker-w001     Ready  ✓ worker         R1    YES     ⚠️ simulated-failure, node.kubernetes.io/unschedulable ⚠️
2026-01-05 14:51:57,753 [INFO]   worker-w002     Ready  ✓ worker         R1    No       None 
2026-01-05 14:51:57,922 [INFO]   worker-w003     Ready  ✓ worker         R2    No       None 
2026-01-05 14:51:58,093 [INFO]   worker-w004     Ready  ✓ worker         R2    No       None 
2026-01-05 14:51:58,277 [INFO]   worker-w005     Ready  ✓ worker         R3    No       None 
2026-01-05 14:51:58,451 [INFO]   worker-w006     Ready  ✓ worker         R3    No       None 
2026-01-05 14:51:58,451 [INFO] 
Legend:
2026-01-05 14:51:58,451 [INFO]   ✓ = Node is Ready
2026-01-05 14:51:58,451 [INFO]   ⚠️ = Warning indicator (NotReady, Cordoned, or has simulated-failure taint)
2026-01-05 14:51:58,452 [INFO] ============ DETAILED POD INFORMATION ============
2026-01-05 14:51:58,452 [INFO] Running 'kubectl get pods -o wide' to show detailed pod placement:
2026-01-05 14:51:58,558 [INFO]   NAMESPACE     NAME                                                    READY   STATUS      RESTARTS        AGE     IP                NODE          NOMINATED NODE   READINESS GATES
2026-01-05 14:51:58,559 [INFO]   argo          argo-server-5c69cb69db-gdkl6                            1/1     Running     0               8d      192.168.221.65    master-m003   <none>           <none>
2026-01-05 14:51:58,559 [INFO]   argo          resilience-bench-ptngm-initialize-metrics-940927476     0/2     Completed   0               2d16h   192.168.221.125   master-m003   <none>           <none>
2026-01-05 14:51:58,559 [INFO]   argo          resilience-bench-ptngm-run-health-check-1168793163      0/2     Completed   0               2d16h   192.168.221.67    master-m003   <none>           <none>
2026-01-05 14:51:58,559 [INFO]   argo          resilience-bench-ptngm-run-health-check-1185570782      0/2     Completed   0               2d16h   192.168.221.66    master-m003   <none>           <none>
2026-01-05 14:51:58,559 [INFO]   argo          resilience-bench-ptngm-run-health-check-1202348401      0/2     Completed   0               2d16h   192.168.221.122   master-m003   <none>           <none>
2026-01-05 14:51:58,559 [INFO]   argo          resilience-bench-wf87f-initialize-metrics-3677018408    0/2     Completed   0               2d18h   192.168.221.100   master-m003   <none>           <none>
2026-01-05 14:51:58,559 [INFO]   argo          resilience-bench-wf87f-run-health-check-2914150102      0/2     Completed   0               2d18h   192.168.221.105   master-m003   <none>           <none>
2026-01-05 14:51:58,559 [INFO]   argo          resilience-bench-wf87f-run-health-check-3931060015      0/2     Completed   0               2d18h   192.168.221.104   master-m003   <none>           <none>
2026-01-05 14:51:58,559 [INFO]   argo          resilience-bench-wf87f-run-health-check-3947837634      0/2     Completed   0               2d18h   192.168.221.98    master-m003   <none>           <none>
2026-01-05 14:51:58,559 [INFO]   argo          resilience-bench-wf87f-run-health-check-3964615253      0/2     Completed   0               2d18h   192.168.221.103   master-m003   <none>           <none>
2026-01-05 14:51:58,559 [INFO]   argo          resilience-bench-wf87f-run-node-simulation-4173146970   0/2     Completed   0               2d18h   192.168.221.101   master-m003   <none>           <none>
2026-01-05 14:51:58,559 [INFO]   argo          resilience-bench-wf87f-run-rack-simulation-1356070019   0/2     Completed   0               2d18h   192.168.221.109   master-m003   <none>           <none>
2026-01-05 14:51:58,559 [INFO]   argo          resilience-heft-4cqbx-health-check-1589398418           0/2     Error       0               88m     192.168.15.231    worker-w005   <none>           <none>
2026-01-05 14:51:58,559 [INFO]   argo          resilience-heft-4cqbx-heft-initialize-2594215169        0/2     Completed   0               89m     192.168.221.79    master-m003   <none>           <none>
2026-01-05 14:51:58,559 [INFO]   argo          resilience-heft-76kbp-health-check-270747540            0/2     Error       0               79m     192.168.15.237    worker-w005   <none>           <none>
2026-01-05 14:51:58,559 [INFO]   argo          resilience-heft-76kbp-heft-initialize-3574960751        0/2     Completed   0               80m     192.168.221.86    master-m003   <none>           <none>
2026-01-05 14:51:58,559 [INFO]   argo          resilience-heft-b85xr-health-check-2840770649           0/2     Error       0               76m     192.168.15.240    worker-w005   <none>           <none>
2026-01-05 14:51:58,560 [INFO]   argo          resilience-heft-b85xr-heft-initialize-1754406316        0/2     Completed   0               76m     192.168.221.88    master-m003   <none>           <none>
2026-01-05 14:51:58,560 [INFO]   argo          resilience-heft-fdw9f-health-check-1896357264           0/2     Error       0               82m     192.168.15.235    worker-w005   <none>           <none>
2026-01-05 14:51:58,560 [INFO]   argo          resilience-heft-fdw9f-heft-initialize-2651619139        0/2     Completed   0               82m     192.168.221.77    master-m003   <none>           <none>
2026-01-05 14:51:58,560 [INFO]   argo          resilience-heft-gf256-health-check-980174358            0/2     Error       0               80m     192.168.15.236    worker-w005   <none>           <none>
2026-01-05 14:51:58,560 [INFO]   argo          resilience-heft-gf256-heft-initialize-1548293021        0/2     Completed   0               81m     192.168.221.84    master-m003   <none>           <none>
2026-01-05 14:51:58,560 [INFO]   argo          resilience-heft-gxwpg-health-check-3445304423           0/2     Error       0               84m     192.168.15.233    worker-w005   <none>           <none>
2026-01-05 14:51:58,560 [INFO]   argo          resilience-heft-gxwpg-heft-initialize-2377397022        0/2     Completed   0               84m     192.168.221.81    master-m003   <none>           <none>
2026-01-05 14:51:58,560 [INFO]   argo          resilience-heft-pc2kb-health-check-3258812212           0/2     Error       0               78m     192.168.15.238    worker-w005   <none>           <none>
2026-01-05 14:51:58,560 [INFO]   argo          resilience-heft-pc2kb-heft-initialize-321333135         0/2     Completed   0               79m     192.168.221.83    master-m003   <none>           <none>
2026-01-05 14:51:58,560 [INFO]   argo          resilience-heft-qsmch-health-check-283908068            0/2     Error       0               85m     192.168.15.232    worker-w005   <none>           <none>
2026-01-05 14:51:58,560 [INFO]   argo          resilience-heft-qsmch-heft-initialize-1908810015        0/2     Completed   0               86m     192.168.221.82    master-m003   <none>           <none>
2026-01-05 14:51:58,560 [INFO]   argo          resilience-heft-r2w8l-health-check-366927355            0/2     Error       0               77m     192.168.15.239    worker-w005   <none>           <none>
2026-01-05 14:51:58,560 [INFO]   argo          resilience-heft-r2w8l-heft-initialize-3054048250        0/2     Completed   0               78m     192.168.221.85    master-m003   <none>           <none>
2026-01-05 14:51:58,560 [INFO]   argo          resilience-heft-t988k-health-check-612195296            0/2     Error       0               83m     192.168.15.234    worker-w005   <none>           <none>
2026-01-05 14:51:58,560 [INFO]   argo          resilience-heft-t988k-heft-initialize-164752883         0/2     Completed   0               83m     192.168.221.80    master-m003   <none>           <none>
2026-01-05 14:51:58,560 [INFO]   argo          resilience-heft-vlt2m-health-check-2709447919           0/2     Error       0               39m     192.168.221.119   master-m003   <none>           <none>
2026-01-05 14:51:58,560 [INFO]   argo          resilience-heft-vlt2m-health-check-2726225538           0/2     Error       0               39m     192.168.221.120   master-m003   <none>           <none>
2026-01-05 14:51:58,560 [INFO]   argo          resilience-heft-vlt2m-health-check-2743003157           0/2     Error       0               39m     192.168.221.121   master-m003   <none>           <none>
2026-01-05 14:51:58,561 [INFO]   argo          resilience-heft-vlt2m-heft-initialize-840404112         0/2     Completed   0               39m     192.168.221.118   master-m003   <none>           <none>
2026-01-05 14:51:58,561 [INFO]   argo          resilience-heft-x86wd-health-check-2184182048           0/2     Error       0               37m     192.168.221.123   master-m003   <none>           <none>
2026-01-05 14:51:58,561 [INFO]   argo          resilience-heft-x86wd-health-check-2200959667           0/2     Error       0               37m     192.168.221.125   master-m003   <none>           <none>
2026-01-05 14:51:58,561 [INFO]   argo          resilience-heft-x86wd-health-check-2234514905           0/2     Error       0               37m     192.168.221.124   master-m003   <none>           <none>
2026-01-05 14:51:58,561 [INFO]   argo          resilience-heft-x86wd-heft-initialize-2152040770        0/2     Completed   0               38m     192.168.221.127   master-m003   <none>           <none>
2026-01-05 14:51:58,561 [INFO]   argo          workflow-controller-ccbd949dc-t4rx9                     1/1     Running     0               4m9s    192.168.195.243   worker-w002   <none>           <none>
2026-01-05 14:51:58,561 [INFO]   default       resilience-heft-2-202047-tbk8r                          1/1     Running     0               71s     192.168.221.126   master-m003   <none>           <none>
2026-01-05 14:51:58,561 [INFO]   kube-system   calico-kube-controllers-7498b9bb4c-xd7tb                1/1     Running     0               38h     192.168.191.108   worker-w006   <none>           <none>
2026-01-05 14:51:58,561 [INFO]   kube-system   calico-node-4zhd4                                       1/1     Running     0               10d     192.168.56.105    worker-w002   <none>           <none>
2026-01-05 14:51:58,561 [INFO]   kube-system   calico-node-75nx6                                       1/1     Running     0               10d     192.168.56.109    worker-w006   <none>           <none>
2026-01-05 14:51:58,561 [INFO]   kube-system   calico-node-7lkdq                                       1/1     Running     0               10d     192.168.56.104    worker-w001   <none>           <none>
2026-01-05 14:51:58,561 [INFO]   kube-system   calico-node-85f8c                                       1/1     Running     0               10d     192.168.56.102    master-m002   <none>           <none>
2026-01-05 14:51:58,561 [INFO]   kube-system   calico-node-j8nb9                                       1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:51:58,561 [INFO]   kube-system   calico-node-lbcb2                                       1/1     Running     0               10d     192.168.56.106    worker-w003   <none>           <none>
2026-01-05 14:51:58,561 [INFO]   kube-system   calico-node-wlr5v                                       1/1     Running     0               10d     192.168.56.107    worker-w004   <none>           <none>
2026-01-05 14:51:58,561 [INFO]   kube-system   calico-node-xnzjw                                       1/1     Running     0               10d     192.168.56.103    master-m003   <none>           <none>
2026-01-05 14:51:58,561 [INFO]   kube-system   calico-node-xsltn                                       1/1     Running     0               10d     192.168.56.108    worker-w005   <none>           <none>
2026-01-05 14:51:58,561 [INFO]   kube-system   coredns-668d6bf9bc-2f74f                                1/1     Running     0               39h     192.168.15.230    worker-w005   <none>           <none>
2026-01-05 14:51:58,561 [INFO]   kube-system   coredns-668d6bf9bc-kqndz                                1/1     Running     0               62s     192.168.153.242   worker-w004   <none>           <none>
2026-01-05 14:51:58,561 [INFO]   kube-system   etcd-master-m001                                        1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:51:58,562 [INFO]   kube-system   kube-apiserver-master-m001                              1/1     Running     1 (2d11h ago)   10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:51:58,562 [INFO]   kube-system   kube-controller-manager-master-m001                     1/1     Running     12 (36m ago)    10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:51:58,562 [INFO]   kube-system   kube-proxy-5lzhj                                        1/1     Running     0               10d     192.168.56.102    master-m002   <none>           <none>
2026-01-05 14:51:58,562 [INFO]   kube-system   kube-proxy-7jqkv                                        1/1     Running     0               10d     192.168.56.109    worker-w006   <none>           <none>
2026-01-05 14:51:58,562 [INFO]   kube-system   kube-proxy-cv5dt                                        1/1     Running     0               10d     192.168.56.108    worker-w005   <none>           <none>
2026-01-05 14:51:58,562 [INFO]   kube-system   kube-proxy-fvpmr                                        1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:51:58,562 [INFO]   kube-system   kube-proxy-hgs5z                                        1/1     Running     0               10d     192.168.56.106    worker-w003   <none>           <none>
2026-01-05 14:51:58,562 [INFO]   kube-system   kube-proxy-kmgqr                                        1/1     Running     0               10d     192.168.56.105    worker-w002   <none>           <none>
2026-01-05 14:51:58,562 [INFO]   kube-system   kube-proxy-rdbz5                                        1/1     Running     0               10d     192.168.56.107    worker-w004   <none>           <none>
2026-01-05 14:51:58,562 [INFO]   kube-system   kube-proxy-w8mnb                                        1/1     Running     0               10d     192.168.56.103    master-m003   <none>           <none>
2026-01-05 14:51:58,562 [INFO]   kube-system   kube-proxy-x9jxr                                        1/1     Running     0               10d     192.168.56.104    worker-w001   <none>           <none>
2026-01-05 14:51:58,562 [INFO]   kube-system   kube-scheduler-master-m001                              1/1     Running     11 (36m ago)    10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:51:58,562 [INFO] 
Pod distribution by node:
2026-01-05 14:51:58,663 [INFO]   Node master-m003: 33 pods
2026-01-05 14:51:58,664 [INFO]   Node worker-w005: 13 pods
2026-01-05 14:51:58,664 [INFO]   Node worker-w002: 3 pods
2026-01-05 14:51:58,664 [INFO]   Node worker-w006: 3 pods
2026-01-05 14:51:58,664 [INFO]   Node worker-w001: 2 pods
2026-01-05 14:51:58,664 [INFO]   Node master-m002: 2 pods
2026-01-05 14:51:58,664 [INFO]   Node master-m001: 3 pods
2026-01-05 14:51:58,664 [INFO]   Node worker-w003: 2 pods
2026-01-05 14:51:58,664 [INFO]   Node worker-w004: 3 pods
2026-01-05 14:51:58,664 [INFO]   Node 10d: 3 pods
2026-01-05 14:51:58,664 [INFO] 
Filtering for simulation services:
2026-01-05 14:51:58,756 [INFO] Node master-m001 is Ready
2026-01-05 14:51:58,756 [INFO] Node master-m002 is Ready
2026-01-05 14:51:58,757 [INFO] Node master-m003 is Ready
2026-01-05 14:51:58,757 [INFO] Node worker-w001 is Ready
2026-01-05 14:51:58,757 [INFO] Node worker-w002 is Ready
2026-01-05 14:51:58,757 [INFO] Node worker-w003 is Ready
2026-01-05 14:51:58,757 [INFO] Node worker-w004 is Ready
2026-01-05 14:51:58,757 [INFO] Node worker-w005 is Ready
2026-01-05 14:51:58,757 [INFO] Node worker-w006 is Ready
2026-01-05 14:51:58,760 [WARNING] No pods found for etcd-sim
2026-01-05 14:51:58,762 [WARNING] No pods found for postgres-sim
2026-01-05 14:51:58,765 [WARNING] No pods found for redis-sim
2026-01-05 14:51:58,767 [WARNING] No pods found for nginx-sim
2026-01-05 14:51:58,770 [WARNING] No pods found for auth-sim
2026-01-05 14:51:58,770 [INFO] Completed full health check
2026-01-05 14:52:08,786 [INFO] Running health check before node power on
2026-01-05 14:52:08,786 [INFO] Starting full health check
2026-01-05 14:52:08,786 [INFO] 
============ DETAILED NODE STATUS ============
2026-01-05 14:52:08,786 [INFO] Basic Node Information (kubectl get nodes -o wide):
2026-01-05 14:52:08,856 [INFO]   NAME          STATUS                     ROLES           AGE   VERSION    INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
2026-01-05 14:52:08,856 [INFO]   master-m001   Ready                      control-plane   10d   v1.32.11   192.168.56.101   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:52:08,856 [INFO]   master-m002   Ready                      control-plane   10d   v1.32.11   192.168.56.102   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:52:08,856 [INFO]   master-m003   Ready                      control-plane   10d   v1.32.11   192.168.56.103   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:52:08,856 [INFO]   worker-w001   Ready,SchedulingDisabled   <none>          10d   v1.32.11   192.168.56.104   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:52:08,857 [INFO]   worker-w002   Ready                      <none>          10d   v1.32.11   192.168.56.105   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:52:08,857 [INFO]   worker-w003   Ready                      <none>          10d   v1.32.11   192.168.56.106   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:52:08,857 [INFO]   worker-w004   Ready                      <none>          10d   v1.32.11   192.168.56.107   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:52:08,857 [INFO]   worker-w005   Ready                      <none>          10d   v1.32.11   192.168.56.108   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:52:08,857 [INFO]   worker-w006   Ready                      <none>          10d   v1.32.11   192.168.56.109   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:52:08,857 [INFO] 
Enhanced Node Status (with taint and cordon indicators):
2026-01-05 14:52:08,857 [INFO]   NAME                STATUS    ROLES           ZONE   CORDONED   TAINTS
2026-01-05 14:52:09,919 [INFO]   master-m001     Ready  ✓ worker         R1    No       node-role.kubernetes.io/control-plane 
2026-01-05 14:52:10,234 [INFO]   master-m002     Ready  ✓ worker         R2    No       None 
2026-01-05 14:52:10,394 [INFO]   master-m003     Ready  ✓ worker         R3    No       None 
2026-01-05 14:52:10,575 [INFO]   worker-w001     Ready  ✓ worker         R1    YES     ⚠️ simulated-failure, node.kubernetes.io/unschedulable ⚠️
2026-01-05 14:52:10,724 [INFO]   worker-w002     Ready  ✓ worker         R1    No       None 
2026-01-05 14:52:10,899 [INFO]   worker-w003     Ready  ✓ worker         R2    No       None 
2026-01-05 14:52:11,119 [INFO]   worker-w004     Ready  ✓ worker         R2    No       None 
2026-01-05 14:52:11,292 [INFO]   worker-w005     Ready  ✓ worker         R3    No       None 
2026-01-05 14:52:11,487 [INFO]   worker-w006     Ready  ✓ worker         R3    No       None 
2026-01-05 14:52:11,488 [INFO] 
Legend:
2026-01-05 14:52:11,488 [INFO]   ✓ = Node is Ready
2026-01-05 14:52:11,488 [INFO]   ⚠️ = Warning indicator (NotReady, Cordoned, or has simulated-failure taint)
2026-01-05 14:52:11,488 [INFO] ============ DETAILED POD INFORMATION ============
2026-01-05 14:52:11,488 [INFO] Running 'kubectl get pods -o wide' to show detailed pod placement:
2026-01-05 14:52:11,594 [INFO]   NAMESPACE     NAME                                                    READY   STATUS      RESTARTS        AGE     IP                NODE          NOMINATED NODE   READINESS GATES
2026-01-05 14:52:11,595 [INFO]   argo          argo-server-5c69cb69db-gdkl6                            1/1     Running     0               8d      192.168.221.65    master-m003   <none>           <none>
2026-01-05 14:52:11,595 [INFO]   argo          resilience-bench-ptngm-initialize-metrics-940927476     0/2     Completed   0               2d16h   192.168.221.125   master-m003   <none>           <none>
2026-01-05 14:52:11,595 [INFO]   argo          resilience-bench-ptngm-run-health-check-1168793163      0/2     Completed   0               2d16h   192.168.221.67    master-m003   <none>           <none>
2026-01-05 14:52:11,595 [INFO]   argo          resilience-bench-ptngm-run-health-check-1185570782      0/2     Completed   0               2d16h   192.168.221.66    master-m003   <none>           <none>
2026-01-05 14:52:11,595 [INFO]   argo          resilience-bench-ptngm-run-health-check-1202348401      0/2     Completed   0               2d16h   192.168.221.122   master-m003   <none>           <none>
2026-01-05 14:52:11,595 [INFO]   argo          resilience-bench-wf87f-initialize-metrics-3677018408    0/2     Completed   0               2d18h   192.168.221.100   master-m003   <none>           <none>
2026-01-05 14:52:11,595 [INFO]   argo          resilience-bench-wf87f-run-health-check-2914150102      0/2     Completed   0               2d18h   192.168.221.105   master-m003   <none>           <none>
2026-01-05 14:52:11,595 [INFO]   argo          resilience-bench-wf87f-run-health-check-3931060015      0/2     Completed   0               2d18h   192.168.221.104   master-m003   <none>           <none>
2026-01-05 14:52:11,596 [INFO]   argo          resilience-bench-wf87f-run-health-check-3947837634      0/2     Completed   0               2d18h   192.168.221.98    master-m003   <none>           <none>
2026-01-05 14:52:11,596 [INFO]   argo          resilience-bench-wf87f-run-health-check-3964615253      0/2     Completed   0               2d18h   192.168.221.103   master-m003   <none>           <none>
2026-01-05 14:52:11,596 [INFO]   argo          resilience-bench-wf87f-run-node-simulation-4173146970   0/2     Completed   0               2d18h   192.168.221.101   master-m003   <none>           <none>
2026-01-05 14:52:11,596 [INFO]   argo          resilience-bench-wf87f-run-rack-simulation-1356070019   0/2     Completed   0               2d18h   192.168.221.109   master-m003   <none>           <none>
2026-01-05 14:52:11,596 [INFO]   argo          resilience-heft-4cqbx-health-check-1589398418           0/2     Error       0               88m     192.168.15.231    worker-w005   <none>           <none>
2026-01-05 14:52:11,596 [INFO]   argo          resilience-heft-4cqbx-heft-initialize-2594215169        0/2     Completed   0               89m     192.168.221.79    master-m003   <none>           <none>
2026-01-05 14:52:11,597 [INFO]   argo          resilience-heft-76kbp-health-check-270747540            0/2     Error       0               80m     192.168.15.237    worker-w005   <none>           <none>
2026-01-05 14:52:11,597 [INFO]   argo          resilience-heft-76kbp-heft-initialize-3574960751        0/2     Completed   0               80m     192.168.221.86    master-m003   <none>           <none>
2026-01-05 14:52:11,597 [INFO]   argo          resilience-heft-b85xr-health-check-2840770649           0/2     Error       0               76m     192.168.15.240    worker-w005   <none>           <none>
2026-01-05 14:52:11,597 [INFO]   argo          resilience-heft-b85xr-heft-initialize-1754406316        0/2     Completed   0               77m     192.168.221.88    master-m003   <none>           <none>
2026-01-05 14:52:11,597 [INFO]   argo          resilience-heft-fdw9f-health-check-1896357264           0/2     Error       0               82m     192.168.15.235    worker-w005   <none>           <none>
2026-01-05 14:52:11,597 [INFO]   argo          resilience-heft-fdw9f-heft-initialize-2651619139        0/2     Completed   0               82m     192.168.221.77    master-m003   <none>           <none>
2026-01-05 14:52:11,597 [INFO]   argo          resilience-heft-gf256-health-check-980174358            0/2     Error       0               81m     192.168.15.236    worker-w005   <none>           <none>
2026-01-05 14:52:11,598 [INFO]   argo          resilience-heft-gf256-heft-initialize-1548293021        0/2     Completed   0               81m     192.168.221.84    master-m003   <none>           <none>
2026-01-05 14:52:11,598 [INFO]   argo          resilience-heft-gxwpg-health-check-3445304423           0/2     Error       0               84m     192.168.15.233    worker-w005   <none>           <none>
2026-01-05 14:52:11,598 [INFO]   argo          resilience-heft-gxwpg-heft-initialize-2377397022        0/2     Completed   0               84m     192.168.221.81    master-m003   <none>           <none>
2026-01-05 14:52:11,598 [INFO]   argo          resilience-heft-pc2kb-health-check-3258812212           0/2     Error       0               78m     192.168.15.238    worker-w005   <none>           <none>
2026-01-05 14:52:11,598 [INFO]   argo          resilience-heft-pc2kb-heft-initialize-321333135         0/2     Completed   0               79m     192.168.221.83    master-m003   <none>           <none>
2026-01-05 14:52:11,598 [INFO]   argo          resilience-heft-qsmch-health-check-283908068            0/2     Error       0               85m     192.168.15.232    worker-w005   <none>           <none>
2026-01-05 14:52:11,598 [INFO]   argo          resilience-heft-qsmch-heft-initialize-1908810015        0/2     Completed   0               86m     192.168.221.82    master-m003   <none>           <none>
2026-01-05 14:52:11,598 [INFO]   argo          resilience-heft-r2w8l-health-check-366927355            0/2     Error       0               77m     192.168.15.239    worker-w005   <none>           <none>
2026-01-05 14:52:11,599 [INFO]   argo          resilience-heft-r2w8l-heft-initialize-3054048250        0/2     Completed   0               78m     192.168.221.85    master-m003   <none>           <none>
2026-01-05 14:52:11,599 [INFO]   argo          resilience-heft-t988k-health-check-612195296            0/2     Error       0               83m     192.168.15.234    worker-w005   <none>           <none>
2026-01-05 14:52:11,599 [INFO]   argo          resilience-heft-t988k-heft-initialize-164752883         0/2     Completed   0               83m     192.168.221.80    master-m003   <none>           <none>
2026-01-05 14:52:11,599 [INFO]   argo          resilience-heft-vlt2m-health-check-2709447919           0/2     Error       0               39m     192.168.221.119   master-m003   <none>           <none>
2026-01-05 14:52:11,599 [INFO]   argo          resilience-heft-vlt2m-health-check-2726225538           0/2     Error       0               39m     192.168.221.120   master-m003   <none>           <none>
2026-01-05 14:52:11,599 [INFO]   argo          resilience-heft-vlt2m-health-check-2743003157           0/2     Error       0               39m     192.168.221.121   master-m003   <none>           <none>
2026-01-05 14:52:11,599 [INFO]   argo          resilience-heft-vlt2m-heft-initialize-840404112         0/2     Completed   0               40m     192.168.221.118   master-m003   <none>           <none>
2026-01-05 14:52:11,600 [INFO]   argo          resilience-heft-x86wd-health-check-2184182048           0/2     Error       0               38m     192.168.221.123   master-m003   <none>           <none>
2026-01-05 14:52:11,600 [INFO]   argo          resilience-heft-x86wd-health-check-2200959667           0/2     Error       0               38m     192.168.221.125   master-m003   <none>           <none>
2026-01-05 14:52:11,600 [INFO]   argo          resilience-heft-x86wd-health-check-2234514905           0/2     Error       0               38m     192.168.221.124   master-m003   <none>           <none>
2026-01-05 14:52:11,600 [INFO]   argo          resilience-heft-x86wd-heft-initialize-2152040770        0/2     Completed   0               38m     192.168.221.127   master-m003   <none>           <none>
2026-01-05 14:52:11,600 [INFO]   argo          workflow-controller-ccbd949dc-t4rx9                     1/1     Running     0               4m22s   192.168.195.243   worker-w002   <none>           <none>
2026-01-05 14:52:11,600 [INFO]   default       resilience-heft-2-202047-tbk8r                          1/1     Running     0               84s     192.168.221.126   master-m003   <none>           <none>
2026-01-05 14:52:11,600 [INFO]   kube-system   calico-kube-controllers-7498b9bb4c-xd7tb                1/1     Running     0               38h     192.168.191.108   worker-w006   <none>           <none>
2026-01-05 14:52:11,601 [INFO]   kube-system   calico-node-4zhd4                                       1/1     Running     0               10d     192.168.56.105    worker-w002   <none>           <none>
2026-01-05 14:52:11,601 [INFO]   kube-system   calico-node-75nx6                                       1/1     Running     0               10d     192.168.56.109    worker-w006   <none>           <none>
2026-01-05 14:52:11,601 [INFO]   kube-system   calico-node-7lkdq                                       1/1     Running     0               10d     192.168.56.104    worker-w001   <none>           <none>
2026-01-05 14:52:11,601 [INFO]   kube-system   calico-node-85f8c                                       1/1     Running     0               10d     192.168.56.102    master-m002   <none>           <none>
2026-01-05 14:52:11,601 [INFO]   kube-system   calico-node-j8nb9                                       1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:52:11,601 [INFO]   kube-system   calico-node-lbcb2                                       1/1     Running     0               10d     192.168.56.106    worker-w003   <none>           <none>
2026-01-05 14:52:11,601 [INFO]   kube-system   calico-node-wlr5v                                       1/1     Running     0               10d     192.168.56.107    worker-w004   <none>           <none>
2026-01-05 14:52:11,601 [INFO]   kube-system   calico-node-xnzjw                                       1/1     Running     0               10d     192.168.56.103    master-m003   <none>           <none>
2026-01-05 14:52:11,602 [INFO]   kube-system   calico-node-xsltn                                       1/1     Running     0               10d     192.168.56.108    worker-w005   <none>           <none>
2026-01-05 14:52:11,602 [INFO]   kube-system   coredns-668d6bf9bc-2f74f                                1/1     Running     0               39h     192.168.15.230    worker-w005   <none>           <none>
2026-01-05 14:52:11,602 [INFO]   kube-system   coredns-668d6bf9bc-kqndz                                1/1     Running     0               75s     192.168.153.242   worker-w004   <none>           <none>
2026-01-05 14:52:11,602 [INFO]   kube-system   etcd-master-m001                                        1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:52:11,602 [INFO]   kube-system   kube-apiserver-master-m001                              1/1     Running     1 (2d11h ago)   10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:52:11,602 [INFO]   kube-system   kube-controller-manager-master-m001                     1/1     Running     12 (36m ago)    10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:52:11,602 [INFO]   kube-system   kube-proxy-5lzhj                                        1/1     Running     0               10d     192.168.56.102    master-m002   <none>           <none>
2026-01-05 14:52:11,603 [INFO]   kube-system   kube-proxy-7jqkv                                        1/1     Running     0               10d     192.168.56.109    worker-w006   <none>           <none>
2026-01-05 14:52:11,603 [INFO]   kube-system   kube-proxy-cv5dt                                        1/1     Running     0               10d     192.168.56.108    worker-w005   <none>           <none>
2026-01-05 14:52:11,603 [INFO]   kube-system   kube-proxy-fvpmr                                        1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:52:11,603 [INFO]   kube-system   kube-proxy-hgs5z                                        1/1     Running     0               10d     192.168.56.106    worker-w003   <none>           <none>
2026-01-05 14:52:11,603 [INFO]   kube-system   kube-proxy-kmgqr                                        1/1     Running     0               10d     192.168.56.105    worker-w002   <none>           <none>
2026-01-05 14:52:11,603 [INFO]   kube-system   kube-proxy-rdbz5                                        1/1     Running     0               10d     192.168.56.107    worker-w004   <none>           <none>
2026-01-05 14:52:11,603 [INFO]   kube-system   kube-proxy-w8mnb                                        1/1     Running     0               10d     192.168.56.103    master-m003   <none>           <none>
2026-01-05 14:52:11,604 [INFO]   kube-system   kube-proxy-x9jxr                                        1/1     Running     0               10d     192.168.56.104    worker-w001   <none>           <none>
2026-01-05 14:52:11,604 [INFO]   kube-system   kube-scheduler-master-m001                              1/1     Running     11 (36m ago)    10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:52:11,604 [INFO] 
Pod distribution by node:
2026-01-05 14:52:11,714 [INFO]   Node master-m003: 33 pods
2026-01-05 14:52:11,714 [INFO]   Node worker-w005: 13 pods
2026-01-05 14:52:11,714 [INFO]   Node worker-w002: 3 pods
2026-01-05 14:52:11,714 [INFO]   Node worker-w006: 3 pods
2026-01-05 14:52:11,714 [INFO]   Node worker-w001: 2 pods
2026-01-05 14:52:11,715 [INFO]   Node master-m002: 2 pods
2026-01-05 14:52:11,715 [INFO]   Node master-m001: 3 pods
2026-01-05 14:52:11,715 [INFO]   Node worker-w003: 2 pods
2026-01-05 14:52:11,715 [INFO]   Node worker-w004: 3 pods
2026-01-05 14:52:11,715 [INFO]   Node 10d: 3 pods
2026-01-05 14:52:11,715 [INFO] 
Filtering for simulation services:
2026-01-05 14:52:11,801 [INFO] Node master-m001 is Ready
2026-01-05 14:52:11,801 [INFO] Node master-m002 is Ready
2026-01-05 14:52:11,801 [INFO] Node master-m003 is Ready
2026-01-05 14:52:11,801 [INFO] Node worker-w001 is Ready
2026-01-05 14:52:11,802 [INFO] Node worker-w002 is Ready
2026-01-05 14:52:11,802 [INFO] Node worker-w003 is Ready
2026-01-05 14:52:11,802 [INFO] Node worker-w004 is Ready
2026-01-05 14:52:11,802 [INFO] Node worker-w005 is Ready
2026-01-05 14:52:11,802 [INFO] Node worker-w006 is Ready
2026-01-05 14:52:11,805 [WARNING] No pods found for etcd-sim
2026-01-05 14:52:11,807 [WARNING] No pods found for postgres-sim
2026-01-05 14:52:11,809 [WARNING] No pods found for redis-sim
2026-01-05 14:52:11,811 [WARNING] No pods found for nginx-sim
2026-01-05 14:52:11,813 [WARNING] No pods found for auth-sim
2026-01-05 14:52:11,814 [INFO] Completed full health check
2026-01-05 14:52:11,814 [INFO] Simulating node recovery for worker-w001 using Kubernetes API
2026-01-05 14:52:11,829 [INFO] Removed simulated-failure taint from node worker-w001
2026-01-05 14:52:11,837 [INFO] Node worker-w001 uncordoned and ready
2026-01-05 14:52:11,837 [INFO] Node worker-w001 has been powered back on
2026-01-05 14:52:11,837 [INFO] Waiting 60 seconds for the cluster to stabilize after recovery...
2026-01-05 14:53:11,900 [INFO] Running final health check
2026-01-05 14:53:11,900 [INFO] Starting full health check
2026-01-05 14:53:11,900 [INFO] 
============ DETAILED NODE STATUS ============
2026-01-05 14:53:11,900 [INFO] Basic Node Information (kubectl get nodes -o wide):
2026-01-05 14:53:11,971 [INFO]   NAME          STATUS   ROLES           AGE   VERSION    INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
2026-01-05 14:53:11,972 [INFO]   master-m001   Ready    control-plane   10d   v1.32.11   192.168.56.101   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:53:11,972 [INFO]   master-m002   Ready    control-plane   10d   v1.32.11   192.168.56.102   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:53:11,972 [INFO]   master-m003   Ready    control-plane   10d   v1.32.11   192.168.56.103   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:53:11,972 [INFO]   worker-w001   Ready    <none>          10d   v1.32.11   192.168.56.104   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:53:11,972 [INFO]   worker-w002   Ready    <none>          10d   v1.32.11   192.168.56.105   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:53:11,972 [INFO]   worker-w003   Ready    <none>          10d   v1.32.11   192.168.56.106   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:53:11,972 [INFO]   worker-w004   Ready    <none>          10d   v1.32.11   192.168.56.107   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:53:11,972 [INFO]   worker-w005   Ready    <none>          10d   v1.32.11   192.168.56.108   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:53:11,972 [INFO]   worker-w006   Ready    <none>          10d   v1.32.11   192.168.56.109   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:53:11,972 [INFO] 
Enhanced Node Status (with taint and cordon indicators):
2026-01-05 14:53:11,973 [INFO]   NAME                STATUS    ROLES           ZONE   CORDONED   TAINTS
2026-01-05 14:53:12,231 [INFO]   master-m001     Ready  ✓ worker         R1    No       node-role.kubernetes.io/control-plane 
2026-01-05 14:53:12,431 [INFO]   master-m002     Ready  ✓ worker         R2    No       None 
2026-01-05 14:53:12,616 [INFO]   master-m003     Ready  ✓ worker         R3    No       None 
2026-01-05 14:53:12,794 [INFO]   worker-w001     Ready  ✓ worker         R1    No       None 
2026-01-05 14:53:12,984 [INFO]   worker-w002     Ready  ✓ worker         R1    No       None 
2026-01-05 14:53:13,320 [INFO]   worker-w003     Ready  ✓ worker         R2    No       None 
2026-01-05 14:53:13,509 [INFO]   worker-w004     Ready  ✓ worker         R2    No       None 
2026-01-05 14:53:13,806 [INFO]   worker-w005     Ready  ✓ worker         R3    No       None 
2026-01-05 14:53:13,989 [INFO]   worker-w006     Ready  ✓ worker         R3    No       None 
2026-01-05 14:53:13,990 [INFO] 
Legend:
2026-01-05 14:53:13,990 [INFO]   ✓ = Node is Ready
2026-01-05 14:53:13,990 [INFO]   ⚠️ = Warning indicator (NotReady, Cordoned, or has simulated-failure taint)
2026-01-05 14:53:13,990 [INFO] ============ DETAILED POD INFORMATION ============
2026-01-05 14:53:13,990 [INFO] Running 'kubectl get pods -o wide' to show detailed pod placement:
2026-01-05 14:53:14,097 [INFO]   NAMESPACE     NAME                                                    READY   STATUS      RESTARTS        AGE     IP                NODE          NOMINATED NODE   READINESS GATES
2026-01-05 14:53:14,097 [INFO]   argo          argo-server-5c69cb69db-gdkl6                            1/1     Running     0               8d      192.168.221.65    master-m003   <none>           <none>
2026-01-05 14:53:14,097 [INFO]   argo          resilience-bench-ptngm-initialize-metrics-940927476     0/2     Completed   0               2d16h   192.168.221.125   master-m003   <none>           <none>
2026-01-05 14:53:14,098 [INFO]   argo          resilience-bench-ptngm-run-health-check-1168793163      0/2     Completed   0               2d16h   192.168.221.67    master-m003   <none>           <none>
2026-01-05 14:53:14,098 [INFO]   argo          resilience-bench-ptngm-run-health-check-1185570782      0/2     Completed   0               2d16h   192.168.221.66    master-m003   <none>           <none>
2026-01-05 14:53:14,098 [INFO]   argo          resilience-bench-ptngm-run-health-check-1202348401      0/2     Completed   0               2d16h   192.168.221.122   master-m003   <none>           <none>
2026-01-05 14:53:14,098 [INFO]   argo          resilience-bench-wf87f-initialize-metrics-3677018408    0/2     Completed   0               2d18h   192.168.221.100   master-m003   <none>           <none>
2026-01-05 14:53:14,098 [INFO]   argo          resilience-bench-wf87f-run-health-check-2914150102      0/2     Completed   0               2d18h   192.168.221.105   master-m003   <none>           <none>
2026-01-05 14:53:14,098 [INFO]   argo          resilience-bench-wf87f-run-health-check-3931060015      0/2     Completed   0               2d18h   192.168.221.104   master-m003   <none>           <none>
2026-01-05 14:53:14,098 [INFO]   argo          resilience-bench-wf87f-run-health-check-3947837634      0/2     Completed   0               2d18h   192.168.221.98    master-m003   <none>           <none>
2026-01-05 14:53:14,098 [INFO]   argo          resilience-bench-wf87f-run-health-check-3964615253      0/2     Completed   0               2d18h   192.168.221.103   master-m003   <none>           <none>
2026-01-05 14:53:14,098 [INFO]   argo          resilience-bench-wf87f-run-node-simulation-4173146970   0/2     Completed   0               2d18h   192.168.221.101   master-m003   <none>           <none>
2026-01-05 14:53:14,098 [INFO]   argo          resilience-bench-wf87f-run-rack-simulation-1356070019   0/2     Completed   0               2d18h   192.168.221.109   master-m003   <none>           <none>
2026-01-05 14:53:14,098 [INFO]   argo          resilience-heft-4cqbx-health-check-1589398418           0/2     Error       0               89m     192.168.15.231    worker-w005   <none>           <none>
2026-01-05 14:53:14,099 [INFO]   argo          resilience-heft-4cqbx-heft-initialize-2594215169        0/2     Completed   0               90m     192.168.221.79    master-m003   <none>           <none>
2026-01-05 14:53:14,099 [INFO]   argo          resilience-heft-76kbp-health-check-270747540            0/2     Error       0               81m     192.168.15.237    worker-w005   <none>           <none>
2026-01-05 14:53:14,099 [INFO]   argo          resilience-heft-76kbp-heft-initialize-3574960751        0/2     Completed   0               81m     192.168.221.86    master-m003   <none>           <none>
2026-01-05 14:53:14,099 [INFO]   argo          resilience-heft-b85xr-health-check-2840770649           0/2     Error       0               77m     192.168.15.240    worker-w005   <none>           <none>
2026-01-05 14:53:14,099 [INFO]   argo          resilience-heft-b85xr-heft-initialize-1754406316        0/2     Completed   0               78m     192.168.221.88    master-m003   <none>           <none>
2026-01-05 14:53:14,099 [INFO]   argo          resilience-heft-fdw9f-health-check-1896357264           0/2     Error       0               83m     192.168.15.235    worker-w005   <none>           <none>
2026-01-05 14:53:14,099 [INFO]   argo          resilience-heft-fdw9f-heft-initialize-2651619139        0/2     Completed   0               83m     192.168.221.77    master-m003   <none>           <none>
2026-01-05 14:53:14,099 [INFO]   argo          resilience-heft-gf256-health-check-980174358            0/2     Error       0               82m     192.168.15.236    worker-w005   <none>           <none>
2026-01-05 14:53:14,099 [INFO]   argo          resilience-heft-gf256-heft-initialize-1548293021        0/2     Completed   0               82m     192.168.221.84    master-m003   <none>           <none>
2026-01-05 14:53:14,099 [INFO]   argo          resilience-heft-gxwpg-health-check-3445304423           0/2     Error       0               85m     192.168.15.233    worker-w005   <none>           <none>
2026-01-05 14:53:14,099 [INFO]   argo          resilience-heft-gxwpg-heft-initialize-2377397022        0/2     Completed   0               85m     192.168.221.81    master-m003   <none>           <none>
2026-01-05 14:53:14,099 [INFO]   argo          resilience-heft-pc2kb-health-check-3258812212           0/2     Error       0               79m     192.168.15.238    worker-w005   <none>           <none>
2026-01-05 14:53:14,100 [INFO]   argo          resilience-heft-pc2kb-heft-initialize-321333135         0/2     Completed   0               80m     192.168.221.83    master-m003   <none>           <none>
2026-01-05 14:53:14,100 [INFO]   argo          resilience-heft-qsmch-health-check-283908068            0/2     Error       0               86m     192.168.15.232    worker-w005   <none>           <none>
2026-01-05 14:53:14,100 [INFO]   argo          resilience-heft-qsmch-heft-initialize-1908810015        0/2     Completed   0               87m     192.168.221.82    master-m003   <none>           <none>
2026-01-05 14:53:14,100 [INFO]   argo          resilience-heft-r2w8l-health-check-366927355            0/2     Error       0               78m     192.168.15.239    worker-w005   <none>           <none>
2026-01-05 14:53:14,100 [INFO]   argo          resilience-heft-r2w8l-heft-initialize-3054048250        0/2     Completed   0               79m     192.168.221.85    master-m003   <none>           <none>
2026-01-05 14:53:14,100 [INFO]   argo          resilience-heft-t988k-health-check-612195296            0/2     Error       0               84m     192.168.15.234    worker-w005   <none>           <none>
2026-01-05 14:53:14,100 [INFO]   argo          resilience-heft-t988k-heft-initialize-164752883         0/2     Completed   0               84m     192.168.221.80    master-m003   <none>           <none>
2026-01-05 14:53:14,100 [INFO]   argo          resilience-heft-vlt2m-health-check-2709447919           0/2     Error       0               40m     192.168.221.119   master-m003   <none>           <none>
2026-01-05 14:53:14,100 [INFO]   argo          resilience-heft-vlt2m-health-check-2726225538           0/2     Error       0               40m     192.168.221.120   master-m003   <none>           <none>
2026-01-05 14:53:14,100 [INFO]   argo          resilience-heft-vlt2m-health-check-2743003157           0/2     Error       0               40m     192.168.221.121   master-m003   <none>           <none>
2026-01-05 14:53:14,100 [INFO]   argo          resilience-heft-vlt2m-heft-initialize-840404112         0/2     Completed   0               41m     192.168.221.118   master-m003   <none>           <none>
2026-01-05 14:53:14,101 [INFO]   argo          resilience-heft-x86wd-health-check-2184182048           0/2     Error       0               39m     192.168.221.123   master-m003   <none>           <none>
2026-01-05 14:53:14,101 [INFO]   argo          resilience-heft-x86wd-health-check-2200959667           0/2     Error       0               39m     192.168.221.125   master-m003   <none>           <none>
2026-01-05 14:53:14,101 [INFO]   argo          resilience-heft-x86wd-health-check-2234514905           0/2     Error       0               39m     192.168.221.124   master-m003   <none>           <none>
2026-01-05 14:53:14,101 [INFO]   argo          resilience-heft-x86wd-heft-initialize-2152040770        0/2     Completed   0               39m     192.168.221.127   master-m003   <none>           <none>
2026-01-05 14:53:14,101 [INFO]   argo          workflow-controller-ccbd949dc-t4rx9                     1/1     Running     0               5m25s   192.168.195.243   worker-w002   <none>           <none>
2026-01-05 14:53:14,101 [INFO]   default       resilience-heft-2-202047-tbk8r                          1/1     Running     0               2m27s   192.168.221.126   master-m003   <none>           <none>
2026-01-05 14:53:14,101 [INFO]   kube-system   calico-kube-controllers-7498b9bb4c-xd7tb                1/1     Running     0               38h     192.168.191.108   worker-w006   <none>           <none>
2026-01-05 14:53:14,101 [INFO]   kube-system   calico-node-4zhd4                                       1/1     Running     0               10d     192.168.56.105    worker-w002   <none>           <none>
2026-01-05 14:53:14,101 [INFO]   kube-system   calico-node-75nx6                                       1/1     Running     0               10d     192.168.56.109    worker-w006   <none>           <none>
2026-01-05 14:53:14,102 [INFO]   kube-system   calico-node-7lkdq                                       1/1     Running     0               10d     192.168.56.104    worker-w001   <none>           <none>
2026-01-05 14:53:14,102 [INFO]   kube-system   calico-node-85f8c                                       1/1     Running     0               10d     192.168.56.102    master-m002   <none>           <none>
2026-01-05 14:53:14,102 [INFO]   kube-system   calico-node-j8nb9                                       1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:53:14,102 [INFO]   kube-system   calico-node-lbcb2                                       1/1     Running     0               10d     192.168.56.106    worker-w003   <none>           <none>
2026-01-05 14:53:14,102 [INFO]   kube-system   calico-node-wlr5v                                       1/1     Running     0               10d     192.168.56.107    worker-w004   <none>           <none>
2026-01-05 14:53:14,102 [INFO]   kube-system   calico-node-xnzjw                                       1/1     Running     0               10d     192.168.56.103    master-m003   <none>           <none>
2026-01-05 14:53:14,102 [INFO]   kube-system   calico-node-xsltn                                       1/1     Running     0               10d     192.168.56.108    worker-w005   <none>           <none>
2026-01-05 14:53:14,102 [INFO]   kube-system   coredns-668d6bf9bc-2f74f                                1/1     Running     0               39h     192.168.15.230    worker-w005   <none>           <none>
2026-01-05 14:53:14,102 [INFO]   kube-system   coredns-668d6bf9bc-kqndz                                1/1     Running     0               2m18s   192.168.153.242   worker-w004   <none>           <none>
2026-01-05 14:53:14,102 [INFO]   kube-system   etcd-master-m001                                        1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:53:14,103 [INFO]   kube-system   kube-apiserver-master-m001                              1/1     Running     1 (2d11h ago)   10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:53:14,103 [INFO]   kube-system   kube-controller-manager-master-m001                     1/1     Running     12 (37m ago)    10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:53:14,103 [INFO]   kube-system   kube-proxy-5lzhj                                        1/1     Running     0               10d     192.168.56.102    master-m002   <none>           <none>
2026-01-05 14:53:14,103 [INFO]   kube-system   kube-proxy-7jqkv                                        1/1     Running     0               10d     192.168.56.109    worker-w006   <none>           <none>
2026-01-05 14:53:14,103 [INFO]   kube-system   kube-proxy-cv5dt                                        1/1     Running     0               10d     192.168.56.108    worker-w005   <none>           <none>
2026-01-05 14:53:14,103 [INFO]   kube-system   kube-proxy-fvpmr                                        1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:53:14,103 [INFO]   kube-system   kube-proxy-hgs5z                                        1/1     Running     0               10d     192.168.56.106    worker-w003   <none>           <none>
2026-01-05 14:53:14,103 [INFO]   kube-system   kube-proxy-kmgqr                                        1/1     Running     0               10d     192.168.56.105    worker-w002   <none>           <none>
2026-01-05 14:53:14,103 [INFO]   kube-system   kube-proxy-rdbz5                                        1/1     Running     0               10d     192.168.56.107    worker-w004   <none>           <none>
2026-01-05 14:53:14,103 [INFO]   kube-system   kube-proxy-w8mnb                                        1/1     Running     0               10d     192.168.56.103    master-m003   <none>           <none>
2026-01-05 14:53:14,103 [INFO]   kube-system   kube-proxy-x9jxr                                        1/1     Running     0               10d     192.168.56.104    worker-w001   <none>           <none>
2026-01-05 14:53:14,104 [INFO]   kube-system   kube-scheduler-master-m001                              1/1     Running     11 (37m ago)    10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:53:14,104 [INFO] 
Pod distribution by node:
2026-01-05 14:53:14,213 [INFO]   Node master-m003: 33 pods
2026-01-05 14:53:14,213 [INFO]   Node worker-w005: 13 pods
2026-01-05 14:53:14,213 [INFO]   Node worker-w002: 3 pods
2026-01-05 14:53:14,213 [INFO]   Node worker-w006: 3 pods
2026-01-05 14:53:14,213 [INFO]   Node worker-w001: 2 pods
2026-01-05 14:53:14,213 [INFO]   Node master-m002: 2 pods
2026-01-05 14:53:14,213 [INFO]   Node master-m001: 3 pods
2026-01-05 14:53:14,214 [INFO]   Node worker-w003: 2 pods
2026-01-05 14:53:14,214 [INFO]   Node worker-w004: 3 pods
2026-01-05 14:53:14,214 [INFO]   Node 10d: 3 pods
2026-01-05 14:53:14,214 [INFO] 
Filtering for simulation services:
2026-01-05 14:53:14,337 [INFO] Node master-m001 is Ready
2026-01-05 14:53:14,337 [INFO] Node master-m002 is Ready
2026-01-05 14:53:14,337 [INFO] Node master-m003 is Ready
2026-01-05 14:53:14,337 [INFO] Node worker-w001 is Ready
2026-01-05 14:53:14,338 [INFO] Node worker-w002 is Ready
2026-01-05 14:53:14,338 [INFO] Node worker-w003 is Ready
2026-01-05 14:53:14,338 [INFO] Node worker-w004 is Ready
2026-01-05 14:53:14,338 [INFO] Node worker-w005 is Ready
2026-01-05 14:53:14,338 [INFO] Node worker-w006 is Ready
2026-01-05 14:53:14,384 [WARNING] No pods found for etcd-sim
2026-01-05 14:53:14,388 [WARNING] No pods found for postgres-sim
2026-01-05 14:53:14,390 [WARNING] No pods found for redis-sim
2026-01-05 14:53:14,392 [WARNING] No pods found for nginx-sim
2026-01-05 14:53:14,394 [WARNING] No pods found for auth-sim
2026-01-05 14:53:14,394 [INFO] Completed full health check
[2026-01-05 14:53:14] TIMING: NODE_SIMULATION completed in 138 seconds

================================================================
 STEP 3: INTERIM HEALTH CHECK
 Time: Mon Jan  5 14:53:14 UTC 2026
================================================================
Log directory created/verified: /app/logs
File logging configured successfully
2026-01-05 14:53:14,831 [INFO] Loaded in-cluster Kubernetes config
2026-01-05 14:53:14,831 [INFO] Running on host: resilience-heft-2-202047-tbk8r
2026-01-05 14:53:14,831 [INFO] Detected current node: master-m003, zone: R3
2026-01-05 14:53:14,831 [INFO] Checking if we have permissions to modify nodes...
2026-01-05 14:53:14,864 [INFO] Testing permissions using node: master-m001
2026-01-05 14:53:14,871 [INFO] Permission check successful - we can modify nodes
2026-01-05 14:53:14,872 [INFO] Using real Kubernetes API for node control
2026-01-05 14:53:14,872 [INFO] Action received: health-check
2026-01-05 14:53:14,873 [INFO] Stabilization time: 10 seconds
2026-01-05 14:53:14,873 [INFO] Starting full health check
2026-01-05 14:53:14,873 [INFO] 
============ DETAILED NODE STATUS ============
2026-01-05 14:53:14,873 [INFO] Basic Node Information (kubectl get nodes -o wide):
2026-01-05 14:53:14,944 [INFO]   NAME          STATUS   ROLES           AGE   VERSION    INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
2026-01-05 14:53:14,944 [INFO]   master-m001   Ready    control-plane   10d   v1.32.11   192.168.56.101   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:53:14,944 [INFO]   master-m002   Ready    control-plane   10d   v1.32.11   192.168.56.102   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:53:14,944 [INFO]   master-m003   Ready    control-plane   10d   v1.32.11   192.168.56.103   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:53:14,944 [INFO]   worker-w001   Ready    <none>          10d   v1.32.11   192.168.56.104   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:53:14,945 [INFO]   worker-w002   Ready    <none>          10d   v1.32.11   192.168.56.105   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:53:14,945 [INFO]   worker-w003   Ready    <none>          10d   v1.32.11   192.168.56.106   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:53:14,945 [INFO]   worker-w004   Ready    <none>          10d   v1.32.11   192.168.56.107   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:53:14,945 [INFO]   worker-w005   Ready    <none>          10d   v1.32.11   192.168.56.108   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:53:14,945 [INFO]   worker-w006   Ready    <none>          10d   v1.32.11   192.168.56.109   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:53:14,945 [INFO] 
Enhanced Node Status (with taint and cordon indicators):
2026-01-05 14:53:14,945 [INFO]   NAME                STATUS    ROLES           ZONE   CORDONED   TAINTS
2026-01-05 14:53:15,193 [INFO]   master-m001     Ready  ✓ worker         R1    No       node-role.kubernetes.io/control-plane 
2026-01-05 14:53:15,366 [INFO]   master-m002     Ready  ✓ worker         R2    No       None 
2026-01-05 14:53:15,544 [INFO]   master-m003     Ready  ✓ worker         R3    No       None 
2026-01-05 14:53:15,720 [INFO]   worker-w001     Ready  ✓ worker         R1    No       None 
2026-01-05 14:53:15,892 [INFO]   worker-w002     Ready  ✓ worker         R1    No       None 
2026-01-05 14:53:16,068 [INFO]   worker-w003     Ready  ✓ worker         R2    No       None 
2026-01-05 14:53:16,244 [INFO]   worker-w004     Ready  ✓ worker         R2    No       None 
2026-01-05 14:53:16,419 [INFO]   worker-w005     Ready  ✓ worker         R3    No       None 
2026-01-05 14:53:16,641 [INFO]   worker-w006     Ready  ✓ worker         R3    No       None 
2026-01-05 14:53:16,642 [INFO] 
Legend:
2026-01-05 14:53:16,642 [INFO]   ✓ = Node is Ready
2026-01-05 14:53:16,642 [INFO]   ⚠️ = Warning indicator (NotReady, Cordoned, or has simulated-failure taint)
2026-01-05 14:53:16,642 [INFO] ============ DETAILED POD INFORMATION ============
2026-01-05 14:53:16,642 [INFO] Running 'kubectl get pods -o wide' to show detailed pod placement:
2026-01-05 14:53:16,826 [INFO]   NAMESPACE     NAME                                                    READY   STATUS      RESTARTS        AGE     IP                NODE          NOMINATED NODE   READINESS GATES
2026-01-05 14:53:16,826 [INFO]   argo          argo-server-5c69cb69db-gdkl6                            1/1     Running     0               8d      192.168.221.65    master-m003   <none>           <none>
2026-01-05 14:53:16,826 [INFO]   argo          resilience-bench-ptngm-initialize-metrics-940927476     0/2     Completed   0               2d16h   192.168.221.125   master-m003   <none>           <none>
2026-01-05 14:53:16,827 [INFO]   argo          resilience-bench-ptngm-run-health-check-1168793163      0/2     Completed   0               2d16h   192.168.221.67    master-m003   <none>           <none>
2026-01-05 14:53:16,827 [INFO]   argo          resilience-bench-ptngm-run-health-check-1185570782      0/2     Completed   0               2d16h   192.168.221.66    master-m003   <none>           <none>
2026-01-05 14:53:16,827 [INFO]   argo          resilience-bench-ptngm-run-health-check-1202348401      0/2     Completed   0               2d16h   192.168.221.122   master-m003   <none>           <none>
2026-01-05 14:53:16,827 [INFO]   argo          resilience-bench-wf87f-initialize-metrics-3677018408    0/2     Completed   0               2d18h   192.168.221.100   master-m003   <none>           <none>
2026-01-05 14:53:16,827 [INFO]   argo          resilience-bench-wf87f-run-health-check-2914150102      0/2     Completed   0               2d18h   192.168.221.105   master-m003   <none>           <none>
2026-01-05 14:53:16,827 [INFO]   argo          resilience-bench-wf87f-run-health-check-3931060015      0/2     Completed   0               2d18h   192.168.221.104   master-m003   <none>           <none>
2026-01-05 14:53:16,827 [INFO]   argo          resilience-bench-wf87f-run-health-check-3947837634      0/2     Completed   0               2d18h   192.168.221.98    master-m003   <none>           <none>
2026-01-05 14:53:16,827 [INFO]   argo          resilience-bench-wf87f-run-health-check-3964615253      0/2     Completed   0               2d18h   192.168.221.103   master-m003   <none>           <none>
2026-01-05 14:53:16,827 [INFO]   argo          resilience-bench-wf87f-run-node-simulation-4173146970   0/2     Completed   0               2d18h   192.168.221.101   master-m003   <none>           <none>
2026-01-05 14:53:16,827 [INFO]   argo          resilience-bench-wf87f-run-rack-simulation-1356070019   0/2     Completed   0               2d18h   192.168.221.109   master-m003   <none>           <none>
2026-01-05 14:53:16,827 [INFO]   argo          resilience-heft-4cqbx-health-check-1589398418           0/2     Error       0               89m     192.168.15.231    worker-w005   <none>           <none>
2026-01-05 14:53:16,827 [INFO]   argo          resilience-heft-4cqbx-heft-initialize-2594215169        0/2     Completed   0               90m     192.168.221.79    master-m003   <none>           <none>
2026-01-05 14:53:16,827 [INFO]   argo          resilience-heft-76kbp-health-check-270747540            0/2     Error       0               81m     192.168.15.237    worker-w005   <none>           <none>
2026-01-05 14:53:16,827 [INFO]   argo          resilience-heft-76kbp-heft-initialize-3574960751        0/2     Completed   0               81m     192.168.221.86    master-m003   <none>           <none>
2026-01-05 14:53:16,827 [INFO]   argo          resilience-heft-b85xr-health-check-2840770649           0/2     Error       0               77m     192.168.15.240    worker-w005   <none>           <none>
2026-01-05 14:53:16,827 [INFO]   argo          resilience-heft-b85xr-heft-initialize-1754406316        0/2     Completed   0               78m     192.168.221.88    master-m003   <none>           <none>
2026-01-05 14:53:16,827 [INFO]   argo          resilience-heft-fdw9f-health-check-1896357264           0/2     Error       0               83m     192.168.15.235    worker-w005   <none>           <none>
2026-01-05 14:53:16,827 [INFO]   argo          resilience-heft-fdw9f-heft-initialize-2651619139        0/2     Completed   0               83m     192.168.221.77    master-m003   <none>           <none>
2026-01-05 14:53:16,827 [INFO]   argo          resilience-heft-gf256-health-check-980174358            0/2     Error       0               82m     192.168.15.236    worker-w005   <none>           <none>
2026-01-05 14:53:16,827 [INFO]   argo          resilience-heft-gf256-heft-initialize-1548293021        0/2     Completed   0               82m     192.168.221.84    master-m003   <none>           <none>
2026-01-05 14:53:16,828 [INFO]   argo          resilience-heft-gxwpg-health-check-3445304423           0/2     Error       0               85m     192.168.15.233    worker-w005   <none>           <none>
2026-01-05 14:53:16,828 [INFO]   argo          resilience-heft-gxwpg-heft-initialize-2377397022        0/2     Completed   0               85m     192.168.221.81    master-m003   <none>           <none>
2026-01-05 14:53:16,828 [INFO]   argo          resilience-heft-pc2kb-health-check-3258812212           0/2     Error       0               79m     192.168.15.238    worker-w005   <none>           <none>
2026-01-05 14:53:16,828 [INFO]   argo          resilience-heft-pc2kb-heft-initialize-321333135         0/2     Completed   0               80m     192.168.221.83    master-m003   <none>           <none>
2026-01-05 14:53:16,828 [INFO]   argo          resilience-heft-qsmch-health-check-283908068            0/2     Error       0               86m     192.168.15.232    worker-w005   <none>           <none>
2026-01-05 14:53:16,828 [INFO]   argo          resilience-heft-qsmch-heft-initialize-1908810015        0/2     Completed   0               87m     192.168.221.82    master-m003   <none>           <none>
2026-01-05 14:53:16,828 [INFO]   argo          resilience-heft-r2w8l-health-check-366927355            0/2     Error       0               79m     192.168.15.239    worker-w005   <none>           <none>
2026-01-05 14:53:16,828 [INFO]   argo          resilience-heft-r2w8l-heft-initialize-3054048250        0/2     Completed   0               79m     192.168.221.85    master-m003   <none>           <none>
2026-01-05 14:53:16,828 [INFO]   argo          resilience-heft-t988k-health-check-612195296            0/2     Error       0               84m     192.168.15.234    worker-w005   <none>           <none>
2026-01-05 14:53:16,828 [INFO]   argo          resilience-heft-t988k-heft-initialize-164752883         0/2     Completed   0               84m     192.168.221.80    master-m003   <none>           <none>
2026-01-05 14:53:16,828 [INFO]   argo          resilience-heft-vlt2m-health-check-2709447919           0/2     Error       0               40m     192.168.221.119   master-m003   <none>           <none>
2026-01-05 14:53:16,828 [INFO]   argo          resilience-heft-vlt2m-health-check-2726225538           0/2     Error       0               40m     192.168.221.120   master-m003   <none>           <none>
2026-01-05 14:53:16,828 [INFO]   argo          resilience-heft-vlt2m-health-check-2743003157           0/2     Error       0               40m     192.168.221.121   master-m003   <none>           <none>
2026-01-05 14:53:16,828 [INFO]   argo          resilience-heft-vlt2m-heft-initialize-840404112         0/2     Completed   0               41m     192.168.221.118   master-m003   <none>           <none>
2026-01-05 14:53:16,828 [INFO]   argo          resilience-heft-x86wd-health-check-2184182048           0/2     Error       0               39m     192.168.221.123   master-m003   <none>           <none>
2026-01-05 14:53:16,828 [INFO]   argo          resilience-heft-x86wd-health-check-2200959667           0/2     Error       0               39m     192.168.221.125   master-m003   <none>           <none>
2026-01-05 14:53:16,828 [INFO]   argo          resilience-heft-x86wd-health-check-2234514905           0/2     Error       0               39m     192.168.221.124   master-m003   <none>           <none>
2026-01-05 14:53:16,828 [INFO]   argo          resilience-heft-x86wd-heft-initialize-2152040770        0/2     Completed   0               39m     192.168.221.127   master-m003   <none>           <none>
2026-01-05 14:53:16,828 [INFO]   argo          workflow-controller-ccbd949dc-t4rx9                     1/1     Running     0               5m27s   192.168.195.243   worker-w002   <none>           <none>
2026-01-05 14:53:16,828 [INFO]   default       resilience-heft-2-202047-tbk8r                          1/1     Running     0               2m29s   192.168.221.126   master-m003   <none>           <none>
2026-01-05 14:53:16,828 [INFO]   kube-system   calico-kube-controllers-7498b9bb4c-xd7tb                1/1     Running     0               38h     192.168.191.108   worker-w006   <none>           <none>
2026-01-05 14:53:16,829 [INFO]   kube-system   calico-node-4zhd4                                       1/1     Running     0               10d     192.168.56.105    worker-w002   <none>           <none>
2026-01-05 14:53:16,829 [INFO]   kube-system   calico-node-75nx6                                       1/1     Running     0               10d     192.168.56.109    worker-w006   <none>           <none>
2026-01-05 14:53:16,829 [INFO]   kube-system   calico-node-7lkdq                                       1/1     Running     0               10d     192.168.56.104    worker-w001   <none>           <none>
2026-01-05 14:53:16,829 [INFO]   kube-system   calico-node-85f8c                                       1/1     Running     0               10d     192.168.56.102    master-m002   <none>           <none>
2026-01-05 14:53:16,829 [INFO]   kube-system   calico-node-j8nb9                                       1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:53:16,829 [INFO]   kube-system   calico-node-lbcb2                                       1/1     Running     0               10d     192.168.56.106    worker-w003   <none>           <none>
2026-01-05 14:53:16,829 [INFO]   kube-system   calico-node-wlr5v                                       1/1     Running     0               10d     192.168.56.107    worker-w004   <none>           <none>
2026-01-05 14:53:16,829 [INFO]   kube-system   calico-node-xnzjw                                       1/1     Running     0               10d     192.168.56.103    master-m003   <none>           <none>
2026-01-05 14:53:16,829 [INFO]   kube-system   calico-node-xsltn                                       1/1     Running     0               10d     192.168.56.108    worker-w005   <none>           <none>
2026-01-05 14:53:16,829 [INFO]   kube-system   coredns-668d6bf9bc-2f74f                                1/1     Running     0               39h     192.168.15.230    worker-w005   <none>           <none>
2026-01-05 14:53:16,829 [INFO]   kube-system   coredns-668d6bf9bc-kqndz                                1/1     Running     0               2m20s   192.168.153.242   worker-w004   <none>           <none>
2026-01-05 14:53:16,829 [INFO]   kube-system   etcd-master-m001                                        1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:53:16,829 [INFO]   kube-system   kube-apiserver-master-m001                              1/1     Running     1 (2d11h ago)   10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:53:16,829 [INFO]   kube-system   kube-controller-manager-master-m001                     1/1     Running     12 (37m ago)    10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:53:16,829 [INFO]   kube-system   kube-proxy-5lzhj                                        1/1     Running     0               10d     192.168.56.102    master-m002   <none>           <none>
2026-01-05 14:53:16,829 [INFO]   kube-system   kube-proxy-7jqkv                                        1/1     Running     0               10d     192.168.56.109    worker-w006   <none>           <none>
2026-01-05 14:53:16,829 [INFO]   kube-system   kube-proxy-cv5dt                                        1/1     Running     0               10d     192.168.56.108    worker-w005   <none>           <none>
2026-01-05 14:53:16,829 [INFO]   kube-system   kube-proxy-fvpmr                                        1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:53:16,829 [INFO]   kube-system   kube-proxy-hgs5z                                        1/1     Running     0               10d     192.168.56.106    worker-w003   <none>           <none>
2026-01-05 14:53:16,829 [INFO]   kube-system   kube-proxy-kmgqr                                        1/1     Running     0               10d     192.168.56.105    worker-w002   <none>           <none>
2026-01-05 14:53:16,829 [INFO]   kube-system   kube-proxy-rdbz5                                        1/1     Running     0               10d     192.168.56.107    worker-w004   <none>           <none>
2026-01-05 14:53:16,829 [INFO]   kube-system   kube-proxy-w8mnb                                        1/1     Running     0               10d     192.168.56.103    master-m003   <none>           <none>
2026-01-05 14:53:16,829 [INFO]   kube-system   kube-proxy-x9jxr                                        1/1     Running     0               10d     192.168.56.104    worker-w001   <none>           <none>
2026-01-05 14:53:16,830 [INFO]   kube-system   kube-scheduler-master-m001                              1/1     Running     11 (37m ago)    10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:53:16,830 [INFO] 
Pod distribution by node:
2026-01-05 14:53:16,937 [INFO]   Node master-m003: 33 pods
2026-01-05 14:53:16,937 [INFO]   Node worker-w005: 13 pods
2026-01-05 14:53:16,937 [INFO]   Node worker-w002: 3 pods
2026-01-05 14:53:16,937 [INFO]   Node worker-w006: 3 pods
2026-01-05 14:53:16,937 [INFO]   Node worker-w001: 2 pods
2026-01-05 14:53:16,937 [INFO]   Node master-m002: 2 pods
2026-01-05 14:53:16,937 [INFO]   Node master-m001: 3 pods
2026-01-05 14:53:16,937 [INFO]   Node worker-w003: 2 pods
2026-01-05 14:53:16,937 [INFO]   Node worker-w004: 3 pods
2026-01-05 14:53:16,937 [INFO]   Node 10d: 3 pods
2026-01-05 14:53:16,937 [INFO] 
Filtering for simulation services:
2026-01-05 14:53:17,027 [INFO] Node master-m001 is Ready
2026-01-05 14:53:17,027 [INFO] Node master-m002 is Ready
2026-01-05 14:53:17,027 [INFO] Node master-m003 is Ready
2026-01-05 14:53:17,027 [INFO] Node worker-w001 is Ready
2026-01-05 14:53:17,027 [INFO] Node worker-w002 is Ready
2026-01-05 14:53:17,027 [INFO] Node worker-w003 is Ready
2026-01-05 14:53:17,027 [INFO] Node worker-w004 is Ready
2026-01-05 14:53:17,028 [INFO] Node worker-w005 is Ready
2026-01-05 14:53:17,028 [INFO] Node worker-w006 is Ready
2026-01-05 14:53:17,031 [WARNING] No pods found for etcd-sim
2026-01-05 14:53:17,033 [WARNING] No pods found for postgres-sim
2026-01-05 14:53:17,035 [WARNING] No pods found for redis-sim
2026-01-05 14:53:17,037 [WARNING] No pods found for nginx-sim
2026-01-05 14:53:17,038 [WARNING] No pods found for auth-sim
2026-01-05 14:53:17,039 [INFO] Completed full health check
[2026-01-05 14:53:17] TIMING: INTERIM_HEALTH_CHECK completed in 3 seconds

================================================================
 STEP 4: RACK FAILURE SIMULATION (HEFT-Aware)
 Excluding zone: R3
 Time: Mon Jan  5 14:53:17 UTC 2026
================================================================
Log directory created/verified: /app/logs
File logging configured successfully
2026-01-05 14:53:17,463 [INFO] Loaded in-cluster Kubernetes config
2026-01-05 14:53:17,463 [INFO] Running on host: resilience-heft-2-202047-tbk8r
2026-01-05 14:53:17,463 [INFO] Detected current node: master-m003, zone: R3
2026-01-05 14:53:17,463 [INFO] Checking if we have permissions to modify nodes...
2026-01-05 14:53:17,535 [INFO] Testing permissions using node: master-m001
2026-01-05 14:53:17,543 [INFO] Permission check successful - we can modify nodes
2026-01-05 14:53:17,543 [INFO] Using real Kubernetes API for node control
2026-01-05 14:53:17,544 [INFO] Action received: simulate-rack
2026-01-05 14:53:17,544 [INFO] Stabilization time: 60 seconds
2026-01-05 14:53:17,544 [INFO] Current node: master-m003 in zone: R3
2026-01-05 14:53:17,544 [INFO] Safe zones for rack simulation: ['R1', 'R2']
2026-01-05 14:53:17,544 [INFO] Simulating full rack (zone) failure: R2 with nodes: ['master-m002', 'worker-w003', 'worker-w004']
2026-01-05 14:53:17,545 [INFO] Simulating node failure for master-m002 using Kubernetes API
2026-01-05 14:53:17,582 [INFO] Node master-m002 cordoned
2026-01-05 14:53:17,601 [INFO] Node master-m002 tainted with NoExecute
2026-01-05 14:53:17,601 [INFO] Node master-m002 powered off (delay 5s)
2026-01-05 14:53:17,601 [INFO] Node master-m002 powered off (delay 5s)
2026-01-05 14:53:22,607 [INFO] Simulating node failure for worker-w003 using Kubernetes API
2026-01-05 14:53:22,624 [INFO] Node worker-w003 cordoned
2026-01-05 14:53:22,635 [INFO] Node worker-w003 tainted with NoExecute
2026-01-05 14:53:22,635 [INFO] Node worker-w003 powered off (delay 5s)
2026-01-05 14:53:22,635 [INFO] Node worker-w003 powered off (delay 5s)
2026-01-05 14:53:27,640 [INFO] Simulating node failure for worker-w004 using Kubernetes API
2026-01-05 14:53:27,659 [INFO] Node worker-w004 cordoned
2026-01-05 14:53:27,670 [INFO] Node worker-w004 tainted with NoExecute
2026-01-05 14:53:27,670 [INFO] Node worker-w004 powered off (delay 5s)
2026-01-05 14:53:27,670 [INFO] Node worker-w004 powered off (delay 5s)
2026-01-05 14:53:32,671 [INFO] Waiting 60 seconds for the cluster to stabilize before health check...
2026-01-05 14:54:32,681 [INFO] Running health check after rack power off
2026-01-05 14:54:32,681 [INFO] Starting full health check
2026-01-05 14:54:32,681 [INFO] 
============ DETAILED NODE STATUS ============
2026-01-05 14:54:32,682 [INFO] Basic Node Information (kubectl get nodes -o wide):
2026-01-05 14:54:32,744 [INFO]   NAME          STATUS                     ROLES           AGE   VERSION    INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
2026-01-05 14:54:32,744 [INFO]   master-m001   Ready                      control-plane   10d   v1.32.11   192.168.56.101   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:54:32,744 [INFO]   master-m002   Ready,SchedulingDisabled   control-plane   10d   v1.32.11   192.168.56.102   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:54:32,744 [INFO]   master-m003   Ready                      control-plane   10d   v1.32.11   192.168.56.103   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:54:32,744 [INFO]   worker-w001   Ready                      <none>          10d   v1.32.11   192.168.56.104   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:54:32,744 [INFO]   worker-w002   Ready                      <none>          10d   v1.32.11   192.168.56.105   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:54:32,744 [INFO]   worker-w003   Ready,SchedulingDisabled   <none>          10d   v1.32.11   192.168.56.106   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:54:32,745 [INFO]   worker-w004   Ready,SchedulingDisabled   <none>          10d   v1.32.11   192.168.56.107   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:54:32,745 [INFO]   worker-w005   Ready                      <none>          10d   v1.32.11   192.168.56.108   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:54:32,745 [INFO]   worker-w006   Ready                      <none>          10d   v1.32.11   192.168.56.109   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:54:32,745 [INFO] 
Enhanced Node Status (with taint and cordon indicators):
2026-01-05 14:54:32,745 [INFO]   NAME                STATUS    ROLES           ZONE   CORDONED   TAINTS
2026-01-05 14:54:33,019 [INFO]   master-m001     Ready  ✓ worker         R1    No       node-role.kubernetes.io/control-plane 
2026-01-05 14:54:33,219 [INFO]   master-m002     Ready  ✓ worker         R2    YES     ⚠️ simulated-failure, node.kubernetes.io/unschedulable ⚠️
2026-01-05 14:54:33,410 [INFO]   master-m003     Ready  ✓ worker         R3    No       None 
2026-01-05 14:54:33,600 [INFO]   worker-w001     Ready  ✓ worker         R1    No       None 
2026-01-05 14:54:33,775 [INFO]   worker-w002     Ready  ✓ worker         R1    No       None 
2026-01-05 14:54:33,963 [INFO]   worker-w003     Ready  ✓ worker         R2    YES     ⚠️ simulated-failure, node.kubernetes.io/unschedulable ⚠️
2026-01-05 14:54:34,169 [INFO]   worker-w004     Ready  ✓ worker         R2    YES     ⚠️ simulated-failure, node.kubernetes.io/unschedulable ⚠️
2026-01-05 14:54:34,379 [INFO]   worker-w005     Ready  ✓ worker         R3    No       None 
2026-01-05 14:54:34,540 [INFO]   worker-w006     Ready  ✓ worker         R3    No       None 
2026-01-05 14:54:34,541 [INFO] 
Legend:
2026-01-05 14:54:34,541 [INFO]   ✓ = Node is Ready
2026-01-05 14:54:34,541 [INFO]   ⚠️ = Warning indicator (NotReady, Cordoned, or has simulated-failure taint)
2026-01-05 14:54:34,541 [INFO] ============ DETAILED POD INFORMATION ============
2026-01-05 14:54:34,542 [INFO] Running 'kubectl get pods -o wide' to show detailed pod placement:
2026-01-05 14:54:34,640 [INFO]   NAMESPACE     NAME                                                    READY   STATUS      RESTARTS        AGE     IP                NODE          NOMINATED NODE   READINESS GATES
2026-01-05 14:54:34,640 [INFO]   argo          argo-server-5c69cb69db-gdkl6                            1/1     Running     0               8d      192.168.221.65    master-m003   <none>           <none>
2026-01-05 14:54:34,641 [INFO]   argo          resilience-bench-ptngm-initialize-metrics-940927476     0/2     Completed   0               2d16h   192.168.221.125   master-m003   <none>           <none>
2026-01-05 14:54:34,641 [INFO]   argo          resilience-bench-ptngm-run-health-check-1168793163      0/2     Completed   0               2d16h   192.168.221.67    master-m003   <none>           <none>
2026-01-05 14:54:34,641 [INFO]   argo          resilience-bench-ptngm-run-health-check-1185570782      0/2     Completed   0               2d16h   192.168.221.66    master-m003   <none>           <none>
2026-01-05 14:54:34,641 [INFO]   argo          resilience-bench-ptngm-run-health-check-1202348401      0/2     Completed   0               2d16h   192.168.221.122   master-m003   <none>           <none>
2026-01-05 14:54:34,641 [INFO]   argo          resilience-bench-wf87f-initialize-metrics-3677018408    0/2     Completed   0               2d18h   192.168.221.100   master-m003   <none>           <none>
2026-01-05 14:54:34,641 [INFO]   argo          resilience-bench-wf87f-run-health-check-2914150102      0/2     Completed   0               2d18h   192.168.221.105   master-m003   <none>           <none>
2026-01-05 14:54:34,641 [INFO]   argo          resilience-bench-wf87f-run-health-check-3931060015      0/2     Completed   0               2d18h   192.168.221.104   master-m003   <none>           <none>
2026-01-05 14:54:34,642 [INFO]   argo          resilience-bench-wf87f-run-health-check-3947837634      0/2     Completed   0               2d18h   192.168.221.98    master-m003   <none>           <none>
2026-01-05 14:54:34,642 [INFO]   argo          resilience-bench-wf87f-run-health-check-3964615253      0/2     Completed   0               2d18h   192.168.221.103   master-m003   <none>           <none>
2026-01-05 14:54:34,642 [INFO]   argo          resilience-bench-wf87f-run-node-simulation-4173146970   0/2     Completed   0               2d18h   192.168.221.101   master-m003   <none>           <none>
2026-01-05 14:54:34,642 [INFO]   argo          resilience-bench-wf87f-run-rack-simulation-1356070019   0/2     Completed   0               2d18h   192.168.221.109   master-m003   <none>           <none>
2026-01-05 14:54:34,642 [INFO]   argo          resilience-heft-4cqbx-health-check-1589398418           0/2     Error       0               90m     192.168.15.231    worker-w005   <none>           <none>
2026-01-05 14:54:34,642 [INFO]   argo          resilience-heft-4cqbx-heft-initialize-2594215169        0/2     Completed   0               91m     192.168.221.79    master-m003   <none>           <none>
2026-01-05 14:54:34,642 [INFO]   argo          resilience-heft-76kbp-health-check-270747540            0/2     Error       0               82m     192.168.15.237    worker-w005   <none>           <none>
2026-01-05 14:54:34,642 [INFO]   argo          resilience-heft-76kbp-heft-initialize-3574960751        0/2     Completed   0               82m     192.168.221.86    master-m003   <none>           <none>
2026-01-05 14:54:34,643 [INFO]   argo          resilience-heft-b85xr-health-check-2840770649           0/2     Error       0               79m     192.168.15.240    worker-w005   <none>           <none>
2026-01-05 14:54:34,643 [INFO]   argo          resilience-heft-b85xr-heft-initialize-1754406316        0/2     Completed   0               79m     192.168.221.88    master-m003   <none>           <none>
2026-01-05 14:54:34,643 [INFO]   argo          resilience-heft-fdw9f-health-check-1896357264           0/2     Error       0               84m     192.168.15.235    worker-w005   <none>           <none>
2026-01-05 14:54:34,643 [INFO]   argo          resilience-heft-fdw9f-heft-initialize-2651619139        0/2     Completed   0               85m     192.168.221.77    master-m003   <none>           <none>
2026-01-05 14:54:34,643 [INFO]   argo          resilience-heft-gf256-health-check-980174358            0/2     Error       0               83m     192.168.15.236    worker-w005   <none>           <none>
2026-01-05 14:54:34,643 [INFO]   argo          resilience-heft-gf256-heft-initialize-1548293021        0/2     Completed   0               83m     192.168.221.84    master-m003   <none>           <none>
2026-01-05 14:54:34,643 [INFO]   argo          resilience-heft-gxwpg-health-check-3445304423           0/2     Error       0               86m     192.168.15.233    worker-w005   <none>           <none>
2026-01-05 14:54:34,643 [INFO]   argo          resilience-heft-gxwpg-heft-initialize-2377397022        0/2     Completed   0               87m     192.168.221.81    master-m003   <none>           <none>
2026-01-05 14:54:34,644 [INFO]   argo          resilience-heft-pc2kb-health-check-3258812212           0/2     Error       0               81m     192.168.15.238    worker-w005   <none>           <none>
2026-01-05 14:54:34,644 [INFO]   argo          resilience-heft-pc2kb-heft-initialize-321333135         0/2     Completed   0               81m     192.168.221.83    master-m003   <none>           <none>
2026-01-05 14:54:34,644 [INFO]   argo          resilience-heft-qsmch-health-check-283908068            0/2     Error       0               88m     192.168.15.232    worker-w005   <none>           <none>
2026-01-05 14:54:34,644 [INFO]   argo          resilience-heft-qsmch-heft-initialize-1908810015        0/2     Completed   0               88m     192.168.221.82    master-m003   <none>           <none>
2026-01-05 14:54:34,644 [INFO]   argo          resilience-heft-r2w8l-health-check-366927355            0/2     Error       0               80m     192.168.15.239    worker-w005   <none>           <none>
2026-01-05 14:54:34,644 [INFO]   argo          resilience-heft-r2w8l-heft-initialize-3054048250        0/2     Completed   0               80m     192.168.221.85    master-m003   <none>           <none>
2026-01-05 14:54:34,644 [INFO]   argo          resilience-heft-t988k-health-check-612195296            0/2     Error       0               85m     192.168.15.234    worker-w005   <none>           <none>
2026-01-05 14:54:34,644 [INFO]   argo          resilience-heft-t988k-heft-initialize-164752883         0/2     Completed   0               86m     192.168.221.80    master-m003   <none>           <none>
2026-01-05 14:54:34,644 [INFO]   argo          resilience-heft-vlt2m-health-check-2709447919           0/2     Error       0               42m     192.168.221.119   master-m003   <none>           <none>
2026-01-05 14:54:34,644 [INFO]   argo          resilience-heft-vlt2m-health-check-2726225538           0/2     Error       0               42m     192.168.221.120   master-m003   <none>           <none>
2026-01-05 14:54:34,645 [INFO]   argo          resilience-heft-vlt2m-health-check-2743003157           0/2     Error       0               42m     192.168.221.121   master-m003   <none>           <none>
2026-01-05 14:54:34,645 [INFO]   argo          resilience-heft-vlt2m-heft-initialize-840404112         0/2     Completed   0               42m     192.168.221.118   master-m003   <none>           <none>
2026-01-05 14:54:34,645 [INFO]   argo          resilience-heft-x86wd-health-check-2184182048           0/2     Error       0               40m     192.168.221.123   master-m003   <none>           <none>
2026-01-05 14:54:34,645 [INFO]   argo          resilience-heft-x86wd-health-check-2200959667           0/2     Error       0               40m     192.168.221.125   master-m003   <none>           <none>
2026-01-05 14:54:34,645 [INFO]   argo          resilience-heft-x86wd-health-check-2234514905           0/2     Error       0               40m     192.168.221.124   master-m003   <none>           <none>
2026-01-05 14:54:34,645 [INFO]   argo          resilience-heft-x86wd-heft-initialize-2152040770        0/2     Completed   0               40m     192.168.221.127   master-m003   <none>           <none>
2026-01-05 14:54:34,645 [INFO]   argo          workflow-controller-ccbd949dc-t4rx9                     1/1     Running     0               6m45s   192.168.195.243   worker-w002   <none>           <none>
2026-01-05 14:54:34,645 [INFO]   default       resilience-heft-2-202047-tbk8r                          1/1     Running     0               3m47s   192.168.221.126   master-m003   <none>           <none>
2026-01-05 14:54:34,646 [INFO]   kube-system   calico-kube-controllers-7498b9bb4c-xd7tb                1/1     Running     0               38h     192.168.191.108   worker-w006   <none>           <none>
2026-01-05 14:54:34,646 [INFO]   kube-system   calico-node-4zhd4                                       1/1     Running     0               10d     192.168.56.105    worker-w002   <none>           <none>
2026-01-05 14:54:34,646 [INFO]   kube-system   calico-node-75nx6                                       1/1     Running     0               10d     192.168.56.109    worker-w006   <none>           <none>
2026-01-05 14:54:34,646 [INFO]   kube-system   calico-node-7lkdq                                       1/1     Running     0               10d     192.168.56.104    worker-w001   <none>           <none>
2026-01-05 14:54:34,646 [INFO]   kube-system   calico-node-85f8c                                       1/1     Running     0               10d     192.168.56.102    master-m002   <none>           <none>
2026-01-05 14:54:34,646 [INFO]   kube-system   calico-node-j8nb9                                       1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:54:34,646 [INFO]   kube-system   calico-node-lbcb2                                       1/1     Running     0               10d     192.168.56.106    worker-w003   <none>           <none>
2026-01-05 14:54:34,646 [INFO]   kube-system   calico-node-wlr5v                                       1/1     Running     0               10d     192.168.56.107    worker-w004   <none>           <none>
2026-01-05 14:54:34,646 [INFO]   kube-system   calico-node-xnzjw                                       1/1     Running     0               10d     192.168.56.103    master-m003   <none>           <none>
2026-01-05 14:54:34,647 [INFO]   kube-system   calico-node-xsltn                                       1/1     Running     0               10d     192.168.56.108    worker-w005   <none>           <none>
2026-01-05 14:54:34,647 [INFO]   kube-system   coredns-668d6bf9bc-2f74f                                1/1     Running     0               40h     192.168.15.230    worker-w005   <none>           <none>
2026-01-05 14:54:34,647 [INFO]   kube-system   coredns-668d6bf9bc-qnzjq                                1/1     Running     0               67s     192.168.132.176   worker-w001   <none>           <none>
2026-01-05 14:54:34,647 [INFO]   kube-system   etcd-master-m001                                        1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:54:34,647 [INFO]   kube-system   kube-apiserver-master-m001                              1/1     Running     1 (2d11h ago)   10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:54:34,647 [INFO]   kube-system   kube-controller-manager-master-m001                     1/1     Running     12 (38m ago)    10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:54:34,647 [INFO]   kube-system   kube-proxy-5lzhj                                        1/1     Running     0               10d     192.168.56.102    master-m002   <none>           <none>
2026-01-05 14:54:34,647 [INFO]   kube-system   kube-proxy-7jqkv                                        1/1     Running     0               10d     192.168.56.109    worker-w006   <none>           <none>
2026-01-05 14:54:34,647 [INFO]   kube-system   kube-proxy-cv5dt                                        1/1     Running     0               10d     192.168.56.108    worker-w005   <none>           <none>
2026-01-05 14:54:34,647 [INFO]   kube-system   kube-proxy-fvpmr                                        1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:54:34,648 [INFO]   kube-system   kube-proxy-hgs5z                                        1/1     Running     0               10d     192.168.56.106    worker-w003   <none>           <none>
2026-01-05 14:54:34,648 [INFO]   kube-system   kube-proxy-kmgqr                                        1/1     Running     0               10d     192.168.56.105    worker-w002   <none>           <none>
2026-01-05 14:54:34,648 [INFO]   kube-system   kube-proxy-rdbz5                                        1/1     Running     0               10d     192.168.56.107    worker-w004   <none>           <none>
2026-01-05 14:54:34,648 [INFO]   kube-system   kube-proxy-w8mnb                                        1/1     Running     0               10d     192.168.56.103    master-m003   <none>           <none>
2026-01-05 14:54:34,648 [INFO]   kube-system   kube-proxy-x9jxr                                        1/1     Running     0               10d     192.168.56.104    worker-w001   <none>           <none>
2026-01-05 14:54:34,648 [INFO]   kube-system   kube-scheduler-master-m001                              1/1     Running     11 (38m ago)    10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:54:34,648 [INFO] 
Pod distribution by node:
2026-01-05 14:54:34,793 [INFO]   Node master-m003: 33 pods
2026-01-05 14:54:34,793 [INFO]   Node worker-w005: 13 pods
2026-01-05 14:54:34,793 [INFO]   Node worker-w002: 3 pods
2026-01-05 14:54:34,794 [INFO]   Node worker-w006: 3 pods
2026-01-05 14:54:34,794 [INFO]   Node worker-w001: 3 pods
2026-01-05 14:54:34,794 [INFO]   Node master-m002: 2 pods
2026-01-05 14:54:34,794 [INFO]   Node master-m001: 3 pods
2026-01-05 14:54:34,794 [INFO]   Node worker-w003: 2 pods
2026-01-05 14:54:34,794 [INFO]   Node worker-w004: 2 pods
2026-01-05 14:54:34,794 [INFO]   Node 10d: 3 pods
2026-01-05 14:54:34,794 [INFO] 
Filtering for simulation services:
2026-01-05 14:54:34,884 [INFO] Node master-m001 is Ready
2026-01-05 14:54:34,884 [INFO] Node master-m002 is Ready
2026-01-05 14:54:34,885 [INFO] Node master-m003 is Ready
2026-01-05 14:54:34,885 [INFO] Node worker-w001 is Ready
2026-01-05 14:54:34,885 [INFO] Node worker-w002 is Ready
2026-01-05 14:54:34,885 [INFO] Node worker-w003 is Ready
2026-01-05 14:54:34,885 [INFO] Node worker-w004 is Ready
2026-01-05 14:54:34,885 [INFO] Node worker-w005 is Ready
2026-01-05 14:54:34,885 [INFO] Node worker-w006 is Ready
2026-01-05 14:54:34,888 [WARNING] No pods found for etcd-sim
2026-01-05 14:54:34,890 [WARNING] No pods found for postgres-sim
2026-01-05 14:54:34,892 [WARNING] No pods found for redis-sim
2026-01-05 14:54:34,894 [WARNING] No pods found for nginx-sim
2026-01-05 14:54:34,896 [WARNING] No pods found for auth-sim
2026-01-05 14:54:34,896 [INFO] Completed full health check
2026-01-05 14:54:34,896 [INFO] Zone R2 remains down for 10 seconds
2026-01-05 14:54:44,906 [INFO] Running health check before rack power on
2026-01-05 14:54:44,907 [INFO] Starting full health check
2026-01-05 14:54:44,907 [INFO] 
============ DETAILED NODE STATUS ============
2026-01-05 14:54:44,907 [INFO] Basic Node Information (kubectl get nodes -o wide):
2026-01-05 14:54:44,967 [INFO]   NAME          STATUS                     ROLES           AGE   VERSION    INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
2026-01-05 14:54:44,967 [INFO]   master-m001   Ready                      control-plane   10d   v1.32.11   192.168.56.101   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:54:44,967 [INFO]   master-m002   Ready,SchedulingDisabled   control-plane   10d   v1.32.11   192.168.56.102   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:54:44,967 [INFO]   master-m003   Ready                      control-plane   10d   v1.32.11   192.168.56.103   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:54:44,967 [INFO]   worker-w001   Ready                      <none>          10d   v1.32.11   192.168.56.104   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:54:44,967 [INFO]   worker-w002   Ready                      <none>          10d   v1.32.11   192.168.56.105   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:54:44,967 [INFO]   worker-w003   Ready,SchedulingDisabled   <none>          10d   v1.32.11   192.168.56.106   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:54:44,967 [INFO]   worker-w004   Ready,SchedulingDisabled   <none>          10d   v1.32.11   192.168.56.107   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:54:44,967 [INFO]   worker-w005   Ready                      <none>          10d   v1.32.11   192.168.56.108   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:54:44,967 [INFO]   worker-w006   Ready                      <none>          10d   v1.32.11   192.168.56.109   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:54:44,967 [INFO] 
Enhanced Node Status (with taint and cordon indicators):
2026-01-05 14:54:44,968 [INFO]   NAME                STATUS    ROLES           ZONE   CORDONED   TAINTS
2026-01-05 14:54:45,198 [INFO]   master-m001     Ready  ✓ worker         R1    No       node-role.kubernetes.io/control-plane 
2026-01-05 14:54:45,342 [INFO]   master-m002     Ready  ✓ worker         R2    YES     ⚠️ simulated-failure, node.kubernetes.io/unschedulable ⚠️
2026-01-05 14:54:45,494 [INFO]   master-m003     Ready  ✓ worker         R3    No       None 
2026-01-05 14:54:45,715 [INFO]   worker-w001     Ready  ✓ worker         R1    No       None 
2026-01-05 14:54:45,887 [INFO]   worker-w002     Ready  ✓ worker         R1    No       None 
2026-01-05 14:54:46,064 [INFO]   worker-w003     Ready  ✓ worker         R2    YES     ⚠️ simulated-failure, node.kubernetes.io/unschedulable ⚠️
2026-01-05 14:54:46,268 [INFO]   worker-w004     Ready  ✓ worker         R2    YES     ⚠️ simulated-failure, node.kubernetes.io/unschedulable ⚠️
2026-01-05 14:54:46,670 [INFO]   worker-w005     Ready  ✓ worker         R3    No       None 
2026-01-05 14:54:46,929 [INFO]   worker-w006     Ready  ✓ worker         R3    No       None 
2026-01-05 14:54:46,930 [INFO] 
Legend:
2026-01-05 14:54:46,930 [INFO]   ✓ = Node is Ready
2026-01-05 14:54:46,930 [INFO]   ⚠️ = Warning indicator (NotReady, Cordoned, or has simulated-failure taint)
2026-01-05 14:54:46,930 [INFO] ============ DETAILED POD INFORMATION ============
2026-01-05 14:54:46,931 [INFO] Running 'kubectl get pods -o wide' to show detailed pod placement:
2026-01-05 14:54:47,033 [INFO]   NAMESPACE     NAME                                                    READY   STATUS      RESTARTS        AGE     IP                NODE          NOMINATED NODE   READINESS GATES
2026-01-05 14:54:47,034 [INFO]   argo          argo-server-5c69cb69db-gdkl6                            1/1     Running     0               8d      192.168.221.65    master-m003   <none>           <none>
2026-01-05 14:54:47,034 [INFO]   argo          resilience-bench-ptngm-initialize-metrics-940927476     0/2     Completed   0               2d16h   192.168.221.125   master-m003   <none>           <none>
2026-01-05 14:54:47,034 [INFO]   argo          resilience-bench-ptngm-run-health-check-1168793163      0/2     Completed   0               2d16h   192.168.221.67    master-m003   <none>           <none>
2026-01-05 14:54:47,034 [INFO]   argo          resilience-bench-ptngm-run-health-check-1185570782      0/2     Completed   0               2d16h   192.168.221.66    master-m003   <none>           <none>
2026-01-05 14:54:47,034 [INFO]   argo          resilience-bench-ptngm-run-health-check-1202348401      0/2     Completed   0               2d16h   192.168.221.122   master-m003   <none>           <none>
2026-01-05 14:54:47,035 [INFO]   argo          resilience-bench-wf87f-initialize-metrics-3677018408    0/2     Completed   0               2d18h   192.168.221.100   master-m003   <none>           <none>
2026-01-05 14:54:47,035 [INFO]   argo          resilience-bench-wf87f-run-health-check-2914150102      0/2     Completed   0               2d18h   192.168.221.105   master-m003   <none>           <none>
2026-01-05 14:54:47,035 [INFO]   argo          resilience-bench-wf87f-run-health-check-3931060015      0/2     Completed   0               2d18h   192.168.221.104   master-m003   <none>           <none>
2026-01-05 14:54:47,035 [INFO]   argo          resilience-bench-wf87f-run-health-check-3947837634      0/2     Completed   0               2d18h   192.168.221.98    master-m003   <none>           <none>
2026-01-05 14:54:47,035 [INFO]   argo          resilience-bench-wf87f-run-health-check-3964615253      0/2     Completed   0               2d18h   192.168.221.103   master-m003   <none>           <none>
2026-01-05 14:54:47,035 [INFO]   argo          resilience-bench-wf87f-run-node-simulation-4173146970   0/2     Completed   0               2d18h   192.168.221.101   master-m003   <none>           <none>
2026-01-05 14:54:47,035 [INFO]   argo          resilience-bench-wf87f-run-rack-simulation-1356070019   0/2     Completed   0               2d18h   192.168.221.109   master-m003   <none>           <none>
2026-01-05 14:54:47,035 [INFO]   argo          resilience-heft-4cqbx-health-check-1589398418           0/2     Error       0               91m     192.168.15.231    worker-w005   <none>           <none>
2026-01-05 14:54:47,035 [INFO]   argo          resilience-heft-4cqbx-heft-initialize-2594215169        0/2     Completed   0               92m     192.168.221.79    master-m003   <none>           <none>
2026-01-05 14:54:47,035 [INFO]   argo          resilience-heft-76kbp-health-check-270747540            0/2     Error       0               82m     192.168.15.237    worker-w005   <none>           <none>
2026-01-05 14:54:47,035 [INFO]   argo          resilience-heft-76kbp-heft-initialize-3574960751        0/2     Completed   0               83m     192.168.221.86    master-m003   <none>           <none>
2026-01-05 14:54:47,035 [INFO]   argo          resilience-heft-b85xr-health-check-2840770649           0/2     Error       0               79m     192.168.15.240    worker-w005   <none>           <none>
2026-01-05 14:54:47,036 [INFO]   argo          resilience-heft-b85xr-heft-initialize-1754406316        0/2     Completed   0               79m     192.168.221.88    master-m003   <none>           <none>
2026-01-05 14:54:47,036 [INFO]   argo          resilience-heft-fdw9f-health-check-1896357264           0/2     Error       0               84m     192.168.15.235    worker-w005   <none>           <none>
2026-01-05 14:54:47,036 [INFO]   argo          resilience-heft-fdw9f-heft-initialize-2651619139        0/2     Completed   0               85m     192.168.221.77    master-m003   <none>           <none>
2026-01-05 14:54:47,036 [INFO]   argo          resilience-heft-gf256-health-check-980174358            0/2     Error       0               83m     192.168.15.236    worker-w005   <none>           <none>
2026-01-05 14:54:47,036 [INFO]   argo          resilience-heft-gf256-heft-initialize-1548293021        0/2     Completed   0               84m     192.168.221.84    master-m003   <none>           <none>
2026-01-05 14:54:47,036 [INFO]   argo          resilience-heft-gxwpg-health-check-3445304423           0/2     Error       0               86m     192.168.15.233    worker-w005   <none>           <none>
2026-01-05 14:54:47,036 [INFO]   argo          resilience-heft-gxwpg-heft-initialize-2377397022        0/2     Completed   0               87m     192.168.221.81    master-m003   <none>           <none>
2026-01-05 14:54:47,036 [INFO]   argo          resilience-heft-pc2kb-health-check-3258812212           0/2     Error       0               81m     192.168.15.238    worker-w005   <none>           <none>
2026-01-05 14:54:47,036 [INFO]   argo          resilience-heft-pc2kb-heft-initialize-321333135         0/2     Completed   0               81m     192.168.221.83    master-m003   <none>           <none>
2026-01-05 14:54:47,036 [INFO]   argo          resilience-heft-qsmch-health-check-283908068            0/2     Error       0               88m     192.168.15.232    worker-w005   <none>           <none>
2026-01-05 14:54:47,036 [INFO]   argo          resilience-heft-qsmch-heft-initialize-1908810015        0/2     Completed   0               88m     192.168.221.82    master-m003   <none>           <none>
2026-01-05 14:54:47,036 [INFO]   argo          resilience-heft-r2w8l-health-check-366927355            0/2     Error       0               80m     192.168.15.239    worker-w005   <none>           <none>
2026-01-05 14:54:47,036 [INFO]   argo          resilience-heft-r2w8l-heft-initialize-3054048250        0/2     Completed   0               80m     192.168.221.85    master-m003   <none>           <none>
2026-01-05 14:54:47,037 [INFO]   argo          resilience-heft-t988k-health-check-612195296            0/2     Error       0               85m     192.168.15.234    worker-w005   <none>           <none>
2026-01-05 14:54:47,037 [INFO]   argo          resilience-heft-t988k-heft-initialize-164752883         0/2     Completed   0               86m     192.168.221.80    master-m003   <none>           <none>
2026-01-05 14:54:47,037 [INFO]   argo          resilience-heft-vlt2m-health-check-2709447919           0/2     Error       0               42m     192.168.221.119   master-m003   <none>           <none>
2026-01-05 14:54:47,037 [INFO]   argo          resilience-heft-vlt2m-health-check-2726225538           0/2     Error       0               42m     192.168.221.120   master-m003   <none>           <none>
2026-01-05 14:54:47,037 [INFO]   argo          resilience-heft-vlt2m-health-check-2743003157           0/2     Error       0               42m     192.168.221.121   master-m003   <none>           <none>
2026-01-05 14:54:47,037 [INFO]   argo          resilience-heft-vlt2m-heft-initialize-840404112         0/2     Completed   0               42m     192.168.221.118   master-m003   <none>           <none>
2026-01-05 14:54:47,037 [INFO]   argo          resilience-heft-x86wd-health-check-2184182048           0/2     Error       0               40m     192.168.221.123   master-m003   <none>           <none>
2026-01-05 14:54:47,037 [INFO]   argo          resilience-heft-x86wd-health-check-2200959667           0/2     Error       0               40m     192.168.221.125   master-m003   <none>           <none>
2026-01-05 14:54:47,037 [INFO]   argo          resilience-heft-x86wd-health-check-2234514905           0/2     Error       0               40m     192.168.221.124   master-m003   <none>           <none>
2026-01-05 14:54:47,037 [INFO]   argo          resilience-heft-x86wd-heft-initialize-2152040770        0/2     Completed   0               41m     192.168.221.127   master-m003   <none>           <none>
2026-01-05 14:54:47,037 [INFO]   argo          workflow-controller-ccbd949dc-t4rx9                     1/1     Running     0               6m57s   192.168.195.243   worker-w002   <none>           <none>
2026-01-05 14:54:47,037 [INFO]   default       resilience-heft-2-202047-tbk8r                          1/1     Running     0               3m59s   192.168.221.126   master-m003   <none>           <none>
2026-01-05 14:54:47,037 [INFO]   kube-system   calico-kube-controllers-7498b9bb4c-xd7tb                1/1     Running     0               38h     192.168.191.108   worker-w006   <none>           <none>
2026-01-05 14:54:47,038 [INFO]   kube-system   calico-node-4zhd4                                       1/1     Running     0               10d     192.168.56.105    worker-w002   <none>           <none>
2026-01-05 14:54:47,038 [INFO]   kube-system   calico-node-75nx6                                       1/1     Running     0               10d     192.168.56.109    worker-w006   <none>           <none>
2026-01-05 14:54:47,038 [INFO]   kube-system   calico-node-7lkdq                                       1/1     Running     0               10d     192.168.56.104    worker-w001   <none>           <none>
2026-01-05 14:54:47,038 [INFO]   kube-system   calico-node-85f8c                                       1/1     Running     0               10d     192.168.56.102    master-m002   <none>           <none>
2026-01-05 14:54:47,038 [INFO]   kube-system   calico-node-j8nb9                                       1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:54:47,038 [INFO]   kube-system   calico-node-lbcb2                                       1/1     Running     0               10d     192.168.56.106    worker-w003   <none>           <none>
2026-01-05 14:54:47,038 [INFO]   kube-system   calico-node-wlr5v                                       1/1     Running     0               10d     192.168.56.107    worker-w004   <none>           <none>
2026-01-05 14:54:47,038 [INFO]   kube-system   calico-node-xnzjw                                       1/1     Running     0               10d     192.168.56.103    master-m003   <none>           <none>
2026-01-05 14:54:47,038 [INFO]   kube-system   calico-node-xsltn                                       1/1     Running     0               10d     192.168.56.108    worker-w005   <none>           <none>
2026-01-05 14:54:47,038 [INFO]   kube-system   coredns-668d6bf9bc-2f74f                                1/1     Running     0               40h     192.168.15.230    worker-w005   <none>           <none>
2026-01-05 14:54:47,038 [INFO]   kube-system   coredns-668d6bf9bc-qnzjq                                1/1     Running     0               79s     192.168.132.176   worker-w001   <none>           <none>
2026-01-05 14:54:47,038 [INFO]   kube-system   etcd-master-m001                                        1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:54:47,038 [INFO]   kube-system   kube-apiserver-master-m001                              1/1     Running     1 (2d11h ago)   10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:54:47,039 [INFO]   kube-system   kube-controller-manager-master-m001                     1/1     Running     12 (39m ago)    10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:54:47,039 [INFO]   kube-system   kube-proxy-5lzhj                                        1/1     Running     0               10d     192.168.56.102    master-m002   <none>           <none>
2026-01-05 14:54:47,039 [INFO]   kube-system   kube-proxy-7jqkv                                        1/1     Running     0               10d     192.168.56.109    worker-w006   <none>           <none>
2026-01-05 14:54:47,039 [INFO]   kube-system   kube-proxy-cv5dt                                        1/1     Running     0               10d     192.168.56.108    worker-w005   <none>           <none>
2026-01-05 14:54:47,039 [INFO]   kube-system   kube-proxy-fvpmr                                        1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:54:47,039 [INFO]   kube-system   kube-proxy-hgs5z                                        1/1     Running     0               10d     192.168.56.106    worker-w003   <none>           <none>
2026-01-05 14:54:47,039 [INFO]   kube-system   kube-proxy-kmgqr                                        1/1     Running     0               10d     192.168.56.105    worker-w002   <none>           <none>
2026-01-05 14:54:47,039 [INFO]   kube-system   kube-proxy-rdbz5                                        1/1     Running     0               10d     192.168.56.107    worker-w004   <none>           <none>
2026-01-05 14:54:47,039 [INFO]   kube-system   kube-proxy-w8mnb                                        1/1     Running     0               10d     192.168.56.103    master-m003   <none>           <none>
2026-01-05 14:54:47,039 [INFO]   kube-system   kube-proxy-x9jxr                                        1/1     Running     0               10d     192.168.56.104    worker-w001   <none>           <none>
2026-01-05 14:54:47,039 [INFO]   kube-system   kube-scheduler-master-m001                              1/1     Running     11 (38m ago)    10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:54:47,039 [INFO] 
Pod distribution by node:
2026-01-05 14:54:47,143 [INFO]   Node master-m003: 33 pods
2026-01-05 14:54:47,144 [INFO]   Node worker-w005: 13 pods
2026-01-05 14:54:47,144 [INFO]   Node worker-w002: 3 pods
2026-01-05 14:54:47,144 [INFO]   Node worker-w006: 3 pods
2026-01-05 14:54:47,144 [INFO]   Node worker-w001: 3 pods
2026-01-05 14:54:47,144 [INFO]   Node master-m002: 2 pods
2026-01-05 14:54:47,144 [INFO]   Node master-m001: 3 pods
2026-01-05 14:54:47,144 [INFO]   Node worker-w003: 2 pods
2026-01-05 14:54:47,145 [INFO]   Node worker-w004: 2 pods
2026-01-05 14:54:47,145 [INFO]   Node 10d: 3 pods
2026-01-05 14:54:47,145 [INFO] 
Filtering for simulation services:
2026-01-05 14:54:47,229 [INFO] Node master-m001 is Ready
2026-01-05 14:54:47,230 [INFO] Node master-m002 is Ready
2026-01-05 14:54:47,230 [INFO] Node master-m003 is Ready
2026-01-05 14:54:47,230 [INFO] Node worker-w001 is Ready
2026-01-05 14:54:47,230 [INFO] Node worker-w002 is Ready
2026-01-05 14:54:47,230 [INFO] Node worker-w003 is Ready
2026-01-05 14:54:47,231 [INFO] Node worker-w004 is Ready
2026-01-05 14:54:47,231 [INFO] Node worker-w005 is Ready
2026-01-05 14:54:47,231 [INFO] Node worker-w006 is Ready
2026-01-05 14:54:47,234 [WARNING] No pods found for etcd-sim
2026-01-05 14:54:47,236 [WARNING] No pods found for postgres-sim
2026-01-05 14:54:47,238 [WARNING] No pods found for redis-sim
2026-01-05 14:54:47,240 [WARNING] No pods found for nginx-sim
2026-01-05 14:54:47,241 [WARNING] No pods found for auth-sim
2026-01-05 14:54:47,242 [INFO] Completed full health check
2026-01-05 14:54:47,242 [INFO] Simulating node recovery for master-m002 using Kubernetes API
2026-01-05 14:54:47,256 [INFO] Removed simulated-failure taint from node master-m002
2026-01-05 14:54:47,268 [INFO] Node master-m002 uncordoned and ready
2026-01-05 14:54:47,268 [INFO] Node master-m002 powered on (delay 5s)
2026-01-05 14:54:52,270 [INFO] Simulating node recovery for worker-w003 using Kubernetes API
2026-01-05 14:54:52,286 [INFO] Removed simulated-failure taint from node worker-w003
2026-01-05 14:54:52,298 [INFO] Node worker-w003 uncordoned and ready
2026-01-05 14:54:52,299 [INFO] Node worker-w003 powered on (delay 5s)
2026-01-05 14:54:57,300 [INFO] Simulating node recovery for worker-w004 using Kubernetes API
2026-01-05 14:54:57,316 [INFO] Removed simulated-failure taint from node worker-w004
2026-01-05 14:54:57,328 [INFO] Node worker-w004 uncordoned and ready
2026-01-05 14:54:57,329 [INFO] Node worker-w004 powered on (delay 5s)
2026-01-05 14:55:02,333 [INFO] Waiting 60 seconds for the cluster to stabilize after recovery...
2026-01-05 14:56:02,366 [INFO] Running final health check
2026-01-05 14:56:02,366 [INFO] Starting full health check
2026-01-05 14:56:02,366 [INFO] 
============ DETAILED NODE STATUS ============
2026-01-05 14:56:02,366 [INFO] Basic Node Information (kubectl get nodes -o wide):
2026-01-05 14:56:02,431 [INFO]   NAME          STATUS   ROLES           AGE   VERSION    INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
2026-01-05 14:56:02,431 [INFO]   master-m001   Ready    control-plane   10d   v1.32.11   192.168.56.101   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:56:02,431 [INFO]   master-m002   Ready    control-plane   10d   v1.32.11   192.168.56.102   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:56:02,431 [INFO]   master-m003   Ready    control-plane   10d   v1.32.11   192.168.56.103   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:56:02,431 [INFO]   worker-w001   Ready    <none>          10d   v1.32.11   192.168.56.104   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:56:02,431 [INFO]   worker-w002   Ready    <none>          10d   v1.32.11   192.168.56.105   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:56:02,431 [INFO]   worker-w003   Ready    <none>          10d   v1.32.11   192.168.56.106   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:56:02,432 [INFO]   worker-w004   Ready    <none>          10d   v1.32.11   192.168.56.107   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:56:02,432 [INFO]   worker-w005   Ready    <none>          10d   v1.32.11   192.168.56.108   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:56:02,432 [INFO]   worker-w006   Ready    <none>          10d   v1.32.11   192.168.56.109   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:56:02,432 [INFO] 
Enhanced Node Status (with taint and cordon indicators):
2026-01-05 14:56:02,432 [INFO]   NAME                STATUS    ROLES           ZONE   CORDONED   TAINTS
2026-01-05 14:56:02,857 [INFO]   master-m001     Ready  ✓ worker         R1    No       node-role.kubernetes.io/control-plane 
2026-01-05 14:56:03,039 [INFO]   master-m002     Ready  ✓ worker         R2    No       None 
2026-01-05 14:56:03,251 [INFO]   master-m003     Ready  ✓ worker         R3    No       None 
2026-01-05 14:56:03,426 [INFO]   worker-w001     Ready  ✓ worker         R1    No       None 
2026-01-05 14:56:03,622 [INFO]   worker-w002     Ready  ✓ worker         R1    No       None 
2026-01-05 14:56:03,895 [INFO]   worker-w003     Ready  ✓ worker         R2    No       None 
2026-01-05 14:56:04,088 [INFO]   worker-w004     Ready  ✓ worker         R2    No       None 
2026-01-05 14:56:04,302 [INFO]   worker-w005     Ready  ✓ worker         R3    No       None 
2026-01-05 14:56:04,627 [INFO]   worker-w006     Ready  ✓ worker         R3    No       None 
2026-01-05 14:56:04,627 [INFO] 
Legend:
2026-01-05 14:56:04,627 [INFO]   ✓ = Node is Ready
2026-01-05 14:56:04,627 [INFO]   ⚠️ = Warning indicator (NotReady, Cordoned, or has simulated-failure taint)
2026-01-05 14:56:04,628 [INFO] ============ DETAILED POD INFORMATION ============
2026-01-05 14:56:04,628 [INFO] Running 'kubectl get pods -o wide' to show detailed pod placement:
2026-01-05 14:56:04,749 [INFO]   NAMESPACE     NAME                                                    READY   STATUS      RESTARTS        AGE     IP                NODE          NOMINATED NODE   READINESS GATES
2026-01-05 14:56:04,750 [INFO]   argo          argo-server-5c69cb69db-gdkl6                            1/1     Running     0               8d      192.168.221.65    master-m003   <none>           <none>
2026-01-05 14:56:04,750 [INFO]   argo          resilience-bench-ptngm-initialize-metrics-940927476     0/2     Completed   0               2d16h   192.168.221.125   master-m003   <none>           <none>
2026-01-05 14:56:04,750 [INFO]   argo          resilience-bench-ptngm-run-health-check-1168793163      0/2     Completed   0               2d16h   192.168.221.67    master-m003   <none>           <none>
2026-01-05 14:56:04,750 [INFO]   argo          resilience-bench-ptngm-run-health-check-1185570782      0/2     Completed   0               2d16h   192.168.221.66    master-m003   <none>           <none>
2026-01-05 14:56:04,750 [INFO]   argo          resilience-bench-ptngm-run-health-check-1202348401      0/2     Completed   0               2d16h   192.168.221.122   master-m003   <none>           <none>
2026-01-05 14:56:04,750 [INFO]   argo          resilience-bench-wf87f-initialize-metrics-3677018408    0/2     Completed   0               2d18h   192.168.221.100   master-m003   <none>           <none>
2026-01-05 14:56:04,750 [INFO]   argo          resilience-bench-wf87f-run-health-check-2914150102      0/2     Completed   0               2d18h   192.168.221.105   master-m003   <none>           <none>
2026-01-05 14:56:04,750 [INFO]   argo          resilience-bench-wf87f-run-health-check-3931060015      0/2     Completed   0               2d18h   192.168.221.104   master-m003   <none>           <none>
2026-01-05 14:56:04,750 [INFO]   argo          resilience-bench-wf87f-run-health-check-3947837634      0/2     Completed   0               2d18h   192.168.221.98    master-m003   <none>           <none>
2026-01-05 14:56:04,750 [INFO]   argo          resilience-bench-wf87f-run-health-check-3964615253      0/2     Completed   0               2d18h   192.168.221.103   master-m003   <none>           <none>
2026-01-05 14:56:04,750 [INFO]   argo          resilience-bench-wf87f-run-node-simulation-4173146970   0/2     Completed   0               2d18h   192.168.221.101   master-m003   <none>           <none>
2026-01-05 14:56:04,750 [INFO]   argo          resilience-bench-wf87f-run-rack-simulation-1356070019   0/2     Completed   0               2d18h   192.168.221.109   master-m003   <none>           <none>
2026-01-05 14:56:04,751 [INFO]   argo          resilience-heft-4cqbx-health-check-1589398418           0/2     Error       0               92m     192.168.15.231    worker-w005   <none>           <none>
2026-01-05 14:56:04,751 [INFO]   argo          resilience-heft-4cqbx-heft-initialize-2594215169        0/2     Completed   0               93m     192.168.221.79    master-m003   <none>           <none>
2026-01-05 14:56:04,751 [INFO]   argo          resilience-heft-76kbp-health-check-270747540            0/2     Error       0               84m     192.168.15.237    worker-w005   <none>           <none>
2026-01-05 14:56:04,751 [INFO]   argo          resilience-heft-76kbp-heft-initialize-3574960751        0/2     Completed   0               84m     192.168.221.86    master-m003   <none>           <none>
2026-01-05 14:56:04,751 [INFO]   argo          resilience-heft-b85xr-health-check-2840770649           0/2     Error       0               80m     192.168.15.240    worker-w005   <none>           <none>
2026-01-05 14:56:04,751 [INFO]   argo          resilience-heft-b85xr-heft-initialize-1754406316        0/2     Completed   0               81m     192.168.221.88    master-m003   <none>           <none>
2026-01-05 14:56:04,751 [INFO]   argo          resilience-heft-fdw9f-health-check-1896357264           0/2     Error       0               86m     192.168.15.235    worker-w005   <none>           <none>
2026-01-05 14:56:04,751 [INFO]   argo          resilience-heft-fdw9f-heft-initialize-2651619139        0/2     Completed   0               86m     192.168.221.77    master-m003   <none>           <none>
2026-01-05 14:56:04,751 [INFO]   argo          resilience-heft-gf256-health-check-980174358            0/2     Error       0               85m     192.168.15.236    worker-w005   <none>           <none>
2026-01-05 14:56:04,751 [INFO]   argo          resilience-heft-gf256-heft-initialize-1548293021        0/2     Completed   0               85m     192.168.221.84    master-m003   <none>           <none>
2026-01-05 14:56:04,751 [INFO]   argo          resilience-heft-gxwpg-health-check-3445304423           0/2     Error       0               88m     192.168.15.233    worker-w005   <none>           <none>
2026-01-05 14:56:04,751 [INFO]   argo          resilience-heft-gxwpg-heft-initialize-2377397022        0/2     Completed   0               88m     192.168.221.81    master-m003   <none>           <none>
2026-01-05 14:56:04,751 [INFO]   argo          resilience-heft-pc2kb-health-check-3258812212           0/2     Error       0               82m     192.168.15.238    worker-w005   <none>           <none>
2026-01-05 14:56:04,751 [INFO]   argo          resilience-heft-pc2kb-heft-initialize-321333135         0/2     Completed   0               83m     192.168.221.83    master-m003   <none>           <none>
2026-01-05 14:56:04,751 [INFO]   argo          resilience-heft-qsmch-health-check-283908068            0/2     Error       0               89m     192.168.15.232    worker-w005   <none>           <none>
2026-01-05 14:56:04,751 [INFO]   argo          resilience-heft-qsmch-heft-initialize-1908810015        0/2     Completed   0               90m     192.168.221.82    master-m003   <none>           <none>
2026-01-05 14:56:04,751 [INFO]   argo          resilience-heft-r2w8l-health-check-366927355            0/2     Error       0               81m     192.168.15.239    worker-w005   <none>           <none>
2026-01-05 14:56:04,751 [INFO]   argo          resilience-heft-r2w8l-heft-initialize-3054048250        0/2     Completed   0               82m     192.168.221.85    master-m003   <none>           <none>
2026-01-05 14:56:04,752 [INFO]   argo          resilience-heft-t988k-health-check-612195296            0/2     Error       0               87m     192.168.15.234    worker-w005   <none>           <none>
2026-01-05 14:56:04,752 [INFO]   argo          resilience-heft-t988k-heft-initialize-164752883         0/2     Completed   0               87m     192.168.221.80    master-m003   <none>           <none>
2026-01-05 14:56:04,752 [INFO]   argo          resilience-heft-vlt2m-health-check-2709447919           0/2     Error       0               43m     192.168.221.119   master-m003   <none>           <none>
2026-01-05 14:56:04,752 [INFO]   argo          resilience-heft-vlt2m-health-check-2726225538           0/2     Error       0               43m     192.168.221.120   master-m003   <none>           <none>
2026-01-05 14:56:04,752 [INFO]   argo          resilience-heft-vlt2m-health-check-2743003157           0/2     Error       0               43m     192.168.221.121   master-m003   <none>           <none>
2026-01-05 14:56:04,752 [INFO]   argo          resilience-heft-vlt2m-heft-initialize-840404112         0/2     Completed   0               44m     192.168.221.118   master-m003   <none>           <none>
2026-01-05 14:56:04,752 [INFO]   argo          resilience-heft-x86wd-health-check-2184182048           0/2     Error       0               42m     192.168.221.123   master-m003   <none>           <none>
2026-01-05 14:56:04,752 [INFO]   argo          resilience-heft-x86wd-health-check-2200959667           0/2     Error       0               42m     192.168.221.125   master-m003   <none>           <none>
2026-01-05 14:56:04,752 [INFO]   argo          resilience-heft-x86wd-health-check-2234514905           0/2     Error       0               42m     192.168.221.124   master-m003   <none>           <none>
2026-01-05 14:56:04,752 [INFO]   argo          resilience-heft-x86wd-heft-initialize-2152040770        0/2     Completed   0               42m     192.168.221.127   master-m003   <none>           <none>
2026-01-05 14:56:04,752 [INFO]   argo          workflow-controller-ccbd949dc-t4rx9                     1/1     Running     0               8m15s   192.168.195.243   worker-w002   <none>           <none>
2026-01-05 14:56:04,752 [INFO]   default       resilience-heft-2-202047-tbk8r                          1/1     Running     0               5m17s   192.168.221.126   master-m003   <none>           <none>
2026-01-05 14:56:04,752 [INFO]   kube-system   calico-kube-controllers-7498b9bb4c-xd7tb                1/1     Running     0               38h     192.168.191.108   worker-w006   <none>           <none>
2026-01-05 14:56:04,752 [INFO]   kube-system   calico-node-4zhd4                                       1/1     Running     0               10d     192.168.56.105    worker-w002   <none>           <none>
2026-01-05 14:56:04,752 [INFO]   kube-system   calico-node-75nx6                                       1/1     Running     0               10d     192.168.56.109    worker-w006   <none>           <none>
2026-01-05 14:56:04,752 [INFO]   kube-system   calico-node-7lkdq                                       1/1     Running     0               10d     192.168.56.104    worker-w001   <none>           <none>
2026-01-05 14:56:04,752 [INFO]   kube-system   calico-node-85f8c                                       1/1     Running     0               10d     192.168.56.102    master-m002   <none>           <none>
2026-01-05 14:56:04,753 [INFO]   kube-system   calico-node-j8nb9                                       1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:56:04,753 [INFO]   kube-system   calico-node-lbcb2                                       1/1     Running     0               10d     192.168.56.106    worker-w003   <none>           <none>
2026-01-05 14:56:04,753 [INFO]   kube-system   calico-node-wlr5v                                       1/1     Running     0               10d     192.168.56.107    worker-w004   <none>           <none>
2026-01-05 14:56:04,753 [INFO]   kube-system   calico-node-xnzjw                                       1/1     Running     0               10d     192.168.56.103    master-m003   <none>           <none>
2026-01-05 14:56:04,753 [INFO]   kube-system   calico-node-xsltn                                       1/1     Running     0               10d     192.168.56.108    worker-w005   <none>           <none>
2026-01-05 14:56:04,753 [INFO]   kube-system   coredns-668d6bf9bc-2f74f                                1/1     Running     0               40h     192.168.15.230    worker-w005   <none>           <none>
2026-01-05 14:56:04,753 [INFO]   kube-system   coredns-668d6bf9bc-qnzjq                                1/1     Running     0               2m37s   192.168.132.176   worker-w001   <none>           <none>
2026-01-05 14:56:04,753 [INFO]   kube-system   etcd-master-m001                                        1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:56:04,753 [INFO]   kube-system   kube-apiserver-master-m001                              1/1     Running     1 (2d11h ago)   10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:56:04,753 [INFO]   kube-system   kube-controller-manager-master-m001                     1/1     Running     12 (40m ago)    10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:56:04,753 [INFO]   kube-system   kube-proxy-5lzhj                                        1/1     Running     0               10d     192.168.56.102    master-m002   <none>           <none>
2026-01-05 14:56:04,753 [INFO]   kube-system   kube-proxy-7jqkv                                        1/1     Running     0               10d     192.168.56.109    worker-w006   <none>           <none>
2026-01-05 14:56:04,753 [INFO]   kube-system   kube-proxy-cv5dt                                        1/1     Running     0               10d     192.168.56.108    worker-w005   <none>           <none>
2026-01-05 14:56:04,753 [INFO]   kube-system   kube-proxy-fvpmr                                        1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:56:04,753 [INFO]   kube-system   kube-proxy-hgs5z                                        1/1     Running     0               10d     192.168.56.106    worker-w003   <none>           <none>
2026-01-05 14:56:04,753 [INFO]   kube-system   kube-proxy-kmgqr                                        1/1     Running     0               10d     192.168.56.105    worker-w002   <none>           <none>
2026-01-05 14:56:04,753 [INFO]   kube-system   kube-proxy-rdbz5                                        1/1     Running     0               10d     192.168.56.107    worker-w004   <none>           <none>
2026-01-05 14:56:04,753 [INFO]   kube-system   kube-proxy-w8mnb                                        1/1     Running     0               10d     192.168.56.103    master-m003   <none>           <none>
2026-01-05 14:56:04,754 [INFO]   kube-system   kube-proxy-x9jxr                                        1/1     Running     0               10d     192.168.56.104    worker-w001   <none>           <none>
2026-01-05 14:56:04,754 [INFO]   kube-system   kube-scheduler-master-m001                              1/1     Running     11 (40m ago)    10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:56:04,754 [INFO] 
Pod distribution by node:
2026-01-05 14:56:04,864 [INFO]   Node master-m003: 33 pods
2026-01-05 14:56:04,865 [INFO]   Node worker-w005: 13 pods
2026-01-05 14:56:04,865 [INFO]   Node worker-w002: 3 pods
2026-01-05 14:56:04,865 [INFO]   Node worker-w006: 3 pods
2026-01-05 14:56:04,865 [INFO]   Node worker-w001: 3 pods
2026-01-05 14:56:04,865 [INFO]   Node master-m002: 2 pods
2026-01-05 14:56:04,865 [INFO]   Node master-m001: 3 pods
2026-01-05 14:56:04,865 [INFO]   Node worker-w003: 2 pods
2026-01-05 14:56:04,865 [INFO]   Node worker-w004: 2 pods
2026-01-05 14:56:04,865 [INFO]   Node 10d: 3 pods
2026-01-05 14:56:04,865 [INFO] 
Filtering for simulation services:
2026-01-05 14:56:04,951 [INFO] Node master-m001 is Ready
2026-01-05 14:56:04,951 [INFO] Node master-m002 is Ready
2026-01-05 14:56:04,951 [INFO] Node master-m003 is Ready
2026-01-05 14:56:04,952 [INFO] Node worker-w001 is Ready
2026-01-05 14:56:04,952 [INFO] Node worker-w002 is Ready
2026-01-05 14:56:04,952 [INFO] Node worker-w003 is Ready
2026-01-05 14:56:04,952 [INFO] Node worker-w004 is Ready
2026-01-05 14:56:04,952 [INFO] Node worker-w005 is Ready
2026-01-05 14:56:04,952 [INFO] Node worker-w006 is Ready
2026-01-05 14:56:04,956 [WARNING] No pods found for etcd-sim
2026-01-05 14:56:04,957 [WARNING] No pods found for postgres-sim
2026-01-05 14:56:04,959 [WARNING] No pods found for redis-sim
2026-01-05 14:56:04,961 [WARNING] No pods found for nginx-sim
2026-01-05 14:56:04,963 [WARNING] No pods found for auth-sim
2026-01-05 14:56:04,963 [INFO] Completed full health check
2026-01-05 14:56:04,963 [INFO] Rack R2 has been fully restored
[2026-01-05 14:56:05] TIMING: RACK_SIMULATION completed in 168 seconds

================================================================
 STEP 5: FINAL HEALTH CHECK
 Time: Mon Jan  5 14:56:05 UTC 2026
================================================================
Log directory created/verified: /app/logs
File logging configured successfully
2026-01-05 14:56:05,388 [INFO] Loaded in-cluster Kubernetes config
2026-01-05 14:56:05,389 [INFO] Running on host: resilience-heft-2-202047-tbk8r
2026-01-05 14:56:05,389 [INFO] Detected current node: master-m003, zone: R3
2026-01-05 14:56:05,389 [INFO] Checking if we have permissions to modify nodes...
2026-01-05 14:56:05,420 [INFO] Testing permissions using node: master-m001
2026-01-05 14:56:05,428 [INFO] Permission check successful - we can modify nodes
2026-01-05 14:56:05,428 [INFO] Using real Kubernetes API for node control
2026-01-05 14:56:05,429 [INFO] Action received: health-check
2026-01-05 14:56:05,429 [INFO] Stabilization time: 10 seconds
2026-01-05 14:56:05,429 [INFO] Starting full health check
2026-01-05 14:56:05,429 [INFO] 
============ DETAILED NODE STATUS ============
2026-01-05 14:56:05,429 [INFO] Basic Node Information (kubectl get nodes -o wide):
2026-01-05 14:56:05,498 [INFO]   NAME          STATUS   ROLES           AGE   VERSION    INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
2026-01-05 14:56:05,498 [INFO]   master-m001   Ready    control-plane   10d   v1.32.11   192.168.56.101   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:56:05,498 [INFO]   master-m002   Ready    control-plane   10d   v1.32.11   192.168.56.102   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:56:05,498 [INFO]   master-m003   Ready    control-plane   10d   v1.32.11   192.168.56.103   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:56:05,498 [INFO]   worker-w001   Ready    <none>          10d   v1.32.11   192.168.56.104   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:56:05,498 [INFO]   worker-w002   Ready    <none>          10d   v1.32.11   192.168.56.105   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:56:05,499 [INFO]   worker-w003   Ready    <none>          10d   v1.32.11   192.168.56.106   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:56:05,499 [INFO]   worker-w004   Ready    <none>          10d   v1.32.11   192.168.56.107   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:56:05,499 [INFO]   worker-w005   Ready    <none>          10d   v1.32.11   192.168.56.108   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:56:05,499 [INFO]   worker-w006   Ready    <none>          10d   v1.32.11   192.168.56.109   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-05 14:56:05,499 [INFO] 
Enhanced Node Status (with taint and cordon indicators):
2026-01-05 14:56:05,499 [INFO]   NAME                STATUS    ROLES           ZONE   CORDONED   TAINTS
2026-01-05 14:56:05,765 [INFO]   master-m001     Ready  ✓ worker         R1    No       node-role.kubernetes.io/control-plane 
2026-01-05 14:56:05,939 [INFO]   master-m002     Ready  ✓ worker         R2    No       None 
2026-01-05 14:56:06,126 [INFO]   master-m003     Ready  ✓ worker         R3    No       None 
2026-01-05 14:56:06,338 [INFO]   worker-w001     Ready  ✓ worker         R1    No       None 
2026-01-05 14:56:06,662 [INFO]   worker-w002     Ready  ✓ worker         R1    No       None 
2026-01-05 14:56:06,925 [INFO]   worker-w003     Ready  ✓ worker         R2    No       None 
2026-01-05 14:56:07,115 [INFO]   worker-w004     Ready  ✓ worker         R2    No       None 
2026-01-05 14:56:07,302 [INFO]   worker-w005     Ready  ✓ worker         R3    No       None 
2026-01-05 14:56:07,477 [INFO]   worker-w006     Ready  ✓ worker         R3    No       None 
2026-01-05 14:56:07,477 [INFO] 
Legend:
2026-01-05 14:56:07,477 [INFO]   ✓ = Node is Ready
2026-01-05 14:56:07,477 [INFO]   ⚠️ = Warning indicator (NotReady, Cordoned, or has simulated-failure taint)
2026-01-05 14:56:07,478 [INFO] ============ DETAILED POD INFORMATION ============
2026-01-05 14:56:07,478 [INFO] Running 'kubectl get pods -o wide' to show detailed pod placement:
2026-01-05 14:56:07,580 [INFO]   NAMESPACE     NAME                                                    READY   STATUS      RESTARTS        AGE     IP                NODE          NOMINATED NODE   READINESS GATES
2026-01-05 14:56:07,581 [INFO]   argo          argo-server-5c69cb69db-gdkl6                            1/1     Running     0               8d      192.168.221.65    master-m003   <none>           <none>
2026-01-05 14:56:07,581 [INFO]   argo          resilience-bench-ptngm-initialize-metrics-940927476     0/2     Completed   0               2d16h   192.168.221.125   master-m003   <none>           <none>
2026-01-05 14:56:07,581 [INFO]   argo          resilience-bench-ptngm-run-health-check-1168793163      0/2     Completed   0               2d16h   192.168.221.67    master-m003   <none>           <none>
2026-01-05 14:56:07,581 [INFO]   argo          resilience-bench-ptngm-run-health-check-1185570782      0/2     Completed   0               2d16h   192.168.221.66    master-m003   <none>           <none>
2026-01-05 14:56:07,581 [INFO]   argo          resilience-bench-ptngm-run-health-check-1202348401      0/2     Completed   0               2d16h   192.168.221.122   master-m003   <none>           <none>
2026-01-05 14:56:07,582 [INFO]   argo          resilience-bench-wf87f-initialize-metrics-3677018408    0/2     Completed   0               2d18h   192.168.221.100   master-m003   <none>           <none>
2026-01-05 14:56:07,582 [INFO]   argo          resilience-bench-wf87f-run-health-check-2914150102      0/2     Completed   0               2d18h   192.168.221.105   master-m003   <none>           <none>
2026-01-05 14:56:07,582 [INFO]   argo          resilience-bench-wf87f-run-health-check-3931060015      0/2     Completed   0               2d18h   192.168.221.104   master-m003   <none>           <none>
2026-01-05 14:56:07,582 [INFO]   argo          resilience-bench-wf87f-run-health-check-3947837634      0/2     Completed   0               2d18h   192.168.221.98    master-m003   <none>           <none>
2026-01-05 14:56:07,582 [INFO]   argo          resilience-bench-wf87f-run-health-check-3964615253      0/2     Completed   0               2d18h   192.168.221.103   master-m003   <none>           <none>
2026-01-05 14:56:07,582 [INFO]   argo          resilience-bench-wf87f-run-node-simulation-4173146970   0/2     Completed   0               2d18h   192.168.221.101   master-m003   <none>           <none>
2026-01-05 14:56:07,582 [INFO]   argo          resilience-bench-wf87f-run-rack-simulation-1356070019   0/2     Completed   0               2d18h   192.168.221.109   master-m003   <none>           <none>
2026-01-05 14:56:07,583 [INFO]   argo          resilience-heft-4cqbx-health-check-1589398418           0/2     Error       0               92m     192.168.15.231    worker-w005   <none>           <none>
2026-01-05 14:56:07,583 [INFO]   argo          resilience-heft-4cqbx-heft-initialize-2594215169        0/2     Completed   0               93m     192.168.221.79    master-m003   <none>           <none>
2026-01-05 14:56:07,583 [INFO]   argo          resilience-heft-76kbp-health-check-270747540            0/2     Error       0               84m     192.168.15.237    worker-w005   <none>           <none>
2026-01-05 14:56:07,583 [INFO]   argo          resilience-heft-76kbp-heft-initialize-3574960751        0/2     Completed   0               84m     192.168.221.86    master-m003   <none>           <none>
2026-01-05 14:56:07,583 [INFO]   argo          resilience-heft-b85xr-health-check-2840770649           0/2     Error       0               80m     192.168.15.240    worker-w005   <none>           <none>
2026-01-05 14:56:07,583 [INFO]   argo          resilience-heft-b85xr-heft-initialize-1754406316        0/2     Completed   0               81m     192.168.221.88    master-m003   <none>           <none>
2026-01-05 14:56:07,583 [INFO]   argo          resilience-heft-fdw9f-health-check-1896357264           0/2     Error       0               86m     192.168.15.235    worker-w005   <none>           <none>
2026-01-05 14:56:07,583 [INFO]   argo          resilience-heft-fdw9f-heft-initialize-2651619139        0/2     Completed   0               86m     192.168.221.77    master-m003   <none>           <none>
2026-01-05 14:56:07,584 [INFO]   argo          resilience-heft-gf256-health-check-980174358            0/2     Error       0               85m     192.168.15.236    worker-w005   <none>           <none>
2026-01-05 14:56:07,584 [INFO]   argo          resilience-heft-gf256-heft-initialize-1548293021        0/2     Completed   0               85m     192.168.221.84    master-m003   <none>           <none>
2026-01-05 14:56:07,584 [INFO]   argo          resilience-heft-gxwpg-health-check-3445304423           0/2     Error       0               88m     192.168.15.233    worker-w005   <none>           <none>
2026-01-05 14:56:07,584 [INFO]   argo          resilience-heft-gxwpg-heft-initialize-2377397022        0/2     Completed   0               88m     192.168.221.81    master-m003   <none>           <none>
2026-01-05 14:56:07,584 [INFO]   argo          resilience-heft-pc2kb-health-check-3258812212           0/2     Error       0               82m     192.168.15.238    worker-w005   <none>           <none>
2026-01-05 14:56:07,584 [INFO]   argo          resilience-heft-pc2kb-heft-initialize-321333135         0/2     Completed   0               83m     192.168.221.83    master-m003   <none>           <none>
2026-01-05 14:56:07,584 [INFO]   argo          resilience-heft-qsmch-health-check-283908068            0/2     Error       0               89m     192.168.15.232    worker-w005   <none>           <none>
2026-01-05 14:56:07,584 [INFO]   argo          resilience-heft-qsmch-heft-initialize-1908810015        0/2     Completed   0               90m     192.168.221.82    master-m003   <none>           <none>
2026-01-05 14:56:07,585 [INFO]   argo          resilience-heft-r2w8l-health-check-366927355            0/2     Error       0               81m     192.168.15.239    worker-w005   <none>           <none>
2026-01-05 14:56:07,585 [INFO]   argo          resilience-heft-r2w8l-heft-initialize-3054048250        0/2     Completed   0               82m     192.168.221.85    master-m003   <none>           <none>
2026-01-05 14:56:07,585 [INFO]   argo          resilience-heft-t988k-health-check-612195296            0/2     Error       0               87m     192.168.15.234    worker-w005   <none>           <none>
2026-01-05 14:56:07,585 [INFO]   argo          resilience-heft-t988k-heft-initialize-164752883         0/2     Completed   0               87m     192.168.221.80    master-m003   <none>           <none>
2026-01-05 14:56:07,585 [INFO]   argo          resilience-heft-vlt2m-health-check-2709447919           0/2     Error       0               43m     192.168.221.119   master-m003   <none>           <none>
2026-01-05 14:56:07,585 [INFO]   argo          resilience-heft-vlt2m-health-check-2726225538           0/2     Error       0               43m     192.168.221.120   master-m003   <none>           <none>
2026-01-05 14:56:07,585 [INFO]   argo          resilience-heft-vlt2m-health-check-2743003157           0/2     Error       0               43m     192.168.221.121   master-m003   <none>           <none>
2026-01-05 14:56:07,586 [INFO]   argo          resilience-heft-vlt2m-heft-initialize-840404112         0/2     Completed   0               44m     192.168.221.118   master-m003   <none>           <none>
2026-01-05 14:56:07,586 [INFO]   argo          resilience-heft-x86wd-health-check-2184182048           0/2     Error       0               42m     192.168.221.123   master-m003   <none>           <none>
2026-01-05 14:56:07,586 [INFO]   argo          resilience-heft-x86wd-health-check-2200959667           0/2     Error       0               42m     192.168.221.125   master-m003   <none>           <none>
2026-01-05 14:56:07,586 [INFO]   argo          resilience-heft-x86wd-health-check-2234514905           0/2     Error       0               42m     192.168.221.124   master-m003   <none>           <none>
2026-01-05 14:56:07,586 [INFO]   argo          resilience-heft-x86wd-heft-initialize-2152040770        0/2     Completed   0               42m     192.168.221.127   master-m003   <none>           <none>
2026-01-05 14:56:07,586 [INFO]   argo          workflow-controller-ccbd949dc-t4rx9                     1/1     Running     0               8m18s   192.168.195.243   worker-w002   <none>           <none>
2026-01-05 14:56:07,586 [INFO]   default       resilience-heft-2-202047-tbk8r                          1/1     Running     0               5m20s   192.168.221.126   master-m003   <none>           <none>
2026-01-05 14:56:07,586 [INFO]   kube-system   calico-kube-controllers-7498b9bb4c-xd7tb                1/1     Running     0               38h     192.168.191.108   worker-w006   <none>           <none>
2026-01-05 14:56:07,587 [INFO]   kube-system   calico-node-4zhd4                                       1/1     Running     0               10d     192.168.56.105    worker-w002   <none>           <none>
2026-01-05 14:56:07,587 [INFO]   kube-system   calico-node-75nx6                                       1/1     Running     0               10d     192.168.56.109    worker-w006   <none>           <none>
2026-01-05 14:56:07,587 [INFO]   kube-system   calico-node-7lkdq                                       1/1     Running     0               10d     192.168.56.104    worker-w001   <none>           <none>
2026-01-05 14:56:07,587 [INFO]   kube-system   calico-node-85f8c                                       1/1     Running     0               10d     192.168.56.102    master-m002   <none>           <none>
2026-01-05 14:56:07,587 [INFO]   kube-system   calico-node-j8nb9                                       1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:56:07,587 [INFO]   kube-system   calico-node-lbcb2                                       1/1     Running     0               10d     192.168.56.106    worker-w003   <none>           <none>
2026-01-05 14:56:07,587 [INFO]   kube-system   calico-node-wlr5v                                       1/1     Running     0               10d     192.168.56.107    worker-w004   <none>           <none>
2026-01-05 14:56:07,587 [INFO]   kube-system   calico-node-xnzjw                                       1/1     Running     0               10d     192.168.56.103    master-m003   <none>           <none>
2026-01-05 14:56:07,588 [INFO]   kube-system   calico-node-xsltn                                       1/1     Running     0               10d     192.168.56.108    worker-w005   <none>           <none>
2026-01-05 14:56:07,588 [INFO]   kube-system   coredns-668d6bf9bc-2f74f                                1/1     Running     0               40h     192.168.15.230    worker-w005   <none>           <none>
2026-01-05 14:56:07,588 [INFO]   kube-system   coredns-668d6bf9bc-qnzjq                                1/1     Running     0               2m40s   192.168.132.176   worker-w001   <none>           <none>
2026-01-05 14:56:07,588 [INFO]   kube-system   etcd-master-m001                                        1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:56:07,588 [INFO]   kube-system   kube-apiserver-master-m001                              1/1     Running     1 (2d11h ago)   10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:56:07,588 [INFO]   kube-system   kube-controller-manager-master-m001                     1/1     Running     12 (40m ago)    10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:56:07,588 [INFO]   kube-system   kube-proxy-5lzhj                                        1/1     Running     0               10d     192.168.56.102    master-m002   <none>           <none>
2026-01-05 14:56:07,589 [INFO]   kube-system   kube-proxy-7jqkv                                        1/1     Running     0               10d     192.168.56.109    worker-w006   <none>           <none>
2026-01-05 14:56:07,589 [INFO]   kube-system   kube-proxy-cv5dt                                        1/1     Running     0               10d     192.168.56.108    worker-w005   <none>           <none>
2026-01-05 14:56:07,589 [INFO]   kube-system   kube-proxy-fvpmr                                        1/1     Running     0               10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:56:07,589 [INFO]   kube-system   kube-proxy-hgs5z                                        1/1     Running     0               10d     192.168.56.106    worker-w003   <none>           <none>
2026-01-05 14:56:07,589 [INFO]   kube-system   kube-proxy-kmgqr                                        1/1     Running     0               10d     192.168.56.105    worker-w002   <none>           <none>
2026-01-05 14:56:07,589 [INFO]   kube-system   kube-proxy-rdbz5                                        1/1     Running     0               10d     192.168.56.107    worker-w004   <none>           <none>
2026-01-05 14:56:07,589 [INFO]   kube-system   kube-proxy-w8mnb                                        1/1     Running     0               10d     192.168.56.103    master-m003   <none>           <none>
2026-01-05 14:56:07,589 [INFO]   kube-system   kube-proxy-x9jxr                                        1/1     Running     0               10d     192.168.56.104    worker-w001   <none>           <none>
2026-01-05 14:56:07,590 [INFO]   kube-system   kube-scheduler-master-m001                              1/1     Running     11 (40m ago)    10d     192.168.56.101    master-m001   <none>           <none>
2026-01-05 14:56:07,590 [INFO] 
Pod distribution by node:
2026-01-05 14:56:07,705 [INFO]   Node master-m003: 33 pods
2026-01-05 14:56:07,706 [INFO]   Node worker-w005: 13 pods
2026-01-05 14:56:07,706 [INFO]   Node worker-w002: 3 pods
2026-01-05 14:56:07,706 [INFO]   Node worker-w006: 3 pods
2026-01-05 14:56:07,706 [INFO]   Node worker-w001: 3 pods
2026-01-05 14:56:07,706 [INFO]   Node master-m002: 2 pods
2026-01-05 14:56:07,706 [INFO]   Node master-m001: 3 pods
2026-01-05 14:56:07,707 [INFO]   Node worker-w003: 2 pods
2026-01-05 14:56:07,707 [INFO]   Node worker-w004: 2 pods
2026-01-05 14:56:07,707 [INFO]   Node 10d: 3 pods
2026-01-05 14:56:07,707 [INFO] 
Filtering for simulation services:
2026-01-05 14:56:07,806 [INFO] Node master-m001 is Ready
2026-01-05 14:56:07,807 [INFO] Node master-m002 is Ready
2026-01-05 14:56:07,807 [INFO] Node master-m003 is Ready
2026-01-05 14:56:07,807 [INFO] Node worker-w001 is Ready
2026-01-05 14:56:07,807 [INFO] Node worker-w002 is Ready
2026-01-05 14:56:07,807 [INFO] Node worker-w003 is Ready
2026-01-05 14:56:07,808 [INFO] Node worker-w004 is Ready
2026-01-05 14:56:07,808 [INFO] Node worker-w005 is Ready
2026-01-05 14:56:07,808 [INFO] Node worker-w006 is Ready
2026-01-05 14:56:07,811 [WARNING] No pods found for etcd-sim
2026-01-05 14:56:07,813 [WARNING] No pods found for postgres-sim
2026-01-05 14:56:07,816 [WARNING] No pods found for redis-sim
2026-01-05 14:56:07,818 [WARNING] No pods found for nginx-sim
2026-01-05 14:56:07,820 [WARNING] No pods found for auth-sim
2026-01-05 14:56:07,820 [INFO] Completed full health check
[2026-01-05 14:56:07] TIMING: FINAL_HEALTH_CHECK completed in 2 seconds

================================================================
 HEFT-OPTIMIZED SIMULATION COMPLETE
================================================================
Run ID: heft-native-20260105-145051
Total Duration: 316 seconds
HEFT Exclusions Applied:
  - Node Exclusion: worker-w005
  - Zone Exclusion: R3
Metrics saved to: /app/logs/heft-native-20260105-145051/metrics.txt
Timing saved to: /app/logs/heft-native-20260105-145051/timing.csv
