===== GITHUB ACTIONS RESILIENCE SIMULATION =====
Run ID: gha-bench-20260103-101713-9
Start: Sat Jan  3 10:17:22 AM IST 2026
End: Sat Jan  3 10:29:25 AM IST 2026
Duration: 723 seconds


===== health_check_1.log =====
===== HEALTH CHECK 1 STARTED at Sat Jan  3 10:18:19 AM IST 2026 =====
===== HEALTH CHECK 1 =====
Log directory created/verified: /app/logs
File logging configured successfully
2026-01-03 04:48:23,706 [INFO] Loaded in-cluster Kubernetes config
2026-01-03 04:48:23,706 [INFO] Running on host: gha-hc1-9-qbf7f
2026-01-03 04:48:23,706 [INFO] Detected current node: master-m003, zone: R3
2026-01-03 04:48:23,706 [INFO] Checking if we have permissions to modify nodes...
2026-01-03 04:48:23,739 [INFO] Testing permissions using node: master-m001
2026-01-03 04:48:23,746 [INFO] Permission check successful - we can modify nodes
2026-01-03 04:48:23,747 [INFO] Using real Kubernetes API for node control
2026-01-03 04:48:23,748 [INFO] Action received: health-check
2026-01-03 04:48:23,748 [INFO] Stabilization time: 10 seconds
2026-01-03 04:48:23,748 [INFO] Starting full health check
2026-01-03 04:48:23,748 [INFO] 
============ DETAILED NODE STATUS ============
2026-01-03 04:48:23,748 [INFO] Basic Node Information (kubectl get nodes -o wide):
2026-01-03 04:48:23,826 [INFO]   NAME          STATUS   ROLES           AGE   VERSION    INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
2026-01-03 04:48:23,827 [INFO]   master-m001   Ready    control-plane   8d    v1.32.11   192.168.56.101   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-03 04:48:23,827 [INFO]   master-m002   Ready    control-plane   8d    v1.32.11   192.168.56.102   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-03 04:48:23,827 [INFO]   master-m003   Ready    control-plane   8d    v1.32.11   192.168.56.103   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-03 04:48:23,827 [INFO]   worker-w001   Ready    <none>          8d    v1.32.11   192.168.56.104   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-03 04:48:23,827 [INFO]   worker-w002   Ready    <none>          8d    v1.32.11   192.168.56.105   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-03 04:48:23,827 [INFO]   worker-w003   Ready    <none>          8d    v1.32.11   192.168.56.106   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-03 04:48:23,827 [INFO]   worker-w004   Ready    <none>          8d    v1.32.11   192.168.56.107   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-03 04:48:23,827 [INFO]   worker-w005   Ready    <none>          8d    v1.32.11   192.168.56.108   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-03 04:48:23,828 [INFO]   worker-w006   Ready    <none>          8d    v1.32.11   192.168.56.109   <none>        Ubuntu 20.04.6 LTS   5.4.0-216-generic   containerd://1.7.24
2026-01-03 04:48:23,828 [INFO] 
Enhanced Node Status (with taint and cordon indicators):
2026-01-03 04:48:23,828 [INFO]   NAME                STATUS    ROLES           ZONE   CORDONED   TAINTS
2026-01-03 04:48:24,102 [INFO]   master-m001     Ready  ✓ worker         R1    No       node-role.kubernetes.io/control-plane 
2026-01-03 04:48:24,305 [INFO]   master-m002     Ready  ✓ worker         R2    No       None 
2026-01-03 04:48:24,458 [INFO]   master-m003     Ready  ✓ worker         R3    No       None 
2026-01-03 04:48:24,612 [INFO]   worker-w001     Ready  ✓ worker         R1    No       None 
2026-01-03 04:48:24,766 [INFO]   worker-w002     Ready  ✓ worker         R1    No       None 
2026-01-03 04:48:24,955 [INFO]   worker-w003     Ready  ✓ worker         R2    No       None 
2026-01-03 04:48:25,133 [INFO]   worker-w004     Ready  ✓ worker         R2    No       None 
2026-01-03 04:48:25,324 [INFO]   worker-w005     Ready  ✓ worker         R3    No       None 
2026-01-03 04:48:25,512 [INFO]   worker-w006     Ready  ✓ worker         R3    No       None 
2026-01-03 04:48:25,513 [INFO] 
Legend:
2026-01-03 04:48:25,513 [INFO]   ✓ = Node is Ready
2026-01-03 04:48:25,513 [INFO]   ⚠️ = Warning indicator (NotReady, Cordoned, or has simulated-failure taint)
2026-01-03 04:48:25,513 [INFO] ============ DETAILED POD INFORMATION ============
2026-01-03 04:48:25,514 [INFO] Running 'kubectl get pods -o wide' to show detailed pod placement:
2026-01-03 04:48:25,598 [INFO]   NAMESPACE     NAME                                                    READY   STATUS      RESTARTS      AGE     IP                NODE          NOMINATED NODE   READINESS GATES
2026-01-03 04:48:25,598 [INFO]   argo          argo-server-5c69cb69db-gdkl6                            1/1     Running     0             5d14h   192.168.221.65    master-m003   <none>           <none>
2026-01-03 04:48:25,599 [INFO]   argo          resilience-bench-ptngm-initialize-metrics-940927476     0/2     Completed   0             6h51m   192.168.221.125   master-m003   <none>           <none>
2026-01-03 04:48:25,599 [INFO]   argo          resilience-bench-ptngm-run-health-check-1168793163      0/2     Completed   0             6h51m   192.168.221.67    master-m003   <none>           <none>
2026-01-03 04:48:25,599 [INFO]   argo          resilience-bench-ptngm-run-health-check-1185570782      0/2     Completed   0             6h51m   192.168.221.66    master-m003   <none>           <none>
2026-01-03 04:48:25,599 [INFO]   argo          resilience-bench-ptngm-run-health-check-1202348401      0/2     Completed   0             6h51m   192.168.221.122   master-m003   <none>           <none>
2026-01-03 04:48:25,599 [INFO]   argo          resilience-bench-wf87f-initialize-metrics-3677018408    0/2     Completed   0             8h      192.168.221.100   master-m003   <none>           <none>
2026-01-03 04:48:25,599 [INFO]   argo          resilience-bench-wf87f-run-health-check-2914150102      0/2     Completed   0             8h      192.168.221.105   master-m003   <none>           <none>
2026-01-03 04:48:25,599 [INFO]   argo          resilience-bench-wf87f-run-health-check-3931060015      0/2     Completed   0             8h      192.168.221.104   master-m003   <none>           <none>
2026-01-03 04:48:25,599 [INFO]   argo          resilience-bench-wf87f-run-health-check-3947837634      0/2     Completed   0             8h      192.168.221.98    master-m003   <none>           <none>
2026-01-03 04:48:25,600 [INFO]   argo          resilience-bench-wf87f-run-health-check-3964615253      0/2     Completed   0             8h      192.168.221.103   master-m003   <none>           <none>
2026-01-03 04:48:25,600 [INFO]   argo          resilience-bench-wf87f-run-node-simulation-4173146970   0/2     Completed   0             8h      192.168.221.101   master-m003   <none>           <none>
2026-01-03 04:48:25,600 [INFO]   argo          resilience-bench-wf87f-run-rack-simulation-1356070019   0/2     Completed   0             8h      192.168.221.109   master-m003   <none>           <none>
2026-01-03 04:48:25,600 [INFO]   argo          workflow-controller-ccbd949dc-b4rv5                     1/1     Running     0             9m15s   192.168.195.208   worker-w002   <none>           <none>
2026-01-03 04:48:25,600 [INFO]   default       gha-hc1-9-qbf7f                                         1/1     Running     0             6s      192.168.221.117   master-m003   <none>           <none>
2026-01-03 04:48:25,600 [INFO]   kube-system   calico-kube-controllers-7498b9bb4c-tchcl                1/1     Running     0             36m     192.168.191.95    worker-w006   <none>           <none>
2026-01-03 04:48:25,600 [INFO]   kube-system   calico-node-4zhd4                                       1/1     Running     0             8d      192.168.56.105    worker-w002   <none>           <none>
2026-01-03 04:48:25,600 [INFO]   kube-system   calico-node-75nx6                                       1/1     Running     0             8d      192.168.56.109    worker-w006   <none>           <none>
2026-01-03 04:48:25,601 [INFO]   kube-system   calico-node-7lkdq                                       1/1     Running     0             8d      192.168.56.104    worker-w001   <none>           <none>
2026-01-03 04:48:25,601 [INFO]   kube-system   calico-node-85f8c                                       1/1     Running     0             8d      192.168.56.102    master-m002   <none>           <none>
2026-01-03 04:48:25,601 [INFO]   kube-system   calico-node-j8nb9                                       1/1     Running     0             8d      192.168.56.101    master-m001   <none>           <none>
2026-01-03 04:48:25,601 [INFO]   kube-system   calico-node-lbcb2                                       1/1     Running     0             8d      192.168.56.106    worker-w003   <none>           <none>
2026-01-03 04:48:25,601 [INFO]   kube-system   calico-node-wlr5v                                       1/1     Running     0             8d      192.168.56.107    worker-w004   <none>           <none>
2026-01-03 04:48:25,601 [INFO]   kube-system   calico-node-xnzjw                                       1/1     Running     0             8d      192.168.56.103    master-m003   <none>           <none>
2026-01-03 04:48:25,601 [INFO]   kube-system   calico-node-xsltn                                       1/1     Running     0             8d      192.168.56.108    worker-w005   <none>           <none>
2026-01-03 04:48:25,601 [INFO]   kube-system   coredns-668d6bf9bc-4dkfp                                1/1     Running     0             9m21s   192.168.132.137   worker-w001   <none>           <none>
2026-01-03 04:48:25,601 [INFO]   kube-system   coredns-668d6bf9bc-gx4cv                                1/1     Running     0             3h11m   192.168.15.221    worker-w005   <none>           <none>
2026-01-03 04:48:25,602 [INFO]   kube-system   etcd-master-m001                                        1/1     Running     0             8d      192.168.56.101    master-m001   <none>           <none>
2026-01-03 04:48:25,602 [INFO]   kube-system   kube-apiserver-master-m001                              1/1     Running     1 (95m ago)   8d      192.168.56.101    master-m001   <none>           <none>
2026-01-03 04:48:25,602 [INFO]   kube-system   kube-controller-manager-master-m001                     1/1     Running     8 (96m ago)   8d      192.168.56.101    master-m001   <none>           <none>
2026-01-03 04:48:25,602 [INFO]   kube-system   kube-proxy-5lzhj                                        1/1     Running     0             8d      192.168.56.102    master-m002   <none>           <none>
2026-01-03 04:48:25,602 [INFO]   kube-system   kube-proxy-7jqkv                                        1/1     Running     0             8d      192.168.56.109    worker-w006   <none>           <none>
2026-01-03 04:48:25,602 [INFO]   kube-system   kube-proxy-cv5dt                                        1/1     Running     0             8d      192.168.56.108    worker-w005   <none>           <none>
2026-01-03 04:48:25,602 [INFO]   kube-system   kube-proxy-fvpmr                                        1/1     Running     0             8d      192.168.56.101    master-m001   <none>           <none>
2026-01-03 04:48:25,602 [INFO]   kube-system   kube-proxy-hgs5z                                        1/1     Running     0             8d      192.168.56.106    worker-w003   <none>           <none>
2026-01-03 04:48:25,602 [INFO]   kube-system   kube-proxy-kmgqr                                        1/1     Running     0             8d      192.168.56.105    worker-w002   <none>           <none>
2026-01-03 04:48:25,603 [INFO]   kube-system   kube-proxy-rdbz5                                        1/1     Running     0             8d      192.168.56.107    worker-w004   <none>           <none>
2026-01-03 04:48:25,603 [INFO]   kube-system   kube-proxy-w8mnb                                        1/1     Running     0             8d      192.168.56.103    master-m003   <none>           <none>
2026-01-03 04:48:25,603 [INFO]   kube-system   kube-proxy-x9jxr                                        1/1     Running     0             8d      192.168.56.104    worker-w001   <none>           <none>
2026-01-03 04:48:25,603 [INFO]   kube-system   kube-scheduler-master-m001                              1/1     Running     8 (97m ago)   8d      192.168.56.101    master-m001   <none>           <none>
2026-01-03 04:48:25,603 [INFO] 
Pod distribution by node:
2026-01-03 04:48:25,697 [INFO]   Node master-m003: 15 pods
2026-01-03 04:48:25,698 [INFO]   Node worker-w002: 3 pods
2026-01-03 04:48:25,698 [INFO]   Node worker-w006: 3 pods
2026-01-03 04:48:25,698 [INFO]   Node worker-w001: 3 pods
2026-01-03 04:48:25,698 [INFO]   Node master-m002: 2 pods
2026-01-03 04:48:25,698 [INFO]   Node master-m001: 3 pods
2026-01-03 04:48:25,698 [INFO]   Node worker-w003: 2 pods
2026-01-03 04:48:25,698 [INFO]   Node worker-w004: 2 pods
2026-01-03 04:48:25,699 [INFO]   Node worker-w005: 3 pods
2026-01-03 04:48:25,699 [INFO]   Node 8d: 3 pods
2026-01-03 04:48:25,699 [INFO] 
Filtering for simulation services:
2026-01-03 04:48:25,784 [INFO] Node master-m001 is Ready
2026-01-03 04:48:25,784 [INFO] Node master-m002 is Ready
2026-01-03 04:48:25,784 [INFO] Node master-m003 is Ready
2026-01-03 04:48:25,784 [INFO] Node worker-w001 is Ready
2026-01-03 04:48:25,785 [INFO] Node worker-w002 is Ready
2026-01-03 04:48:25,785 [INFO] Node worker-w003 is Ready
2026-01-03 04:48:25,785 [INFO] Node worker-w004 is Ready
2026-01-03 04:48:25,785 [INFO] Node worker-w005 is Ready
2026-01-03 04:48:25,785 [INFO] Node worker-w006 is Ready
2026-01-03 04:48:25,788 [WARNING] No pods found for etcd-sim
2026-01-03 04:48:25,790 [WARNING] No pods found for postgres-sim
2026-01-03 04:48:25,792 [WARNING] No pods found for redis-sim
2026-01-03 04:48:25,793 [WARNING] No pods found for nginx-sim
2026-01-03 04:48:25,795 [WARNING] No pods found for auth-sim
2026-01-03 04:48:25,795 [INFO] Completed full health check
===== HEALTH CHECK 1 COMPLETED at Sat Jan  3 10:18:29 AM IST 2026 (SUCCESS) =====
